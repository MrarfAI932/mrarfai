#!/usr/bin/env python3
"""
MRARFAI V8.0 â€” Phase IV: Self-Evolution Layer (è‡ªè¿›åŒ–å±‚)
=========================================================
å€Ÿé‰´:
  - ADAS (ICLR 2025): å…ƒ Agent è‡ªåŠ¨è®¾è®¡æ›´å¥½çš„ Agent
  - SKILLRL: å†å²è½¨è¿¹è’¸é¦ä¸ºå¯å¤ç”¨æŠ€èƒ½
  - LLM-as-Judge: 53.3% é‡‡ç”¨ç‡ (LangChain 2026 è°ƒç ”)
  - Self-evolving AI Agents Survey: åé¦ˆé—­ç¯æ¡†æ¶

+4 åˆ†æå‡

æ ¸å¿ƒç†å¿µ: Agent åœ¨ä½¿ç”¨ä¸­å˜å¾—æ›´å¥½
  1. Reviewer ç»“æ„åŒ–æ£€æŸ¥ (ç¡¬é—¨æ§) â€” è¾“å‡ºå¿…é¡»è¿‡å…³
  2. è‡ªåŠ¨è¯„ä¼°å¾ªç¯ â€” æ— éœ€äººå·¥æ‰“åˆ†
  3. æŠ€èƒ½è’¸é¦ â€” å¥½çš„åˆ†ææ¨¡å¼è‡ªåŠ¨æ²‰æ·€
  4. æ€§èƒ½è¿½è¸ª â€” æŒç»­ç›‘æ§è´¨é‡è¶‹åŠ¿
"""

import json
import time
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from collections import defaultdict


# ============================================================
# 1. ç»“æ„åŒ– Reviewer (ç¡¬é—¨æ§)
# ============================================================

@dataclass
class ReviewCheckItem:
    """å®¡æŸ¥æ¡ç›®"""
    name: str
    passed: bool
    score: float       # 0-10
    detail: str = ""


@dataclass
class ReviewResult:
    """å®¡æŸ¥ç»“æœ"""
    overall_score: float
    passed: bool
    checks: List[ReviewCheckItem] = field(default_factory=list)
    blockers: List[str] = field(default_factory=list)  # é˜»æ–­æ€§é—®é¢˜
    suggestions: List[str] = field(default_factory=list)
    review_time_ms: float = 0.0

    def to_dict(self) -> dict:
        return {
            "score": round(self.overall_score, 1),
            "passed": self.passed,
            "checks": [
                {"name": c.name, "passed": c.passed, "score": c.score, "detail": c.detail}
                for c in self.checks
            ],
            "blockers": self.blockers,
            "suggestions": self.suggestions,
            "time_ms": round(self.review_time_ms, 1),
        }


class StructuredReviewer:
    """
    ç»“æ„åŒ–å®¡æŸ¥å™¨ â€” ç¡¬é—¨æ§

    vs V7 CriticAgent:
    - V7: LLM æ‰“åˆ† (ä¸»è§‚ã€ä¸ç¨³å®š)
    - V8: è§„åˆ™+LLM æ··åˆå®¡æŸ¥ (ç¡®å®šæ€§ + çµæ´»æ€§)

    æ£€æŸ¥æ¸…å•:
    1. [ç¡¬] æ•°æ®å‡†ç¡®æ€§ â€” å¼•ç”¨çš„æ•°å­—å¿…é¡»å­˜åœ¨äºä¸Šä¸‹æ–‡
    2. [ç¡¬] å›ç­”å®Œæ•´æ€§ â€” å¿…é¡»å›ç­”ç”¨æˆ·é—®é¢˜
    3. [è½¯] å¯æ‰§è¡Œæ€§ â€” å»ºè®®æ˜¯å¦å…·ä½“
    4. [è½¯] æ ¼å¼è´¨é‡ â€” ç»“æ„æ˜¯å¦æ¸…æ™°
    5. [è½¯] ç®€æ´åº¦ â€” æ˜¯å¦å†—ä½™
    """

    # ç¡¬é—¨æ§é˜ˆå€¼
    HARD_GATE_THRESHOLD = 5.0   # ç¡¬é—¨æ§ä¸è¿‡ç›´æ¥æ‹’ç»
    PASS_THRESHOLD = 6.5        # æ€»åˆ†é˜ˆå€¼

    def review(self, answer: str, question: str,
               context_data: str = "", agent_outputs: Dict[str, str] = None) -> ReviewResult:
        """
        ç»“æ„åŒ–å®¡æŸ¥

        Returns:
            ReviewResult
        """
        t0 = time.time()
        checks = []
        blockers = []
        suggestions = []

        # Check 1: æ•°æ®å‡†ç¡®æ€§ [ç¡¬é—¨æ§]
        data_check = self._check_data_accuracy(answer, context_data)
        checks.append(data_check)
        if not data_check.passed:
            blockers.append(f"æ•°æ®å‡†ç¡®æ€§ä¸è¶³: {data_check.detail}")

        # Check 2: å›ç­”å®Œæ•´æ€§ [ç¡¬é—¨æ§]
        completeness = self._check_completeness(answer, question)
        checks.append(completeness)
        if not completeness.passed:
            blockers.append(f"å›ç­”ä¸å®Œæ•´: {completeness.detail}")

        # Check 3: å¯æ‰§è¡Œæ€§ [è½¯]
        actionability = self._check_actionability(answer, question)
        checks.append(actionability)
        if not actionability.passed:
            suggestions.append("å»ºè®®æ›´å…·ä½“åŒ–: å¢åŠ æ—¶é—´èŠ‚ç‚¹ã€è´£ä»»äººã€é¢„æœŸæ•ˆæœ")

        # Check 4: æ ¼å¼è´¨é‡ [è½¯]
        format_check = self._check_format(answer)
        checks.append(format_check)

        # Check 5: ç®€æ´åº¦ [è½¯]
        conciseness = self._check_conciseness(answer)
        checks.append(conciseness)
        if not conciseness.passed:
            suggestions.append("å›ç­”å¯ä»¥æ›´ç®€æ´ï¼Œåˆ é™¤é‡å¤å†…å®¹")

        # æ€»åˆ†
        weights = [0.30, 0.25, 0.20, 0.15, 0.10]
        overall = sum(c.score * w for c, w in zip(checks, weights))

        # ç¡¬é—¨æ§: ä»»ä½•é˜»æ–­æ€§é—®é¢˜éƒ½ä¸é€šè¿‡
        passed = len(blockers) == 0 and overall >= self.PASS_THRESHOLD

        elapsed = (time.time() - t0) * 1000

        return ReviewResult(
            overall_score=overall,
            passed=passed,
            checks=checks,
            blockers=blockers,
            suggestions=suggestions,
            review_time_ms=elapsed,
        )

    def _check_data_accuracy(self, answer: str, context: str) -> ReviewCheckItem:
        """æ£€æŸ¥æ•°æ®å‡†ç¡®æ€§"""
        # æå–å›ç­”ä¸­çš„æ•°å­—
        answer_numbers = set(re.findall(r'\d+\.?\d*', answer))
        if not answer_numbers:
            return ReviewCheckItem("æ•°æ®å‡†ç¡®æ€§", True, 7.0, "æ— æ•°å­—å¼•ç”¨")

        # æ£€æŸ¥å…³é”®æ•°å­—æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
        context_numbers = set(re.findall(r'\d+\.?\d*', context))
        if not context_numbers:
            return ReviewCheckItem("æ•°æ®å‡†ç¡®æ€§", True, 6.0, "ä¸Šä¸‹æ–‡æ— æ•°å­—åŸºå‡†")

        # å¤§æ•°å­—éªŒè¯ (>100 çš„æ•°å­—æ›´éœ€è¦éªŒè¯)
        big_numbers = [n for n in answer_numbers if float(n) > 100]
        verified = sum(1 for n in big_numbers if n in context_numbers)
        total_big = max(len(big_numbers), 1)

        accuracy_rate = verified / total_big
        score = accuracy_rate * 10
        passed = score >= self.HARD_GATE_THRESHOLD

        return ReviewCheckItem(
            "æ•°æ®å‡†ç¡®æ€§", passed, score,
            f"å¤§æ•°å­—éªŒè¯ç‡: {accuracy_rate:.0%} ({verified}/{total_big})"
        )

    def _check_completeness(self, answer: str, question: str) -> ReviewCheckItem:
        """æ£€æŸ¥å›ç­”å®Œæ•´æ€§"""
        q = question.lower()

        # æ£€æŸ¥é—®é¢˜ä¸­çš„å…³é”®è¯æ˜¯å¦è¢«å›ç­”
        key_topics = []
        topic_patterns = {
            "å®¢æˆ·": ['å®¢æˆ·', 'å“ç‰Œ', 'å‚å•†'],
            "é£é™©": ['é£é™©', 'é¢„è­¦', 'æµå¤±'],
            "å¢é•¿": ['å¢é•¿', 'æœºä¼š', 'æ½œåŠ›'],
            "è¶‹åŠ¿": ['è¶‹åŠ¿', 'å˜åŒ–', 'èµ°åŠ¿'],
            "å»ºè®®": ['å»ºè®®', 'ç­–ç•¥', 'æ–¹æ¡ˆ'],
        }

        for topic, keywords in topic_patterns.items():
            if any(kw in q for kw in keywords):
                key_topics.append(topic)

        if not key_topics:
            return ReviewCheckItem("å›ç­”å®Œæ•´æ€§", True, 7.0, "é€šç”¨é—®é¢˜")

        covered = sum(1 for t in key_topics if t in answer or any(
            kw in answer for kw in topic_patterns[t]
        ))
        coverage = covered / max(len(key_topics), 1)
        score = coverage * 10
        passed = score >= self.HARD_GATE_THRESHOLD

        return ReviewCheckItem(
            "å›ç­”å®Œæ•´æ€§", passed, score,
            f"è¯é¢˜è¦†ç›–: {covered}/{len(key_topics)} ({', '.join(key_topics)})"
        )

    def _check_actionability(self, answer: str, question: str) -> ReviewCheckItem:
        """æ£€æŸ¥å¯æ‰§è¡Œæ€§"""
        # åªæœ‰æ¶‰åŠå»ºè®®çš„é—®é¢˜æ‰æ£€æŸ¥
        q = question.lower()
        needs_action = any(kw in q for kw in [
            'å»ºè®®', 'æ€ä¹ˆåŠ', 'ç­–ç•¥', 'æ–¹æ¡ˆ', 'åº”è¯¥', 'CEO', 'æŠ¥å‘Š', 'å…¨é¢'
        ])

        if not needs_action:
            return ReviewCheckItem("å¯æ‰§è¡Œæ€§", True, 7.0, "æ— éœ€è¡ŒåŠ¨å»ºè®®")

        # æ£€æŸ¥æ˜¯å¦æœ‰å…·ä½“å»ºè®®
        action_signals = [
            'å»ºè®®', 'æ–¹æ¡ˆ', 'è¡ŒåŠ¨', 'æ­¥éª¤', 'ä¼˜å…ˆ', 'ç«‹å³',
            'åº”è¯¥', 'éœ€è¦', 'å¯ä»¥', 'è®¡åˆ’', 'å®‰æ’', 'é‡ç‚¹',
        ]
        action_count = sum(1 for s in action_signals if s in answer)
        score = min(action_count * 1.5, 10)
        passed = score >= 5.0

        return ReviewCheckItem(
            "å¯æ‰§è¡Œæ€§", passed, score,
            f"è¡ŒåŠ¨ä¿¡å·: {action_count}ä¸ª"
        )

    def _check_format(self, answer: str) -> ReviewCheckItem:
        """æ£€æŸ¥æ ¼å¼è´¨é‡"""
        score = 5.0

        # æœ‰ç»“æ„ (æ ‡é¢˜/åˆ†æ®µ)
        if any(c in answer for c in ['#', '##', '**', 'ğŸ“Š', 'ğŸ”´', 'ğŸ’¡', 'âš ï¸']):
            score += 1.5

        # æœ‰æ¢è¡Œåˆ†æ®µ
        paragraphs = [p for p in answer.split('\n') if p.strip()]
        if len(paragraphs) >= 3:
            score += 1.0

        # é€‚å½“é•¿åº¦ (200-800å­—)
        length = len(answer)
        if 200 <= length <= 800:
            score += 1.5
        elif length < 50:
            score -= 2.0
        elif length > 1500:
            score -= 1.0

        score = max(0, min(10, score))
        return ReviewCheckItem("æ ¼å¼è´¨é‡", score >= 5.0, score,
                              f"{len(answer)}å­—, {len(paragraphs)}æ®µ")

    def _check_conciseness(self, answer: str) -> ReviewCheckItem:
        """æ£€æŸ¥ç®€æ´åº¦"""
        score = 7.0

        # é‡å¤æ£€æµ‹
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', answer)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 10]
        if sentences:
            unique_ratio = len(set(sentences)) / len(sentences)
            if unique_ratio < 0.8:
                score -= 3.0

        # è¿‡é•¿æƒ©ç½š
        if len(answer) > 1200:
            score -= 1.5
        elif len(answer) > 2000:
            score -= 3.0

        score = max(0, min(10, score))
        return ReviewCheckItem("ç®€æ´åº¦", score >= 5.0, score,
                              f"å”¯ä¸€å¥æ¯”ä¾‹: {len(set(sentences))}/{len(sentences)}" if sentences else "")


# ============================================================
# 2. è‡ªåŠ¨è¯„ä¼°å¾ªç¯
# ============================================================

@dataclass
class EvalMetric:
    """è¯„ä¼°æŒ‡æ ‡"""
    name: str
    value: float
    timestamp: float = 0.0
    metadata: Dict = field(default_factory=dict)


class AutoEvalLoop:
    """
    è‡ªåŠ¨è¯„ä¼°å¾ªç¯

    å€Ÿé‰´ LangChain State of Agents 2026:
    - LLM-as-Judge: 53.3% é‡‡ç”¨ç‡
    - Human Review: 59.8%
    - Automated: æœ€ä½³å®è·µ

    åŠŸèƒ½:
    1. æ¯æ¬¡åˆ†æè‡ªåŠ¨è¯„åˆ†
    2. è¿½è¸ªè´¨é‡è¶‹åŠ¿
    3. å¼‚å¸¸æ£€æµ‹ (è´¨é‡çªç„¶ä¸‹é™)
    4. å®šæœŸæŠ¥å‘Š
    """

    def __init__(self):
        self.history: List[EvalMetric] = []
        self.reviewer = StructuredReviewer()
        self.running_avg = 0.0
        self.eval_count = 0

    def evaluate(self, answer: str, question: str,
                 context: str = "", agent_outputs: Dict[str, str] = None,
                 metadata: Dict = None) -> Dict:
        """
        è‡ªåŠ¨è¯„ä¼°ä¸€æ¬¡åˆ†æç»“æœ

        Returns:
            {
                "review": ReviewResult,
                "trend": "improving|stable|declining",
                "avg_score": float,
                "alert": str or None,
            }
        """
        # Reviewer å®¡æŸ¥
        review = self.reviewer.review(answer, question, context, agent_outputs)

        # æ›´æ–°ç»Ÿè®¡
        self.eval_count += 1
        old_avg = self.running_avg
        self.running_avg = (
            old_avg * (self.eval_count - 1) + review.overall_score
        ) / self.eval_count

        # è®°å½•
        metric = EvalMetric(
            name="auto_review",
            value=review.overall_score,
            timestamp=time.time(),
            metadata=metadata or {},
        )
        self.history.append(metric)

        # è¶‹åŠ¿åˆ†æ
        trend = self._analyze_trend()

        # å¼‚å¸¸æ£€æµ‹
        alert = None
        if review.overall_score < old_avg - 2.0 and self.eval_count > 5:
            alert = f"âš ï¸ è´¨é‡å¼‚å¸¸ä¸‹é™: {review.overall_score:.1f} (å‡å€¼ {old_avg:.1f})"

        return {
            "review": review.to_dict(),
            "trend": trend,
            "avg_score": round(self.running_avg, 2),
            "eval_count": self.eval_count,
            "alert": alert,
        }

    def _analyze_trend(self) -> str:
        """åˆ†æè´¨é‡è¶‹åŠ¿"""
        if len(self.history) < 5:
            return "insufficient_data"

        recent = [h.value for h in self.history[-5:]]
        older = [h.value for h in self.history[-10:-5]] if len(self.history) >= 10 else recent

        recent_avg = sum(recent) / len(recent)
        older_avg = sum(older) / len(older)

        if recent_avg > older_avg + 0.5:
            return "improving"
        elif recent_avg < older_avg - 0.5:
            return "declining"
        return "stable"

    def get_report(self) -> Dict:
        """è·å–è¯„ä¼°æŠ¥å‘Š"""
        if not self.history:
            return {"status": "no_data"}

        scores = [h.value for h in self.history]
        return {
            "total_evals": self.eval_count,
            "avg_score": round(self.running_avg, 2),
            "min_score": round(min(scores), 2),
            "max_score": round(max(scores), 2),
            "trend": self._analyze_trend(),
            "pass_rate": f"{sum(1 for s in scores if s >= 6.5) / len(scores):.0%}",
            "recent_5": [round(s, 1) for s in scores[-5:]],
        }


# ============================================================
# 3. æŠ€èƒ½è’¸é¦å™¨ (SKILLRL)
# ============================================================

@dataclass
class DistilledSkill:
    """è’¸é¦åçš„æŠ€èƒ½"""
    skill_id: str
    name: str                    # æŠ€èƒ½åç§°
    pattern: str                 # è§¦å‘æ¨¡å¼
    strategy: str                # åˆ†æç­–ç•¥
    source_questions: List[str]  # æ¥æºé—®é¢˜
    success_rate: float = 0.0   # æˆåŠŸç‡
    usage_count: int = 0
    created_at: float = 0.0


class SkillDistiller:
    """
    æŠ€èƒ½è’¸é¦å™¨ â€” SKILLRL å¯å‘

    ä»å†å²æˆåŠŸçš„åˆ†æè½¨è¿¹ä¸­æå–å¯å¤ç”¨çš„åˆ†ææ¨¡å¼ã€‚

    æµç¨‹:
    1. æ”¶é›†é«˜åˆ†åˆ†æè½¨è¿¹
    2. è¯†åˆ«å…±åŒæ¨¡å¼
    3. è’¸é¦ä¸ºæŠ€èƒ½
    4. åœ¨æ–°æŸ¥è¯¢ä¸­è‡ªåŠ¨åº”ç”¨

    ç¤ºä¾‹:
    - "å®¢æˆ·ABCåˆ†æ" è½¨è¿¹ â†’ è’¸é¦ä¸º "ABCåˆ†çº§åˆ†ææŠ€èƒ½"
    - "é£é™©é¢„è­¦" è½¨è¿¹ â†’ è’¸é¦ä¸º "å¤šç»´é£é™©æ‰«ææŠ€èƒ½"
    """

    def __init__(self):
        self.skills: Dict[str, DistilledSkill] = {}
        self.trajectories: List[Dict] = []
        self._init_default_skills()

    def _init_default_skills(self):
        """åˆå§‹åŒ–é»˜è®¤æŠ€èƒ½"""
        defaults = [
            DistilledSkill(
                skill_id="sk_abc",
                name="ABCåˆ†çº§åˆ†æ",
                pattern="å®¢æˆ·|åˆ†çº§|ABC|åˆ†ç±»|ç­‰çº§",
                strategy="1.æŒ‰é‡‘é¢é™åºæ’åˆ— 2.è®¡ç®—ABCå æ¯” 3.å¯¹æ¯”å»å¹´å˜åŠ¨ 4.è¯†åˆ«å‡é™çº§å®¢æˆ·",
                source_questions=["å®¢æˆ·åˆ†çº§æƒ…å†µ"],
                success_rate=0.85,
                created_at=time.time(),
            ),
            DistilledSkill(
                skill_id="sk_risk",
                name="å¤šç»´é£é™©æ‰«æ",
                pattern="é£é™©|é¢„è­¦|æµå¤±|å¼‚å¸¸|ä¸‹æ»‘",
                strategy="1.æ‰«æ>30%æ–­å´–å®¢æˆ· 2.æ£€æŸ¥HHIé›†ä¸­åº¦ 3.é‡åŒ–é£é™©é‡‘é¢ 4.æŒ‰ç´§æ€¥åº¦æ’åº",
                source_questions=["é£é™©åˆ†æ"],
                success_rate=0.80,
                created_at=time.time(),
            ),
            DistilledSkill(
                skill_id="sk_growth",
                name="å¢é•¿æœºä¼šè¯†åˆ«",
                pattern="å¢é•¿|æœºä¼š|æ½œåŠ›|æå‡|æ‰©å¤§",
                strategy="1.å¯¹æ ‡è¡Œä¸šå¢é•¿ç‡ 2.è¯†åˆ«ä½ä»½é¢é«˜æ½œå®¢æˆ· 3.å“ç±»äº¤å‰åˆ†æ 4.TAMè®¡ç®—",
                source_questions=["å¢é•¿æœºä¼š"],
                success_rate=0.75,
                created_at=time.time(),
            ),
            DistilledSkill(
                skill_id="sk_ceo",
                name="CEOçº§ç»¼åˆæŠ¥å‘Š",
                pattern="CEO|æ€»ç»“|å…¨é¢|æ¦‚è§ˆ|æŠ¥å‘Š|ç»¼åˆ",
                strategy="1.æ ¸å¿ƒæ•°å­—(3å¥è¯) 2.å®¢æˆ·å¥åº·(ABCå˜åŠ¨) 3.é£é™©é¢„è­¦(Top3) 4.å¢é•¿æœºä¼š 5.è¡ŒåŠ¨é¡¹",
                source_questions=["CEOæŠ¥å‘Š"],
                success_rate=0.90,
                created_at=time.time(),
            ),
        ]
        for skill in defaults:
            self.skills[skill.skill_id] = skill

    def record_trajectory(self, question: str, answer: str,
                          agents_used: List[str], score: float,
                          expert_outputs: Dict[str, str] = None):
        """è®°å½•åˆ†æè½¨è¿¹"""
        self.trajectories.append({
            "question": question,
            "answer_preview": answer[:200],
            "agents": agents_used,
            "score": score,
            "expert_outputs": {k: v[:100] for k, v in (expert_outputs or {}).items()},
            "timestamp": time.time(),
        })

    def match_skills(self, question: str) -> List[DistilledSkill]:
        """åŒ¹é…ç›¸å…³æŠ€èƒ½"""
        matched = []
        q = question.lower()
        for skill in self.skills.values():
            pattern_words = skill.pattern.split('|')
            if any(pw in q for pw in pattern_words):
                matched.append(skill)

        # æŒ‰æˆåŠŸç‡æ’åº
        matched.sort(key=lambda s: s.success_rate, reverse=True)
        return matched[:3]

    def distill(self, min_score: float = 7.0, min_count: int = 3) -> List[DistilledSkill]:
        """
        ä»é«˜åˆ†è½¨è¿¹è’¸é¦æ–°æŠ€èƒ½

        æ¡ä»¶: è¯„åˆ† >= min_score ä¸” ç±»ä¼¼é—®é¢˜ >= min_count
        """
        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„
        groups = defaultdict(list)
        for traj in self.trajectories:
            if traj["score"] >= min_score:
                qtype = self._classify_question(traj["question"])
                groups[qtype].append(traj)

        new_skills = []
        for qtype, trajs in groups.items():
            if len(trajs) >= min_count:
                # æå–å…±åŒæ¨¡å¼
                common_agents = self._find_common(
                    [t["agents"] for t in trajs]
                )
                # åˆ›å»ºæ–°æŠ€èƒ½
                skill_id = f"sk_learned_{qtype}_{int(time.time())}"
                skill = DistilledSkill(
                    skill_id=skill_id,
                    name=f"å­¦ä¹ : {qtype}åˆ†æ",
                    pattern=qtype,
                    strategy=f"Agentç»„åˆ: {','.join(common_agents)}",
                    source_questions=[t["question"][:50] for t in trajs[:3]],
                    success_rate=sum(t["score"] for t in trajs) / len(trajs) / 10,
                    created_at=time.time(),
                )
                self.skills[skill_id] = skill
                new_skills.append(skill)

        return new_skills

    def _classify_question(self, question: str) -> str:
        """ç®€å•é—®é¢˜åˆ†ç±»"""
        q = question.lower()
        if any(kw in q for kw in ['é£é™©', 'é¢„è­¦', 'æµå¤±']):
            return "risk"
        if any(kw in q for kw in ['å¢é•¿', 'æœºä¼š', 'ç­–ç•¥']):
            return "growth"
        if any(kw in q for kw in ['CEO', 'æ€»ç»“', 'å…¨é¢']):
            return "overview"
        return "analysis"

    def _find_common(self, lists: List[List[str]]) -> List[str]:
        """æ‰¾å‡ºæœ€å¸¸å‡ºç°çš„å…ƒç´ """
        counts = defaultdict(int)
        for lst in lists:
            for item in lst:
                counts[item] += 1
        return [k for k, v in sorted(counts.items(), key=lambda x: x[1], reverse=True)]

    def get_stats(self) -> Dict:
        """æŠ€èƒ½ç»Ÿè®¡"""
        return {
            "total_skills": len(self.skills),
            "learned_skills": sum(1 for s in self.skills.values() if s.skill_id.startswith("sk_learned")),
            "total_trajectories": len(self.trajectories),
            "avg_trajectory_score": (
                sum(t["score"] for t in self.trajectories) /
                max(len(self.trajectories), 1)
            ),
            "skills": [
                {
                    "name": s.name,
                    "success_rate": f"{s.success_rate:.0%}",
                    "pattern": s.pattern[:30],
                }
                for s in sorted(self.skills.values(),
                               key=lambda x: x.success_rate, reverse=True)
            ],
        }


# ============================================================
# 4. å…¨å±€å®ä¾‹
# ============================================================

_reviewer: Optional[StructuredReviewer] = None
_eval_loop: Optional[AutoEvalLoop] = None
_distiller: Optional[SkillDistiller] = None


def get_reviewer() -> StructuredReviewer:
    global _reviewer
    if _reviewer is None:
        _reviewer = StructuredReviewer()
    return _reviewer


def get_eval_loop() -> AutoEvalLoop:
    global _eval_loop
    if _eval_loop is None:
        _eval_loop = AutoEvalLoop()
    return _eval_loop


def get_distiller() -> SkillDistiller:
    global _distiller
    if _distiller is None:
        _distiller = SkillDistiller()
    return _distiller
