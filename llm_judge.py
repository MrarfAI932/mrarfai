#!/usr/bin/env python3
"""
MRARFAI LLM-as-Judge v5.0
============================
Phase 1 æ ¸å¿ƒå‡çº§ï¼šç”¨ LLM è‡ªåŠ¨è¯„ä¼° Agent è¾“å‡ºè´¨é‡

ä¸‰ä¸ªè¯„ä¼°ç»´åº¦ï¼š
  â‘  Correctness â€” æ•°æ®å¼•ç”¨å‡†ç¡®å—ï¼Ÿè®¡ç®—å¯¹å—ï¼Ÿ
  â‘¡ Relevance   â€” å›ç­”åˆ‡é¢˜å—ï¼Ÿé‡ç‚¹çªå‡ºå—ï¼Ÿ
  â‘¢ Hallucination â€” æœ‰ç¼–é€ æ•°æ®å—ï¼Ÿè¶…å‡ºåŸå§‹æ•°æ®èŒƒå›´å—ï¼Ÿ

è®¾è®¡åŸåˆ™ï¼š
  - æ¯ä¸ªç»´åº¦ç‹¬ç«‹è¯„åˆ†ï¼Œäº’ä¸å¹²æ‰°
  - è¯„åˆ† 0.0-1.0 + ä¸€å¥è¯ç†ç”±
  - æ”¯æŒæœ¬åœ°è¿è¡Œï¼ˆAnthropic APIï¼‰å’Œ Langfuse é›†æˆ
  - å¼‚æ­¥è¯„ä¼°ï¼Œä¸é˜»å¡ä¸»æµç¨‹
"""

import json
import time
import logging
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger("mrarfai.judge")


# ============================================================
# è¯„åˆ†ç»´åº¦å’Œ Prompt æ¨¡æ¿
# ============================================================

JUDGE_DIMENSIONS = {
    "correctness": {
        "name": "æ•°æ®å‡†ç¡®æ€§",
        "system": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ•°æ®åˆ†æè´¨é‡è¯„å®¡ä¸“å®¶ã€‚ä½ åªè¯„ä¼°æ•°æ®å‡†ç¡®æ€§ï¼Œä¸è¯„ä»·æ–‡é£æˆ–æ ¼å¼ã€‚",
        "template": """è¯„ä¼°ä»¥ä¸‹é”€å”®åˆ†ææŠ¥å‘Šçš„**æ•°æ®å‡†ç¡®æ€§**ã€‚

## è¯„åˆ†æ ‡å‡†ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
- 1.0: æ‰€æœ‰æ•°æ®å¼•ç”¨å‡†ç¡®ï¼Œè®¡ç®—æ­£ç¡®ï¼Œç»“è®ºæœ‰æ•°æ®æ”¯æ’‘
- 0.8: å¤§éƒ¨åˆ†å‡†ç¡®ï¼Œå­˜åœ¨è½»å¾®è¯¯å·®ä½†ä¸å½±å“æ ¸å¿ƒç»“è®º
- 0.5: æœ‰æ˜æ˜¾æ•°æ®é”™è¯¯æˆ–è®¡ç®—å¤±è¯¯ï¼Œä½†æ¡†æ¶åŸºæœ¬æ­£ç¡®
- 0.3: å¤šå¤„æ•°æ®é”™è¯¯ï¼Œç»“è®ºç¼ºä¹æ”¯æ’‘
- 0.0: æ•°æ®ä¸¥é‡é”™è¯¯æˆ–å®Œå…¨ç¼–é€ 

## é‡ç‚¹æ£€æŸ¥
- ç™¾åˆ†æ¯”å’Œå¢é•¿ç‡è®¡ç®—æ˜¯å¦æ­£ç¡®
- å®¢æˆ·åç§°ã€é‡‘é¢ç­‰å¼•ç”¨æ˜¯å¦ä¸åŸå§‹æ•°æ®ä¸€è‡´
- æ’åå’Œå¯¹æ¯”æ˜¯å¦å‡†ç¡®

## å¾…è¯„ä¼°å†…å®¹
ã€ç”¨æˆ·é—®é¢˜ã€‘{question}
ã€åŸå§‹æ•°æ®ï¼ˆæˆªå–ï¼‰ã€‘{context}
ã€Agentå›ç­”ã€‘{output}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼ˆä¸è¦è¿”å›å…¶ä»–å†…å®¹ï¼‰ï¼š
{{"score": 0.0, "reasoning": "ä¸€å¥è¯è¯´æ˜æ‰£åˆ†åŸå› "}}""",
    },
    
    "relevance": {
        "name": "é—®é¢˜ç›¸å…³æ€§",
        "system": "ä½ æ˜¯ç”¨æˆ·ä½“éªŒè¯„å®¡ä¸“å®¶ã€‚ä½ åªè¯„ä¼°å›ç­”æ˜¯å¦ç›´æ¥å›åº”äº†é—®é¢˜ï¼Œä¸è¯„ä»·æ•°æ®æ˜¯å¦å‡†ç¡®ã€‚",
        "template": """è¯„ä¼°å›ç­”æ˜¯å¦**ç›´æ¥å›åº”**äº†ç”¨æˆ·çš„é—®é¢˜ã€‚

## è¯„åˆ†æ ‡å‡†ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
- 1.0: å®Œå…¨å›åº”é—®é¢˜ï¼Œé‡ç‚¹çªå‡ºï¼Œæ— å†—ä½™
- 0.8: åŸºæœ¬å›åº”ï¼Œæœ‰å°‘é‡å†—ä½™ä¿¡æ¯ä½†ä¸å½±å“é˜…è¯»
- 0.5: éƒ¨åˆ†å›åº”ï¼Œé—æ¼äº†é—®é¢˜ä¸­çš„å…³é”®ç‚¹ï¼Œæˆ–æœ‰å¤§é‡æ— å…³å†…å®¹
- 0.3: åªè§¦åŠé—®é¢˜è¾¹ç¼˜ï¼Œæ ¸å¿ƒé—®é¢˜æœªå›ç­”
- 0.0: å®Œå…¨è·‘é¢˜

## é‡ç‚¹æ£€æŸ¥
- ç”¨æˆ·é—®çš„æ˜¯ä»€ä¹ˆï¼Ÿå›ç­”è¦†ç›–äº†å—ï¼Ÿ
- å¦‚æœç”¨æˆ·é—®"å“ªäº›å®¢æˆ·æµå¤±"ï¼Œå›ç­”é‡Œæœ‰å…·ä½“å®¢æˆ·åˆ—è¡¨å—ï¼Ÿ
- å›ç­”çš„ç¯‡å¹…æ˜¯å¦åˆç†ï¼Ÿå¤ªçŸ­ç¼ºä¿¡æ¯ï¼Ÿå¤ªé•¿è·‘é¢˜ï¼Ÿ

ã€ç”¨æˆ·é—®é¢˜ã€‘{question}
ã€Agentå›ç­”ã€‘{output}

è¿”å›JSONï¼š{{"score": 0.0, "reasoning": "ä¸€å¥è¯è¯´æ˜"}}""",
    },
    
    "hallucination": {
        "name": "å¹»è§‰æ£€æµ‹",
        "system": "ä½ æ˜¯äº‹å®æ ¸æŸ¥ä¸“å®¶ã€‚ä½ åªæ£€æŸ¥å›ç­”ä¸­æ˜¯å¦æœ‰è¶…å‡ºåŸå§‹æ•°æ®çš„ç¼–é€ å†…å®¹ã€‚",
        "template": """æ£€æŸ¥å›ç­”ä¸­æ˜¯å¦æœ‰**è¶…å‡ºåŸå§‹æ•°æ®çš„ç¼–é€ å†…å®¹**ã€‚

## è¯„åˆ†æ ‡å‡†ï¼ˆä¸¥æ ¼æ‰§è¡Œï¼‰
- 1.0: æ‰€æœ‰å£°æ˜éƒ½å¯åœ¨æä¾›çš„æ•°æ®ä¸­æ‰¾åˆ°æ˜ç¡®ä¾æ®
- 0.8: åŒ…å«åˆç†çš„è®¡ç®—æ¨å¯¼ï¼ˆå¦‚ä»æ•°æ®ä¸­ç®—å‡ºæ¯”ç‡ï¼‰ï¼Œä½†æ¨å¯¼æ­¥éª¤æ¸…æ™°
- 0.5: æœ‰ä¸€äº›åˆç†æ¨æ–­ä½†è¶…å‡ºæ•°æ®æ˜ç¡®è®°å½•çš„èŒƒå›´
- 0.3: åŒ…å«æ•°æ®ä¸­æ²¡æœ‰çš„å…·ä½“æ•°å­—æˆ–äº‹å®
- 0.0: å¤§é‡ç¼–é€ æ•°æ®æˆ–å¼•ç”¨äº†ä¸å­˜åœ¨çš„æ•°æ®

## é‡ç‚¹æ£€æŸ¥
- å›ç­”ä¸­æåˆ°çš„å…·ä½“æ•°å­—ï¼Œåœ¨åŸå§‹æ•°æ®ä¸­æ‰¾å¾—åˆ°å—ï¼Ÿ
- æœ‰æ²¡æœ‰ç¼–é€ å®¢æˆ·åç§°ã€é‡‘é¢ã€ç™¾åˆ†æ¯”ï¼Ÿ
- è¶‹åŠ¿åˆ¤æ–­æ˜¯å¦æœ‰æ•°æ®æ”¯æ’‘ï¼Ÿ

ã€åŸå§‹æ•°æ®ï¼ˆæˆªå–ï¼‰ã€‘{context}
ã€Agentå›ç­”ã€‘{output}

è¿”å›JSONï¼š{{"score": 0.0, "reasoning": "ä¸€å¥è¯è¯´æ˜"}}""",
    },
}


# ============================================================
# LLM è°ƒç”¨å°è£…ï¼ˆæ”¯æŒå¤š providerï¼‰
# ============================================================

def _call_judge_llm(system: str, prompt: str, 
                    provider: str = "claude", api_key: str = "",
                    model: str = None) -> Dict:
    """è°ƒç”¨ LLM è¿›è¡Œè¯„åˆ†ï¼Œè¿”å› {score, reasoning}"""
    
    if not api_key:
        return {"score": -1, "reasoning": "æ—  API Keyï¼Œè·³è¿‡è¯„ä¼°"}
    
    try:
        if provider == "claude":
            import anthropic
            client = anthropic.Anthropic(api_key=api_key)
            model = model or "claude-sonnet-4-20250514"
            
            resp = client.messages.create(
                model=model,
                max_tokens=150,
                temperature=0,  # è¯„åˆ†éœ€è¦ç¡®å®šæ€§
                system=system,
                messages=[{"role": "user", "content": prompt}],
            )
            raw = resp.content[0].text.strip()
            
        elif provider == "deepseek":
            from openai import OpenAI
            client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
            model = model or "deepseek-chat"
            
            resp = client.chat.completions.create(
                model=model,
                temperature=0,
                max_tokens=150,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": prompt},
                ],
            )
            raw = resp.choices[0].message.content.strip()
        
        else:
            return {"score": -1, "reasoning": f"ä¸æ”¯æŒçš„ provider: {provider}"}
        
        # è§£æ JSON å“åº”
        return _parse_judge_response(raw)
        
    except Exception as e:
        logger.error(f"Judge LLM è°ƒç”¨å¤±è´¥: {e}")
        return {"score": -1, "reasoning": f"è°ƒç”¨å¤±è´¥: {str(e)[:100]}"}


def _parse_judge_response(raw: str) -> Dict:
    """ç¨³å¥åœ°è§£æ LLM è¿”å›çš„ JSON è¯„åˆ†"""
    # å°è¯•ç›´æ¥è§£æ
    try:
        result = json.loads(raw)
        if "score" in result:
            score = float(result["score"])
            score = max(0.0, min(1.0, score))  # é’³ä½åˆ° [0, 1]
            return {
                "score": score,
                "reasoning": str(result.get("reasoning", "")).strip(),
            }
    except (json.JSONDecodeError, ValueError, TypeError):
        pass
    
    # å°è¯•ä»æ–‡æœ¬ä¸­æå– JSON
    import re
    json_match = re.search(r'\{[^}]+\}', raw)
    if json_match:
        try:
            result = json.loads(json_match.group())
            if "score" in result:
                score = float(result["score"])
                score = max(0.0, min(1.0, score))
                return {
                    "score": score,
                    "reasoning": str(result.get("reasoning", "")).strip(),
                }
        except (json.JSONDecodeError, ValueError, TypeError):
            pass
    
    # æœ€åå°è¯•æå–æ•°å­—
    numbers = re.findall(r'(?:score|åˆ†æ•°)[^\d]*(\d+\.?\d*)', raw.lower())
    if numbers:
        score = float(numbers[0])
        if score > 1:
            score = score / 10  # å¦‚æœ LLM è¿”å›äº† 0-10 çš„åˆ†æ•°
        return {"score": min(score, 1.0), "reasoning": raw[:200]}
    
    return {"score": -1, "reasoning": f"æ— æ³•è§£æå“åº”: {raw[:200]}"}


# ============================================================
# ä¸»è¯„ä¼°å‡½æ•°
# ============================================================

def judge_output(
    question: str,
    output: str,
    context: str = "",
    dimensions: List[str] = None,
    provider: str = "claude",
    api_key: str = "",
    parallel: bool = True,
    context_max_chars: int = 3000,
) -> Dict:
    """
    å¯¹ Agent è¾“å‡ºè¿›è¡Œå¤šç»´åº¦ LLM-as-Judge è¯„ä¼°
    
    å‚æ•°:
        question: ç”¨æˆ·åŸå§‹é—®é¢˜
        output: Agent ç”Ÿæˆçš„å›ç­”
        context: åŸå§‹æ•°æ®ä¸Šä¸‹æ–‡ï¼ˆç”¨äºå¹»è§‰æ£€æµ‹ï¼‰
        dimensions: è¦è¯„ä¼°çš„ç»´åº¦åˆ—è¡¨ï¼Œé»˜è®¤å…¨éƒ¨
        provider: LLM provider ("claude" / "deepseek")
        api_key: API Key
        parallel: æ˜¯å¦å¹¶è¡Œè¯„ä¼°
        context_max_chars: ä¸Šä¸‹æ–‡æœ€å¤§å­—ç¬¦æ•°
    
    è¿”å›:
        {
            "correctness": {"score": 0.85, "reasoning": "..."},
            "relevance": {"score": 0.9, "reasoning": "..."},
            "hallucination": {"score": 0.8, "reasoning": "..."},
            "overall": 0.85,
            "elapsed_ms": 1234,
            "provider": "claude",
        }
    """
    t0 = time.time()
    dimensions = dimensions or list(JUDGE_DIMENSIONS.keys())
    
    # æˆªæ–­ä¸Šä¸‹æ–‡
    ctx = context[:context_max_chars] if context else "(æ— åŸå§‹æ•°æ®æä¾›)"
    
    # æ„å»ºè¯„ä¼°ä»»åŠ¡
    def _eval_dimension(dim: str) -> tuple:
        config = JUDGE_DIMENSIONS.get(dim)
        if not config:
            return dim, {"score": -1, "reasoning": f"æœªçŸ¥ç»´åº¦: {dim}"}
        
        prompt = config["template"].format(
            question=question[:1000],
            output=output[:2000],
            context=ctx,
        )
        
        result = _call_judge_llm(
            system=config["system"],
            prompt=prompt,
            provider=provider,
            api_key=api_key,
        )
        return dim, result
    
    # å¹¶è¡Œæˆ–ä¸²è¡Œè¯„ä¼°
    scores = {}
    if parallel and len(dimensions) > 1:
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(_eval_dimension, d): d for d in dimensions}
            for future in futures:
                dim, result = future.result()
                scores[dim] = result
    else:
        for dim in dimensions:
            _, result = _eval_dimension(dim)
            scores[dim] = result
    
    # è®¡ç®—ç»¼åˆåˆ†æ•°ï¼ˆå¿½ç•¥å¤±è´¥çš„ç»´åº¦ï¼‰
    valid_scores = [s["score"] for s in scores.values() if s["score"] >= 0]
    overall = round(sum(valid_scores) / len(valid_scores), 3) if valid_scores else -1
    
    elapsed = (time.time() - t0) * 1000
    
    return {
        **scores,
        "overall": overall,
        "elapsed_ms": round(elapsed, 1),
        "provider": provider,
        "dimensions_evaluated": len(valid_scores),
    }


def judge_quick(question: str, output: str, context: str = "",
                provider: str = "claude", api_key: str = "") -> float:
    """å¿«é€Ÿè¯„åˆ† â€” åªè¿”å›ç»¼åˆåˆ†æ•°ï¼ˆç”¨äºæµæ°´çº¿ä¸­å¿«é€Ÿåˆ¤æ–­ï¼‰"""
    result = judge_output(question, output, context, provider=provider, api_key=api_key)
    return result["overall"]


# ============================================================
# æ‰¹é‡è¯„ä¼°ï¼ˆç”¨äº Golden Dataset å›å½’æµ‹è¯•ï¼‰
# ============================================================

def judge_batch(
    test_cases: List[Dict],
    agent_fn: callable,
    provider: str = "claude",
    api_key: str = "",
    version_tag: str = "dev",
) -> Dict:
    """
    æ‰¹é‡è¯„ä¼°ä¸€ç»„æµ‹è¯•ç”¨ä¾‹
    
    å‚æ•°:
        test_cases: [{"question": "...", "data": {...}, "expected": "..."}]
        agent_fn: è°ƒç”¨ Agent çš„å‡½æ•°ï¼Œç­¾å fn(question, data) -> str
        provider: è¯„åˆ†ç”¨çš„ LLM provider
        api_key: API Key
        version_tag: ç‰ˆæœ¬æ ‡ç­¾ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    
    è¿”å›:
        {
            "version": "v4.3",
            "total": 20,
            "avg_scores": {"correctness": 0.82, "relevance": 0.88, ...},
            "results": [...],
            "summary": "..."
        }
    """
    results = []
    dim_totals = {}
    
    for i, case in enumerate(test_cases):
        logger.info(f"è¯„ä¼° [{i+1}/{len(test_cases)}] {case['question'][:50]}...")
        
        # è°ƒç”¨ Agent
        try:
            output = agent_fn(case["question"], case.get("data", {}))
        except Exception as e:
            results.append({
                "question": case["question"],
                "error": str(e),
                "scores": {},
            })
            continue
        
        # è¯„ä¼°
        scores = judge_output(
            question=case["question"],
            output=output if isinstance(output, str) else str(output),
            context=str(case.get("data", ""))[:3000],
            provider=provider,
            api_key=api_key,
        )
        
        # æ±‡æ€»
        for dim in JUDGE_DIMENSIONS:
            if dim in scores and scores[dim]["score"] >= 0:
                if dim not in dim_totals:
                    dim_totals[dim] = []
                dim_totals[dim].append(scores[dim]["score"])
        
        results.append({
            "question": case["question"],
            "output_preview": output[:200] if isinstance(output, str) else str(output)[:200],
            "scores": scores,
        })
    
    # å¹³å‡åˆ†
    avg_scores = {
        dim: round(sum(vals) / len(vals), 3)
        for dim, vals in dim_totals.items()
        if vals
    }
    
    overall_avg = round(
        sum(avg_scores.values()) / len(avg_scores), 3
    ) if avg_scores else -1
    
    return {
        "version": version_tag,
        "total": len(test_cases),
        "evaluated": len([r for r in results if "error" not in r]),
        "avg_scores": avg_scores,
        "overall_avg": overall_avg,
        "results": results,
    }


# ============================================================
# Langfuse é›†æˆ â€” è‡ªåŠ¨å°†è¯„åˆ†å†™å…¥ Langfuse
# ============================================================

def judge_and_trace(
    question: str,
    output: str,
    context: str = "",
    trace_ctx=None,
    provider: str = "claude",
    api_key: str = "",
) -> Dict:
    """
    è¯„ä¼° + è‡ªåŠ¨å†™å…¥ Langfuse Trace
    
    è¿™æ˜¯æ¨èåœ¨ ask_multi_agent ä¸­ä½¿ç”¨çš„å‡½æ•°ã€‚
    """
    scores = judge_output(question, output, context, provider=provider, api_key=api_key)
    
    # å†™å…¥ Langfuse
    if trace_ctx and hasattr(trace_ctx, 'score'):
        for dim in JUDGE_DIMENSIONS:
            if dim in scores and scores[dim]["score"] >= 0:
                trace_ctx.score(
                    name=f"judge-{dim}",
                    value=scores[dim]["score"],
                    comment=scores[dim].get("reasoning", ""),
                )
        
        if scores["overall"] >= 0:
            trace_ctx.score(
                name="judge-overall",
                value=scores["overall"],
                comment=f"ç»¼åˆè¯„åˆ†: {scores['overall']:.2f}",
            )
    
    return scores


# ============================================================
# CLI æµ‹è¯•
# ============================================================

if __name__ == "__main__":
    import os
    
    print("=" * 60)
    print("MRARFAI LLM-as-Judge æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_question = "åä¸œåŒºä»Šå¹´æ•´ä½“è¡¨ç°å¦‚ä½•ï¼Ÿ"
    test_context = """
    åä¸œåŒº2024å¹´æ•°æ®:
    - æ€»è¥æ”¶: 41.71äº¿å…ƒï¼ˆåŒæ¯”+54.1%ï¼‰
    - Topå®¢æˆ·: Aå…¬å¸ 12.3äº¿, Bå…¬å¸ 8.5äº¿, Cå…¬å¸ 6.2äº¿
    - Q4ç¯æ¯”å¢é•¿: 15.2%
    """
    
    # å¥½å›ç­”
    good_output = """
    åä¸œåŒº2024å¹´æ•´ä½“è¡¨ç°å¼ºåŠ²ï¼š
    1. æ€»è¥æ”¶è¾¾41.71äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿54.1%
    2. Top3å®¢æˆ·å æ¯”64.7%ï¼ˆAå…¬å¸12.3äº¿+Bå…¬å¸8.5äº¿+Cå…¬å¸6.2äº¿ï¼‰
    3. Q4å­£åº¦ç¯æ¯”å¢é•¿15.2%ï¼Œå¢é•¿åŠ¨èƒ½æŒç»­
    å»ºè®®ï¼šå…³æ³¨å®¢æˆ·é›†ä¸­åº¦é£é™©ï¼ŒTop3å æ¯”åé«˜ã€‚
    """
    
    # åå›ç­”ï¼ˆå«å¹»è§‰ï¼‰
    bad_output = """
    åä¸œåŒºè¡¨ç°ä¸é”™ã€‚
    æ€»è¥æ”¶å¤§çº¦50äº¿ï¼Œå¢é•¿äº†60%å·¦å³ã€‚
    Då…¬å¸æ˜¯æœ€å¤§å®¢æˆ·ï¼Œè´¡çŒ®äº†20äº¿ã€‚
    å…¨å›½æ’åç¬¬äºŒï¼Œä»…æ¬¡äºåå—åŒºã€‚
    """
    
    api_key = os.getenv("ANTHROPIC_API_KEY", "")
    if not api_key:
        print("âš ï¸  æœªè®¾ç½® ANTHROPIC_API_KEYï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¯„ä¼°")
        print("\nğŸ“Š å¥½å›ç­”ï¼ˆæ¨¡æ‹Ÿè¯„åˆ†ï¼‰:")
        print("  correctness: 0.95 â€” æ•°æ®å¼•ç”¨å®Œå…¨åŒ¹é…")
        print("  relevance:   0.90 â€” ç›´æ¥å›åº”é—®é¢˜")
        print("  hallucination: 0.95 â€” æ‰€æœ‰æ•°å­—éƒ½èƒ½åœ¨æ•°æ®ä¸­æ‰¾åˆ°")
        print("\nğŸ“Š åå›ç­”ï¼ˆæ¨¡æ‹Ÿè¯„åˆ†ï¼‰:")
        print("  correctness: 0.15 â€” 50äº¿/60%/Då…¬å¸å‡é”™è¯¯")
        print("  relevance:   0.40 â€” å›åº”äº†é—®é¢˜ä½†ç¼ºä¹ç»†èŠ‚")
        print("  hallucination: 0.05 â€” ç¼–é€ äº†Då…¬å¸ã€å…¨å›½æ’åç­‰ä¿¡æ¯")
    else:
        print("\nğŸ“Š è¯„ä¼°ã€Œå¥½å›ç­”ã€...")
        good_scores = judge_output(
            test_question, good_output, test_context,
            provider="claude", api_key=api_key,
        )
        for dim, data in good_scores.items():
            if isinstance(data, dict):
                print(f"  {dim}: {data['score']:.2f} â€” {data['reasoning']}")
            elif dim == "overall":
                print(f"  ç»¼åˆ: {data:.2f}")
        
        print(f"\nğŸ“Š è¯„ä¼°ã€Œåå›ç­”ã€...")
        bad_scores = judge_output(
            test_question, bad_output, test_context,
            provider="claude", api_key=api_key,
        )
        for dim, data in bad_scores.items():
            if isinstance(data, dict):
                print(f"  {dim}: {data['score']:.2f} â€” {data['reasoning']}")
            elif dim == "overall":
                print(f"  ç»¼åˆ: {data:.2f}")
        
        print(f"\nâ± å¥½å›ç­”è€—æ—¶: {good_scores['elapsed_ms']:.0f}ms")
        print(f"â± åå›ç­”è€—æ—¶: {bad_scores['elapsed_ms']:.0f}ms")
