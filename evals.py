#!/usr/bin/env python3
"""
MRARFAI Evaluation Framework v1.0
===================================
è‡ªåŠ¨åŒ–æµ‹è¯• Agent ç³»ç»Ÿè¾“å‡ºè´¨é‡

ä¸‰å±‚è¯„ä¼°ä½“ç³»ï¼š
  â‘  å·¥å…·æ­£ç¡®æ€§ â€” æ¯ä¸ªå·¥å…·çš„è¾“å…¥/è¾“å‡ºéªŒè¯
  â‘¡ Agent è¾“å‡ºè´¨é‡ â€” å‡†ç¡®æ€§/å®Œæ•´æ€§/å¯æ“ä½œæ€§/ä¸­æ–‡è´¨é‡
  â‘¢ ç«¯åˆ°ç«¯ç®¡çº¿ â€” å®Œæ•´é—®ç­”æµç¨‹çš„å»¶è¿Ÿ/æˆæœ¬/è´¨é‡

è¿è¡Œæ–¹å¼:
  python evals.py                    # è¿è¡Œå…¨éƒ¨ç¦»çº¿æµ‹è¯•
  python evals.py --tools            # ä»…å·¥å…·æµ‹è¯•
  python evals.py --agents           # Agent è´¨é‡æµ‹è¯• (éœ€ API Key)
  python evals.py --e2e              # ç«¯åˆ°ç«¯æµ‹è¯• (éœ€ API Key)
  python evals.py --report           # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import json
import time
import sys
import re
import statistics
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Callable, Any


# ============================================================
# Eval Result Types
# ============================================================

@dataclass
class EvalCase:
    """å•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
    id: str
    name: str
    category: str           # tools / agent_quality / e2e
    input_data: dict
    expected: dict           # æœŸæœ›è¾“å‡ºç‰¹å¾
    tags: List[str] = field(default_factory=list)


@dataclass
class EvalResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    case_id: str
    passed: bool
    score: float            # 0.0 - 1.0
    details: str = ""
    elapsed_ms: float = 0
    errors: List[str] = field(default_factory=list)


@dataclass
class EvalReport:
    """è¯„ä¼°æŠ¥å‘Š"""
    total: int = 0
    passed: int = 0
    failed: int = 0
    avg_score: float = 0
    results: List[EvalResult] = field(default_factory=list)
    by_category: Dict[str, dict] = field(default_factory=dict)
    elapsed_sec: float = 0

    def summary(self) -> str:
        lines = [
            f"{'='*55}",
            f"  MRARFAI Eval Report",
            f"{'='*55}",
            f"  Total: {self.total}  |  âœ… {self.passed}  |  âŒ {self.failed}  |  Score: {self.avg_score:.1%}",
            f"  Time: {self.elapsed_sec:.1f}s",
        ]
        for cat, stats in self.by_category.items():
            lines.append(f"  [{cat}] {stats['passed']}/{stats['total']} ({stats['score']:.0%})")
        lines.append(f"{'='*55}")
        if self.failed > 0:
            lines.append("  âŒ Failed cases:")
            for r in self.results:
                if not r.passed:
                    lines.append(f"    - {r.case_id}: {r.details}")
        return "\n".join(lines)


# ============================================================
# â‘  Tool Correctness Tests
# ============================================================

TOOL_TEST_CASES = [
    # ---- calc_yoy_growth ----
    EvalCase("T01", "YoY æ­£å¢é•¿", "tools",
             {"tool": "calc_yoy_growth", "args": {"current": 41.71, "previous": 27.07}},
             {"growth_pct_range": (54.0, 54.2), "delta_positive": True}),
    EvalCase("T02", "YoY è´Ÿå¢é•¿", "tools",
             {"tool": "calc_yoy_growth", "args": {"current": 20.0, "previous": 30.0}},
             {"growth_pct_range": (-33.4, -33.2), "delta_positive": False}),
    EvalCase("T03", "YoY é›¶åŸºæ•°", "tools",
             {"tool": "calc_yoy_growth", "args": {"current": 100, "previous": 0}},
             {"has_error_or_special": True}),

    # ---- calc_concentration ----
    EvalCase("T04", "é«˜é›†ä¸­åº¦", "tools",
             {"tool": "calc_concentration", "args": {"revenues": [
                 {"name": "A", "revenue": 800}, {"name": "B", "revenue": 100},
                 {"name": "C", "revenue": 50}, {"name": "D", "revenue": 30}, {"name": "E", "revenue": 20}]}},
             {"hhi_min": 2500, "top3_pct_min": 90}),
    EvalCase("T05", "ä½é›†ä¸­åº¦", "tools",
             {"tool": "calc_concentration", "args": {"revenues": [
                 {"name": "A", "revenue": 100}, {"name": "B", "revenue": 95},
                 {"name": "C", "revenue": 90}, {"name": "D", "revenue": 85},
                 {"name": "E", "revenue": 80}, {"name": "F", "revenue": 75}]}},
             {"hhi_max": 2500}),

    # ---- detect_churn_risk ----
    EvalCase("T06", "é«˜æµå¤±é£é™©-è¿ç»­ä¸‹é™", "tools",
             {"tool": "detect_churn_risk", "args": {
                 "client_name": "TestHigh",
                 "monthly_values": [100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 5, 0]}},
             {"risk_level_in": ["æé«˜", "é«˜"]}),
    EvalCase("T07", "ä½æµå¤±é£é™©-ç¨³å®šå®¢æˆ·", "tools",
             {"tool": "detect_churn_risk", "args": {
                 "client_name": "TestLow",
                 "monthly_values": [100, 102, 98, 105, 100, 103, 99, 101, 104, 100, 102, 105]}},
             {"risk_level_in": ["ä½"]}),
    EvalCase("T08", "H2æ–­å´–å¼ä¸‹è·Œ", "tools",
             {"tool": "detect_churn_risk", "args": {
                 "client_name": "TestCliff",
                 "monthly_values": [200, 210, 205, 215, 200, 195, 50, 30, 20, 10, 5, 0]}},
             {"risk_level_in": ["æé«˜", "é«˜"]}),

    # ---- analyze_product_mix ----
    EvalCase("T09", "äº§å“BCGåˆ†ç±»", "tools",
             {"tool": "analyze_product_mix", "args": {"products": [
                 {"name": "æ‰‹æœº", "current": 3000, "previous": 2000},
                 {"name": "IoT", "current": 500, "previous": 200},
                 {"name": "å¹³æ¿", "current": 100, "previous": 150}]}},
             {"has_star": True, "total_gt": 3000}),

    # ---- analyze_monthly_trend ----
    EvalCase("T10", "ä¸Šå‡è¶‹åŠ¿", "tools",
             {"tool": "analyze_monthly_trend", "args": {
                 "monthly_values": [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65]}},
             {"peak_month": "12æœˆ", "trough_month": "1æœˆ", "h2_gt_h1": True}),

    # ---- scan_all_risks ----
    EvalCase("T11", "æ‰¹é‡é£é™©æ‰«æ", "tools",
             {"tool": "scan_all_risks", "args": {"client_data": [
                 {"name": "ç¨³å®šA", "monthly_values": [100]*12},
                 {"name": "å±é™©B", "monthly_values": [100, 90, 80, 70, 60, 50, 40, 30, 20, 10, 5, 0]},
                 {"name": "ç¨³å®šC", "monthly_values": [50]*12}]}},
             {"has_high_risk": True, "total_clients": 3}),

    # ---- format_number ----
    EvalCase("T12", "äº¿å…ƒæ ¼å¼åŒ–", "tools",
             {"tool": "format_number", "args": {"value": 41234.5}},
             {"contains": "äº¿"}),
    EvalCase("T13", "ä¸‡å…ƒæ ¼å¼åŒ–", "tools",
             {"tool": "format_number", "args": {"value": 5432.1, "unit": "ä¸‡å…ƒ"}},
             {"contains": "ä¸‡"}),

    # ---- edge cases ----
    EvalCase("T14", "ç©ºåˆ—è¡¨è¾“å…¥", "tools",
             {"tool": "calc_concentration", "args": {"revenues": []}},
             {"handles_empty": True}),
    EvalCase("T15", "æœªçŸ¥å·¥å…·", "tools",
             {"tool": "nonexistent", "args": {}},
             {"has_error": True}),
]


def run_tool_eval(case: EvalCase) -> EvalResult:
    """æ‰§è¡Œå•ä¸ªå·¥å…·æµ‹è¯•"""
    from tool_registry import sales_tools

    t0 = time.time()
    tool_name = case.input_data["tool"]
    args = case.input_data["args"]

    try:
        result = sales_tools.execute(tool_name, args)
    except Exception as e:
        elapsed = (time.time() - t0) * 1000
        # å¦‚æœæœŸæœ›é”™è¯¯ï¼Œè¿™ä¹Ÿç®—é€šè¿‡
        if case.expected.get("has_error") or case.expected.get("has_error_or_special"):
            return EvalResult(case.id, True, 1.0, "Expected error caught", elapsed)
        return EvalResult(case.id, False, 0.0, f"Exception: {e}", elapsed)

    elapsed = (time.time() - t0) * 1000
    errors = []
    score = 1.0
    exp = case.expected
    r = result.get("result", result)

    # Check: has_error
    if exp.get("has_error"):
        if "error" not in result:
            errors.append("Expected error but got success")
        return EvalResult(case.id, len(errors) == 0, 1.0 if not errors else 0.0,
                         "; ".join(errors) if errors else "OK", elapsed, errors)

    # Check: has_error_or_special (zero base etc)
    if exp.get("has_error_or_special"):
        # Accept either error or special value (like "N/A" or very high number)
        return EvalResult(case.id, True, 1.0, "Special case handled", elapsed)

    # Check: handles_empty
    if exp.get("handles_empty"):
        if "error" in result:
            return EvalResult(case.id, True, 1.0, "Empty handled with error", elapsed)
        return EvalResult(case.id, True, 0.8, "Empty handled without error", elapsed)

    # Check: growth_pct_range
    if "growth_pct_range" in exp:
        lo, hi = exp["growth_pct_range"]
        actual = r.get("growth_pct", 0)
        if not (lo <= actual <= hi):
            errors.append(f"growth_pct {actual} not in [{lo}, {hi}]")
            score -= 0.5

    # Check: delta_positive
    if "delta_positive" in exp:
        delta = r.get("delta", 0)
        if exp["delta_positive"] and delta <= 0:
            errors.append(f"Expected positive delta, got {delta}")
            score -= 0.3
        elif not exp["delta_positive"] and delta >= 0:
            errors.append(f"Expected negative delta, got {delta}")
            score -= 0.3

    # Check: hhi
    if "hhi_min" in exp:
        if r.get("hhi", 0) < exp["hhi_min"]:
            errors.append(f"HHI {r.get('hhi')} < {exp['hhi_min']}")
            score -= 0.5
    if "hhi_max" in exp:
        if r.get("hhi", 0) > exp["hhi_max"]:
            errors.append(f"HHI {r.get('hhi')} > {exp['hhi_max']}")
            score -= 0.5

    # Check: top3
    if "top3_pct_min" in exp:
        if r.get("top3_pct", 0) < exp["top3_pct_min"]:
            errors.append(f"top3 {r.get('top3_pct')}% < {exp['top3_pct_min']}%")
            score -= 0.3

    # Check: risk_level
    if "risk_level_in" in exp:
        if r.get("risk_level") not in exp["risk_level_in"]:
            errors.append(f"risk_level '{r.get('risk_level')}' not in {exp['risk_level_in']}")
            score -= 0.5

    # Check: BCG star
    if exp.get("has_star"):
        prods = r.get("products", [])
        has = any("æ˜æ˜Ÿ" in str(p.get("bcg_quadrant", p.get("category", ""))) for p in prods)
        if not has:
            errors.append("No æ˜æ˜Ÿ product found")
            score -= 0.3
    if "total_gt" in exp:
        if r.get("total_revenue", 0) <= exp["total_gt"]:
            errors.append(f"total {r.get('total_revenue')} <= {exp['total_gt']}")
            score -= 0.2

    # Check: trend
    if "peak_month" in exp:
        if r.get("peak", {}).get("month") != exp["peak_month"]:
            errors.append(f"peak month {r.get('peak',{}).get('month')} != {exp['peak_month']}")
            score -= 0.3
    if "trough_month" in exp:
        if r.get("trough", {}).get("month") != exp["trough_month"]:
            errors.append(f"trough month {r.get('trough',{}).get('month')} != {exp['trough_month']}")
            score -= 0.3
    if exp.get("h2_gt_h1"):
        h2 = r.get("h2_avg", r.get("h2_total", 0))
        h1 = r.get("h1_avg", r.get("h1_total", 0))
        if h2 <= h1:
            errors.append("H2 not > H1")
            score -= 0.2

    # Check: batch risk
    if exp.get("has_high_risk"):
        clients = r.get("clients", [])
        has_hr = any(c.get("risk_level") in ("æé«˜", "é«˜") for c in clients)
        if not has_hr:
            errors.append("No high-risk client found in batch scan")
            score -= 0.5
    if "total_clients" in exp:
        if len(r.get("clients", [])) != exp["total_clients"]:
            errors.append(f"client count {len(r.get('clients',[]))} != {exp['total_clients']}")
            score -= 0.2

    # Check: contains
    if "contains" in exp:
        result_str = str(r)
        if exp["contains"] not in result_str:
            errors.append(f"'{exp['contains']}' not found in output")
            score -= 0.5

    score = max(0.0, score)
    passed = len(errors) == 0
    return EvalResult(case.id, passed, score,
                     "; ".join(errors) if errors else "OK", elapsed, errors)


# ============================================================
# â‘¡ Agent Output Quality Tests
# ============================================================

QUALITY_TEST_CASES = [
    EvalCase("Q01", "è¥æ”¶åŒæ¯”é—®é¢˜", "agent_quality",
             {"question": "ä»Šå¹´æ€»è¥æ”¶å’Œå»å¹´æ¯”æ€ä¹ˆæ ·ï¼Ÿ",
              "mock_data": "2024å¹´æ€»è¥æ”¶41.71äº¿å…ƒï¼Œ2023å¹´æ€»è¥æ”¶27.07äº¿å…ƒ"},
             {"must_contain_number": True, "must_chinese": True,
              "min_length": 50, "should_mention": ["å¢é•¿", "åŒæ¯”"]}),

    EvalCase("Q02", "å®¢æˆ·æµå¤±é£é™©", "agent_quality",
             {"question": "å“ªäº›å®¢æˆ·æœ‰æµå¤±é£é™©ï¼Ÿ",
              "mock_data": "å®¢æˆ·A: 1-12æœˆå‡ºè´§[100,95,90,80,60,40,20,10,5,0,0,0]\nå®¢æˆ·B: ç¨³å®šå‡ºè´§[50]*12"},
             {"must_contain_number": False, "must_chinese": True,
              "min_length": 50, "should_mention": ["å®¢æˆ·A", "é£é™©"]}),

    EvalCase("Q03", "CEOç®€æŠ¥", "agent_quality",
             {"question": "CEOæœ¬æœˆè¯¥å…³æ³¨ä»€ä¹ˆï¼Ÿ",
              "mock_data": "æœ¬æœˆè¥æ”¶3.5äº¿ï¼Œç¯æ¯”-5%ï¼Œå®¢æˆ·Aæµå¤±é£é™©é«˜ï¼ŒIoTæ–°å“å¢é•¿120%"},
             {"must_chinese": True, "min_length": 80,
              "should_mention": ["å»ºè®®"]}),

    EvalCase("Q04", "äº§å“ç»“æ„åˆ†æ", "agent_quality",
             {"question": "å„äº§å“çº¿è¡¨ç°å¦‚ä½•ï¼Ÿ",
              "mock_data": "æ‰‹æœºODM: 30äº¿(+50%), IoT: 5äº¿(+150%), å¹³æ¿: 2äº¿(-20%), å¯ç©¿æˆ´: 1äº¿(+80%)"},
             {"must_chinese": True, "min_length": 60,
              "should_mention": ["æ‰‹æœº", "IoT"]}),
]


def eval_agent_output(output: str, expected: dict) -> EvalResult:
    """è¯„ä¼° Agent è¾“å‡ºè´¨é‡ï¼ˆç¦»çº¿ï¼ŒåŸºäºè§„åˆ™ï¼‰"""
    errors = []
    score = 1.0

    # é•¿åº¦æ£€æŸ¥
    min_len = expected.get("min_length", 20)
    if len(output) < min_len:
        errors.append(f"è¾“å‡ºè¿‡çŸ­: {len(output)} < {min_len}")
        score -= 0.3

    # ä¸­æ–‡æ£€æŸ¥
    if expected.get("must_chinese"):
        cn_chars = len(re.findall(r'[\u4e00-\u9fff]', output))
        if cn_chars < 5:
            errors.append(f"ä¸­æ–‡å­—ç¬¦ä¸è¶³: {cn_chars}")
            score -= 0.3

    # æ•°å­—æ£€æŸ¥
    if expected.get("must_contain_number"):
        nums = re.findall(r'\d+\.?\d*', output)
        if not nums:
            errors.append("ç¼ºå°‘æ•°å­—æ•°æ®")
            score -= 0.2

    # å…³é”®è¯æ£€æŸ¥
    for keyword in expected.get("should_mention", []):
        if keyword not in output:
            errors.append(f"ç¼ºå°‘å…³é”®ä¿¡æ¯: '{keyword}'")
            score -= 0.15

    # é”™è¯¯æ¶ˆæ¯æ£€æµ‹
    if any(tag in output for tag in ["è°ƒç”¨å¤±è´¥", "error", "Exception", "è¶…æ—¶"]):
        errors.append("è¾“å‡ºåŒ…å«é”™è¯¯ä¿¡æ¯")
        score -= 0.5

    score = max(0.0, score)
    return EvalResult("", len(errors) == 0, score,
                     "; ".join(errors) if errors else "OK", 0, errors)


# ============================================================
# â‘¢ End-to-End Pipeline Tests
# ============================================================

E2E_TEST_CASES = [
    EvalCase("E01", "åŸºç¡€è¥æ”¶é—®é¢˜", "e2e",
             {"question": "ä»Šå¹´æ€»è¥æ”¶å¤šå°‘ï¼Ÿ"},
             {"max_latency_sec": 30, "min_agents": 1, "has_answer": True}),
    EvalCase("E02", "å¤šAgenté£é™©é—®é¢˜", "e2e",
             {"question": "å“ªäº›å®¢æˆ·æœ‰æµå¤±é£é™©ï¼ŸCEOè¯¥æ€ä¹ˆåº”å¯¹ï¼Ÿ"},
             {"max_latency_sec": 45, "min_agents": 2, "has_answer": True}),
    EvalCase("E03", "å…¨Agenté—®é¢˜", "e2e",
             {"question": "è¯·åšä¸€ä¸ªå®Œæ•´çš„é”€å”®åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬å¢é•¿ã€é£é™©å’Œæˆ˜ç•¥å»ºè®®"},
             {"max_latency_sec": 60, "min_agents": 3, "has_answer": True}),
]


# ============================================================
# MCP Protocol Tests
# ============================================================

MCP_TEST_CASES = [
    EvalCase("M01", "MCP initialize", "mcp",
             {"method": "initialize", "params": {"clientInfo": {"name": "test", "version": "1.0"},
              "protocolVersion": "2025-06-18"}},
             {"has_protocol_version": True, "has_capabilities": True}),
    EvalCase("M02", "MCP tools/list", "mcp",
             {"method": "tools/list", "params": {}},
             {"min_tools": 8}),
    EvalCase("M03", "MCP tools/call", "mcp",
             {"method": "tools/call", "params": {
                 "name": "calc_yoy_growth", "arguments": {"current": 100, "previous": 80}}},
             {"not_error": True, "has_content": True}),
    EvalCase("M04", "MCP resources/list", "mcp",
             {"method": "resources/list", "params": {}},
             {"min_resources": 3}),
    EvalCase("M05", "MCP resources/read", "mcp",
             {"method": "resources/read", "params": {"uri": "mrarfai://tools/catalog"}},
             {"has_contents": True}),
    EvalCase("M06", "MCP prompts/list", "mcp",
             {"method": "prompts/list", "params": {}},
             {"min_prompts": 3}),
    EvalCase("M07", "MCP prompts/get", "mcp",
             {"method": "prompts/get", "params": {"name": "sales-overview", "arguments": {"period": "2024"}}},
             {"has_messages": True}),
    EvalCase("M08", "MCP unknown method", "mcp",
             {"method": "nonexistent/method", "params": {}},
             {"is_error": True}),
    EvalCase("M09", "MCP ping", "mcp",
             {"method": "ping", "params": {}},
             {"is_success": True}),
]


def run_mcp_eval(case: EvalCase) -> EvalResult:
    """æ‰§è¡Œ MCP åè®®æµ‹è¯•"""
    from mcp_server import MCPHandler

    handler = MCPHandler()
    t0 = time.time()

    request = {
        "jsonrpc": "2.0",
        "id": case.id,
        "method": case.input_data["method"],
        "params": case.input_data.get("params", {}),
    }

    try:
        response = handler.handle(request)
    except Exception as e:
        elapsed = (time.time() - t0) * 1000
        return EvalResult(case.id, False, 0.0, f"Exception: {e}", elapsed)

    elapsed = (time.time() - t0) * 1000
    errors = []
    exp = case.expected

    if response is None:
        errors.append("No response")
        return EvalResult(case.id, False, 0.0, "No response", elapsed, errors)

    result = response.get("result", {})
    error = response.get("error")

    if exp.get("is_error"):
        if not error:
            errors.append("Expected error response")
        return EvalResult(case.id, bool(error), 1.0 if error else 0.0,
                         "OK" if error else "Expected error", elapsed, errors)

    if exp.get("is_success"):
        if error:
            errors.append(f"Unexpected error: {error}")
        return EvalResult(case.id, not error, 1.0 if not error else 0.0,
                         "OK" if not error else str(error), elapsed, errors)

    if error:
        return EvalResult(case.id, False, 0.0, f"Error: {error}", elapsed, [str(error)])

    # Specific checks
    if exp.get("has_protocol_version"):
        if "protocolVersion" not in result:
            errors.append("Missing protocolVersion")
    if exp.get("has_capabilities"):
        if "capabilities" not in result:
            errors.append("Missing capabilities")
    if "min_tools" in exp:
        tools = result.get("tools", [])
        if len(tools) < exp["min_tools"]:
            errors.append(f"tools count {len(tools)} < {exp['min_tools']}")
    if exp.get("not_error"):
        if result.get("isError"):
            errors.append("Tool returned error")
    if exp.get("has_content"):
        if not result.get("content"):
            errors.append("Missing content")
    if "min_resources" in exp:
        if len(result.get("resources", [])) < exp["min_resources"]:
            errors.append("Insufficient resources")
    if exp.get("has_contents"):
        if not result.get("contents"):
            errors.append("Missing contents")
    if "min_prompts" in exp:
        if len(result.get("prompts", [])) < exp["min_prompts"]:
            errors.append("Insufficient prompts")
    if exp.get("has_messages"):
        if not result.get("messages"):
            errors.append("Missing messages")

    score = max(0.0, 1.0 - len(errors) * 0.3)
    return EvalResult(case.id, len(errors) == 0, score,
                     "; ".join(errors) if errors else "OK", elapsed, errors)


# ============================================================
# Guardrails Tests
# ============================================================

GUARD_TEST_CASES = [
    EvalCase("G01", "CircuitBreaker çŠ¶æ€æœº", "guardrails",
             {"test": "breaker_lifecycle"}, {"passes": True}),
    EvalCase("G02", "Output Validation æ­£å¸¸", "guardrails",
             {"test": "validate_normal"}, {"passes": True}),
    EvalCase("G03", "Output Validation é”™è¯¯", "guardrails",
             {"test": "validate_error"}, {"passes": True}),
    EvalCase("G04", "Response Cache", "guardrails",
             {"test": "cache_hit_miss"}, {"passes": True}),
    EvalCase("G05", "Token Budget åˆ†çº§", "guardrails",
             {"test": "budget_levels"}, {"passes": True}),
    EvalCase("G06", "Fallback Chain", "guardrails",
             {"test": "fallback_execution"}, {"passes": True}),
    EvalCase("G07", "JSON å®‰å…¨è§£æ", "guardrails",
             {"test": "safe_json_parse"}, {"passes": True}),
]


def run_guard_eval(case: EvalCase) -> EvalResult:
    """æ‰§è¡Œ Guardrails æµ‹è¯•"""
    from guardrails import (
        get_breaker, CircuitState, CircuitBreakerOpenError,
        validate_agent_output, get_cache, get_budget, TokenBudget,
        FallbackChain, safe_parse_llm_json,
    )
    import time as _t

    t0 = _t.time()
    test_name = case.input_data["test"]
    errors = []

    try:
        if test_name == "breaker_lifecycle":
            cb = get_breaker(f"eval_{case.id}", fail_max=2, reset_timeout=0.5)
            assert cb.state == CircuitState.CLOSED
            for _ in range(2):
                try:
                    cb.call(lambda: (_ for _ in ()).throw(ConnectionError()))
                except ConnectionError:
                    pass
            assert cb.state == CircuitState.OPEN, f"Expected OPEN, got {cb.state}"
            try:
                cb.call(lambda: "x")
                errors.append("Should reject in OPEN")
            except CircuitBreakerOpenError:
                pass
            _t.sleep(0.6)
            assert cb.state == CircuitState.HALF_OPEN
            cb.call(lambda: "ok")
            assert cb.state == CircuitState.CLOSED

        elif test_name == "validate_normal":
            v = validate_agent_output("ç¦¾è‹—2024å¹´æ€»è¥æ”¶è¾¾åˆ°41.71äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿54.1%ã€‚æ‰‹æœºODMè´¡çŒ®æœ€å¤§ã€‚")
            assert v.passed, f"Should pass: {v.issues}"
            assert v.confidence >= 0.8

        elif test_name == "validate_error":
            v = validate_agent_output("[è°ƒç”¨å¤±è´¥: timeout]")
            assert not v.passed
            assert v.confidence < 0.5

        elif test_name == "cache_hit_miss":
            cache = get_cache()
            cache.put(f"eval_{case.id}", {"answer": "test"})
            assert cache.get(f"eval_{case.id}") is not None
            assert cache.get("nonexistent_key_xyz") is None

        elif test_name == "budget_levels":
            b = TokenBudget(daily_budget_usd=1.0)
            assert b.check_budget()["level"] == "normal"
            b.record_cost(0.55, "t1")
            assert b.check_budget()["level"] == "caution"
            b.record_cost(0.30, "t2")
            assert b.check_budget()["level"] == "warning"
            b.record_cost(0.10, "t3")
            assert b.check_budget()["level"] == "critical"

        elif test_name == "fallback_execution":
            chain = FallbackChain("eval")
            chain.add(lambda: (_ for _ in ()).throw(Exception("f1")), "L1")
            chain.add(lambda: "ok", "L2")
            result, level = chain.execute()
            assert result == "ok" and level == "L2"

        elif test_name == "safe_json_parse":
            assert safe_parse_llm_json('{"a":1}') == {"a": 1}
            assert safe_parse_llm_json('```json\n{"a":1}\n```') == {"a": 1}
            assert safe_parse_llm_json("not json") is None

    except AssertionError as e:
        errors.append(str(e))
    except Exception as e:
        errors.append(f"Exception: {e}")

    elapsed = (_t.time() - t0) * 1000
    passed = len(errors) == 0
    return EvalResult(case.id, passed, 1.0 if passed else 0.0,
                     "; ".join(errors) if errors else "OK", elapsed, errors)


# ============================================================
# Runner
# ============================================================

# ============================================================
# â‘£ RAG Engine Tests
# ============================================================

RAG_TEST_CASES = [
    EvalCase("R01", "æ–‡æœ¬å¯¼å…¥", "rag",
             {"action": "ingest_text"}, {"chunks_min": 1}),
    EvalCase("R02", "å¤šæ–‡æ¡£å¯¼å…¥", "rag",
             {"action": "ingest_multi"}, {"total_chunks_min": 3}),
    EvalCase("R03", "å‘é‡æ£€ç´¢", "rag",
             {"action": "vector_search"}, {"has_results": True}),
    EvalCase("R04", "BM25æ£€ç´¢", "rag",
             {"action": "bm25_search"}, {"top1_source": "contract"}),
    EvalCase("R05", "æ··åˆæ£€ç´¢ç²¾åº¦", "rag",
             {"action": "precision_test"}, {"precision_min": 0.5}),
    EvalCase("R06", "Contextæ„å»º", "rag",
             {"action": "build_context"}, {"min_length": 50}),
    EvalCase("R07", "Enriché›†æˆ", "rag",
             {"action": "enrich"}, {"has_both": True}),
    EvalCase("R08", "ç©ºåº“æ£€ç´¢", "rag",
             {"action": "search_empty"}, {"empty": True}),
    EvalCase("R09", "Statsç»Ÿè®¡", "rag",
             {"action": "stats"}, {"has_sources": True}),
]


def _make_test_rag():
    """åˆ›å»ºé¢„å¡«å……çš„æµ‹è¯• RAG å®ä¾‹"""
    from rag_engine import RAGEngine
    rag = RAGEngine(chunk_size=200, chunk_overlap=40)
    rag.ingest_text(
        "HMDåŠŸèƒ½æœºåŸºå‡†ä»·æ ¼8.5ç¾å…ƒFOBæ·±åœ³ï¼Œæ™ºèƒ½æœº35è‡³65ç¾å…ƒã€‚"
        "å­£åº¦è°ƒæ•´æœºåˆ¶å…è®¸æ­£è´Ÿ5%æµ®åŠ¨ã€‚ä»˜æ¬¾æ¡ä»¶T/T 60å¤©æœˆç»“ã€‚"
        "HMDæ‰¿è¯ºæœ€ä½é‡‡è´­500ä¸‡å°ï¼ŒåŠŸèƒ½æœº350ä¸‡å°ï¼Œæ™ºèƒ½æœº150ä¸‡å°ã€‚",
        source="contract.pdf")
    rag.ingest_text(
        "HMDè¡¨ç¤º2026å¹´ç¼©å‡åŠŸèƒ½æœºäº§å“çº¿ï¼Œä»12ä¸ªå‹å·å‡è‡³6åˆ°8ä¸ªã€‚"
        "åå‹¤åœ¨ä»·æ ¼ä¸Šæ›´æœ‰ä¼˜åŠ¿ã€‚HMDå¯¹CKDæ•£ä»¶éœ€æ±‚å°†ä¸‹é™ï¼Œ"
        "å› ä¸ºå°åº¦æœ¬åœ°ç»„è£…èƒ½åŠ›åœ¨æå‡ã€‚Action ItemsåŒ…æ‹¬æäº¤Android GoæŠ¥ä»·ã€‚",
        source="meeting.md")
    rag.ingest_text(
        "å…¨çƒåŠŸèƒ½æœºå‡ºè´§7.2äº¿å°åŒæ¯”ä¸‹é™12%ã€‚åå‹¤å¸‚å ç‡35%ç¨³å®šã€‚"
        "é—»æ³°22%è½¬å‘æ±½è½¦ç”µå­ã€‚é¾™æ——18%ã€‚ç¦¾è‹—çº¦5%ã€‚"
        "å°åº¦å¸‚åœºåŠ é€Ÿèç¼©1.1äº¿å°ä¸‹é™19%ã€‚2026å¹´é¢„æµ‹ç»§ç»­èç¼©10åˆ°15%ã€‚",
        source="report.pdf")
    return rag


def run_rag_eval(case: EvalCase) -> EvalResult:
    """æ‰§è¡ŒRAGæµ‹è¯•"""
    from rag_engine import RAGEngine, BM25Index, DocChunk, enrich_context_with_rag

    t0 = time.time()
    action = case.input_data["action"]
    exp = case.expected
    errors = []

    try:
        if action == "ingest_text":
            rag = RAGEngine(chunk_size=200)
            rag.chunker.min_chunk_size = 30
            n = rag.ingest_text(
                "ç¦¾è‹—ä¸HMDåˆåŒï¼šåŠŸèƒ½æœºåŸºå‡†ä»·æ ¼8.5ç¾å…ƒFOBæ·±åœ³ï¼Œæ™ºèƒ½æœº35è‡³65ç¾å…ƒåŒºé—´ã€‚"
                "æœ€ä½é‡‡è´­é‡500ä¸‡å°ï¼Œå…¶ä¸­åŠŸèƒ½æœº350ä¸‡å°ï¼Œä»˜æ¬¾æ¡ä»¶T/T 60å¤©æœˆç»“ã€‚",
                source="contract"
            )
            if n < exp["chunks_min"]:
                errors.append(f"chunks={n} < {exp['chunks_min']}")

        elif action == "ingest_multi":
            rag = _make_test_rag()
            if rag.vector_store.size() < exp["total_chunks_min"]:
                errors.append(f"total={rag.vector_store.size()} < {exp['total_chunks_min']}")

        elif action == "vector_search":
            rag = _make_test_rag()
            results = rag.search("åŠŸèƒ½æœºä»·æ ¼", top_k=3)
            if not results:
                errors.append("No vector search results")

        elif action == "bm25_search":
            bm25 = BM25Index()
            bm25.add([
                DocChunk("c1", "HMDåˆåŒä»·æ ¼8.5ç¾å…ƒåŠŸèƒ½æœºä»˜æ¬¾T/T60å¤©", "contract", "txt"),
                DocChunk("c2", "åå‹¤å¸‚å ç‡35%å…¨çƒåŠŸèƒ½æœºèç¼©12%", "report", "txt"),
            ])
            results = bm25.search("HMDåˆåŒä»·æ ¼8.5ç¾å…ƒ", top_k=2)
            if not results:
                errors.append("No BM25 results")
            elif exp.get("top1_source") and exp["top1_source"] not in results[0][0].source:
                errors.append(f"BM25 top1={results[0][0].source}, expected {exp['top1_source']}")

        elif action == "precision_test":
            rag = _make_test_rag()
            tests = [
                ("åˆåŒä»·æ ¼8.5ç¾å…ƒ", "contract"),
                ("åŠŸèƒ½æœºèç¼©12%", "report"),
                ("CKDéœ€æ±‚ä¸‹é™", "meeting"),
                ("å°åº¦å¸‚åœºèç¼©", "report"),
            ]
            correct = sum(
                1 for q, prefix in tests
                if (r := rag.search(q, top_k=1)) and prefix in r[0].chunk.source
            )
            precision = correct / len(tests)
            if precision < exp["precision_min"]:
                errors.append(f"precision={precision:.0%} < {exp['precision_min']:.0%}")

        elif action == "build_context":
            rag = _make_test_rag()
            ctx = rag.build_context("HMDè®¢å•æƒ…å†µ", max_tokens=1000)
            if len(ctx) < exp["min_length"]:
                errors.append(f"context len={len(ctx)} < {exp['min_length']}")

        elif action == "enrich":
            rag = _make_test_rag()
            structured = "ã€æ€»è¥æ”¶ã€‘41.71äº¿å…ƒï¼ŒåŒæ¯”+54.1%"
            enriched = enrich_context_with_rag("HMDä»·æ ¼", structured, rag)
            if "41.71" not in enriched:
                errors.append("Missing structured data")
            if "å‚è€ƒ" not in enriched:
                errors.append("Missing RAG context")

        elif action == "search_empty":
            rag = RAGEngine()
            results = rag.search("test")
            if results:
                errors.append(f"Expected empty, got {len(results)} results")

        elif action == "stats":
            rag = _make_test_rag()
            stats = rag.get_stats()
            if not stats.get("sources"):
                errors.append("No sources in stats")

    except Exception as e:
        errors.append(f"Exception: {e}")

    elapsed = (time.time() - t0) * 1000
    passed = len(errors) == 0
    return EvalResult(
        case.id, passed, 1.0 if passed else 0.0,
        "OK" if passed else "; ".join(errors), elapsed, errors
    )


def run_eval_suite(
    categories: List[str] = None,
    verbose: bool = True,
) -> EvalReport:
    """è¿è¡Œè¯„ä¼°å¥—ä»¶"""
    categories = categories or ["tools", "mcp", "guardrails", "rag"]

    all_cases = []
    if "tools" in categories:
        all_cases.extend(TOOL_TEST_CASES)
    if "mcp" in categories:
        all_cases.extend(MCP_TEST_CASES)
    if "guardrails" in categories:
        all_cases.extend(GUARD_TEST_CASES)
    if "rag" in categories:
        all_cases.extend(RAG_TEST_CASES)
    if "agent_quality" in categories:
        all_cases.extend(QUALITY_TEST_CASES)

    report = EvalReport()
    t0 = time.time()

    for case in all_cases:
        if verbose:
            sys.stdout.write(f"  {case.id} {case.name}...")
            sys.stdout.flush()

        if case.category == "tools":
            result = run_tool_eval(case)
        elif case.category == "mcp":
            result = run_mcp_eval(case)
        elif case.category == "guardrails":
            result = run_guard_eval(case)
        elif case.category == "rag":
            result = run_rag_eval(case)
        elif case.category == "agent_quality":
            # Agent quality needs mock â€” just validate the checker
            mock_output = case.input_data.get("mock_data", "")
            result = eval_agent_output(mock_output, case.expected)
            result.case_id = case.id
        else:
            result = EvalResult(case.id, False, 0.0, f"Unknown category: {case.category}")

        result.case_id = case.id
        report.results.append(result)

        if verbose:
            status = "âœ…" if result.passed else "âŒ"
            print(f" {status} ({result.score:.0%}) {result.details[:60]}")

        # Category stats
        cat = case.category
        if cat not in report.by_category:
            report.by_category[cat] = {"total": 0, "passed": 0, "scores": []}
        report.by_category[cat]["total"] += 1
        if result.passed:
            report.by_category[cat]["passed"] += 1
        report.by_category[cat]["scores"].append(result.score)

    report.total = len(report.results)
    report.passed = sum(1 for r in report.results if r.passed)
    report.failed = report.total - report.passed
    all_scores = [r.score for r in report.results]
    report.avg_score = statistics.mean(all_scores) if all_scores else 0
    report.elapsed_sec = time.time() - t0

    for cat, stats in report.by_category.items():
        stats["score"] = statistics.mean(stats["scores"]) if stats["scores"] else 0

    return report


# ============================================================
# CLI
# ============================================================

def main():
    import argparse
    parser = argparse.ArgumentParser(description="MRARFAI Evaluation Framework")
    parser.add_argument("--tools", action="store_true", help="Run tool tests only")
    parser.add_argument("--mcp", action="store_true", help="Run MCP protocol tests only")
    parser.add_argument("--guardrails", action="store_true", help="Run guardrails tests only")
    parser.add_argument("--rag", action="store_true", help="Run RAG engine tests only")
    parser.add_argument("--agents", action="store_true", help="Run agent quality tests")
    parser.add_argument("--all", action="store_true", help="Run all offline tests")
    parser.add_argument("--report", action="store_true", help="Print detailed report")
    args = parser.parse_args()

    cats = []
    if args.tools:
        cats.append("tools")
    if args.mcp:
        cats.append("mcp")
    if args.guardrails:
        cats.append("guardrails")
    if args.rag:
        cats.append("rag")
    if args.agents:
        cats.append("agent_quality")
    if args.all or not cats:
        cats = ["tools", "mcp", "guardrails", "rag"]

    print(f"\nğŸ§ª MRARFAI Eval â€” Categories: {', '.join(cats)}\n")
    report = run_eval_suite(cats, verbose=True)
    print(report.summary())

    if args.report:
        # Export JSON report
        report_data = {
            "total": report.total,
            "passed": report.passed,
            "failed": report.failed,
            "avg_score": report.avg_score,
            "elapsed_sec": report.elapsed_sec,
            "by_category": {k: {"total": v["total"], "passed": v["passed"],
                                "score": v["score"]} for k, v in report.by_category.items()},
            "results": [{"case_id": r.case_id, "passed": r.passed, "score": r.score,
                         "details": r.details, "elapsed_ms": r.elapsed_ms}
                        for r in report.results],
        }
        with open("eval_report.json", "w") as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ Report saved: eval_report.json")

    sys.exit(0 if report.failed == 0 else 1)


if __name__ == "__main__":
    main()
