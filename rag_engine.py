#!/usr/bin/env python3
"""
MRARFAI RAG Engine v1.0
=========================
éç»“æ„åŒ–æ–‡æ¡£ç†è§£ â€” å‘é‡æ£€ç´¢ + å…³é”®è¯æ··åˆæœç´¢

è¦†ç›–åœºæ™¯ (80% â†’ 95%):
  - ğŸ“„ åˆåŒ/åè®® â†’ æå–æ¡æ¬¾ã€äº¤ä»˜æ—¶é—´ã€ä»˜æ¬¾æ¡ä»¶
  - ğŸ“§ é‚®ä»¶/ä¼šè®®çºªè¦ â†’ å®¢æˆ·æ„å›¾ã€action items
  - ğŸ“Š è¡Œä¸šæŠ¥å‘Š â†’ å¸‚åœºæ•°æ®ã€ç«å“åŠ¨æ€
  - ğŸ“ å†…éƒ¨å¤‡å¿˜ â†’ å†å²å†³ç­–ã€ä¸Šä¸‹æ–‡

æ¶æ„:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Doc Loader  â”‚ â†’  â”‚ Chunker  â”‚ â†’  â”‚  Embedder   â”‚
  â”‚ PDF/TXT/MD  â”‚    â”‚ æ™ºèƒ½åˆ†æ®µ  â”‚    â”‚ API/TF-IDF  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                            â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          VectorStore (numpy/chromadb)    â”‚
  â”‚  + BM25 å…³é”®è¯ç´¢å¼• (æ··åˆæ£€ç´¢)            â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚     HybridRetriever â†’ ContextBuilder     â”‚
  â”‚     â†’ æ³¨å…¥ Agent Prompt                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é›¶å¤–éƒ¨ä¾èµ–ï¼ˆnumpy onlyï¼‰ï¼Œå¯é€‰ chromadb/faiss åŠ é€Ÿ

Usage:
    rag = RAGEngine()
    rag.ingest_file("contract.pdf")
    rag.ingest_file("meeting_notes.txt")
    rag.ingest_text("Q3å®¢æˆ·æ‹œè®¿çºªè¦ï¼šHMDè¡¨ç¤ºå°†ç¼©å‡ODMè®¢å•...", source="sales_memo")

    # æ£€ç´¢
    results = rag.search("HMDè®¢å•ç¼©å‡çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ", top_k=5)

    # æ„å»ºAgentä¸Šä¸‹æ–‡
    context = rag.build_context("HMDæœ€è¿‘çš„è®¢å•æƒ…å†µ", max_tokens=2000)

    # é›†æˆåˆ° multi_agent.py
    combined = structured_data + "\\n\\n" + context
"""

import os
import re
import json
import math
import time
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from collections import Counter

import numpy as np

logger = logging.getLogger("mrarfai.rag")


# ============================================================
# Data Models
# ============================================================

@dataclass
class DocChunk:
    """æ–‡æ¡£åˆ†å—"""
    chunk_id: str
    text: str
    source: str             # æ–‡ä»¶åæˆ–æ¥æºæ ‡è¯†
    doc_type: str           # pdf / txt / md / manual
    page: int = 0           # PDFé¡µç 
    chunk_index: int = 0    # åœ¨æ–‡æ¡£ä¸­çš„åºå·
    metadata: dict = field(default_factory=dict)
    embedding: Optional[np.ndarray] = None

    def to_dict(self) -> dict:
        return {
            "chunk_id": self.chunk_id,
            "text": self.text[:200] + "..." if len(self.text) > 200 else self.text,
            "source": self.source,
            "doc_type": self.doc_type,
            "page": self.page,
        }


@dataclass
class SearchResult:
    """æ£€ç´¢ç»“æœ"""
    chunk: DocChunk
    score: float            # 0-1 ç›¸ä¼¼åº¦
    match_type: str         # vector / keyword / hybrid


# ============================================================
# 1. Document Loader
# ============================================================

class DocLoader:
    """æ–‡æ¡£åŠ è½½å™¨ â€” æ”¯æŒ PDF / TXT / MD / DOCX"""

    @staticmethod
    def load(filepath: str) -> Tuple[str, str]:
        """
        åŠ è½½æ–‡æ¡£ï¼Œè¿”å› (text, doc_type)
        """
        path = Path(filepath)
        suffix = path.suffix.lower()

        if suffix == ".pdf":
            return DocLoader._load_pdf(filepath), "pdf"
        elif suffix in (".txt", ".text"):
            return DocLoader._load_text(filepath), "txt"
        elif suffix in (".md", ".markdown"):
            return DocLoader._load_text(filepath), "md"
        elif suffix == ".docx":
            return DocLoader._load_docx(filepath), "docx"
        elif suffix in (".json", ".jsonl"):
            return DocLoader._load_text(filepath), "json"
        else:
            # å°è¯•æŒ‰æ–‡æœ¬è¯»å–
            return DocLoader._load_text(filepath), "txt"

    @staticmethod
    def _load_pdf(filepath: str) -> str:
        try:
            from pypdf import PdfReader
            reader = PdfReader(filepath)
            pages = []
            for i, page in enumerate(reader.pages):
                text = page.extract_text() or ""
                if text.strip():
                    pages.append(f"[ç¬¬{i+1}é¡µ]\n{text}")
            return "\n\n".join(pages)
        except ImportError:
            try:
                from PyPDF2 import PdfReader as PdfReader2
                reader = PdfReader2(filepath)
                pages = []
                for i, page in enumerate(reader.pages):
                    text = page.extract_text() or ""
                    if text.strip():
                        pages.append(f"[ç¬¬{i+1}é¡µ]\n{text}")
                return "\n\n".join(pages)
            except ImportError:
                raise ImportError("éœ€è¦ pypdf æˆ– PyPDF2: pip install pypdf")

    @staticmethod
    def _load_docx(filepath: str) -> str:
        try:
            from docx import Document
            doc = Document(filepath)
            return "\n\n".join(p.text for p in doc.paragraphs if p.text.strip())
        except ImportError:
            raise ImportError("éœ€è¦ python-docx: pip install python-docx")

    @staticmethod
    def _load_text(filepath: str) -> str:
        encodings = ["utf-8", "gb2312", "gbk", "gb18030", "latin-1"]
        for enc in encodings:
            try:
                with open(filepath, "r", encoding=enc) as f:
                    return f.read()
            except (UnicodeDecodeError, UnicodeError):
                continue
        raise ValueError(f"æ— æ³•è§£ç æ–‡ä»¶: {filepath}")


# ============================================================
# 2. Smart Chunker (é”€å”®é¢†åŸŸæ„ŸçŸ¥)
# ============================================================

class SmartChunker:
    """
    æ™ºèƒ½åˆ†å—å™¨ â€” é”€å”®é¢†åŸŸæ„ŸçŸ¥

    ç­–ç•¥:
    1. ä¼˜å…ˆæŒ‰è¯­ä¹‰è¾¹ç•Œåˆ‡åˆ†ï¼ˆæ®µè½ã€æ ‡é¢˜ã€åˆ†éš”çº¿ï¼‰
    2. è¶…é•¿æ®µè½æŒ‰å¥å­åˆ‡åˆ†
    3. ä¿ç•™ä¸Šä¸‹æ–‡é‡å ï¼ˆoverlapï¼‰
    4. è¯†åˆ«é”€å”®é¢†åŸŸç‰¹æ®Šç»“æ„ï¼ˆåˆåŒæ¡æ¬¾ã€è¡¨æ ¼ã€action itemsï¼‰
    """

    def __init__(
        self,
        chunk_size: int = 500,     # ç›®æ ‡å­—ç¬¦æ•°
        chunk_overlap: int = 80,   # é‡å å­—ç¬¦æ•°
        min_chunk_size: int = 50,  # æœ€å°å—
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size

        # è¯­ä¹‰åˆ†éš”ç¬¦ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
        self._separators = [
            r'\n#{1,3}\s',          # Markdownæ ‡é¢˜
            r'\n---+\n',            # åˆ†éš”çº¿
            r'\n\n\n+',             # å¤šç©ºè¡Œ
            r'\nç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[æ¡ç« èŠ‚æ¬¾é¡¹]',  # åˆåŒæ¡æ¬¾
            r'\n\d+[\.\)ã€]\s',     # ç¼–å·æ®µè½
            r'\n\n',                # åŒæ¢è¡Œ
            r'[ã€‚ï¼ï¼Ÿ\n]',          # å¥æœ«
        ]

    def chunk(self, text: str, source: str = "", doc_type: str = "txt") -> List[DocChunk]:
        """å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå—"""
        if not text or len(text.strip()) < self.min_chunk_size:
            return []

        # é¢„å¤„ç†
        text = self._clean(text)

        # æŒ‰ä¼˜å…ˆçº§å°è¯•åˆ†éš”ç¬¦
        segments = [text]
        for pattern in self._separators:
            new_segments = []
            for seg in segments:
                if len(seg) <= self.chunk_size:
                    new_segments.append(seg)
                else:
                    parts = re.split(pattern, seg)
                    new_segments.extend(p for p in parts if p.strip())
            segments = new_segments

        # åˆå¹¶è¿‡çŸ­çš„æ®µ + åˆ‡åˆ†è¿‡é•¿çš„æ®µ
        chunks = []
        buffer = ""

        for seg in segments:
            seg = seg.strip()
            if not seg:
                continue

            if len(buffer) + len(seg) <= self.chunk_size:
                buffer += ("\n" if buffer else "") + seg
            else:
                if buffer and len(buffer) >= self.min_chunk_size:
                    chunks.append(buffer)
                if len(seg) > self.chunk_size:
                    # å¼ºåˆ¶æŒ‰å¥å­åˆ‡
                    sub_chunks = self._force_split(seg)
                    chunks.extend(sub_chunks)
                    buffer = ""
                else:
                    buffer = seg

        if buffer and len(buffer) >= self.min_chunk_size:
            chunks.append(buffer)

        # æ·»åŠ  overlap
        if self.chunk_overlap > 0 and len(chunks) > 1:
            chunks = self._add_overlap(chunks)

        # æ„å»º DocChunk å¯¹è±¡
        result = []
        for i, text_chunk in enumerate(chunks):
            chunk_id = hashlib.md5(
                f"{source}:{i}:{text_chunk[:50]}".encode()
            ).hexdigest()[:12]

            # æ£€æµ‹é¡µç 
            page = 0
            page_match = re.search(r'\[ç¬¬(\d+)é¡µ\]', text_chunk)
            if page_match:
                page = int(page_match.group(1))

            result.append(DocChunk(
                chunk_id=chunk_id,
                text=text_chunk.strip(),
                source=source,
                doc_type=doc_type,
                page=page,
                chunk_index=i,
                metadata={"char_count": len(text_chunk)},
            ))

        return result

    def _clean(self, text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—"""
        text = re.sub(r'\r\n', '\n', text)
        text = re.sub(r'[ \t]{4,}', '  ', text)  # å‹ç¼©å¤šä½™ç©ºæ ¼
        text = re.sub(r'\n{4,}', '\n\n\n', text)  # å‹ç¼©å¤šä½™ç©ºè¡Œ
        return text.strip()

    def _force_split(self, text: str) -> List[str]:
        """å¼ºåˆ¶åˆ‡åˆ†è¶…é•¿æ–‡æœ¬"""
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
        chunks = []
        buffer = ""
        for i in range(0, len(sentences)):
            s = sentences[i]
            if len(buffer) + len(s) <= self.chunk_size:
                buffer += s
            else:
                if buffer and len(buffer) >= self.min_chunk_size:
                    chunks.append(buffer)
                buffer = s
        if buffer and len(buffer) >= self.min_chunk_size:
            chunks.append(buffer)
        return chunks

    def _add_overlap(self, chunks: List[str]) -> List[str]:
        """æ·»åŠ å—é—´é‡å """
        result = [chunks[0]]
        for i in range(1, len(chunks)):
            overlap = chunks[i - 1][-self.chunk_overlap:]
            result.append(overlap + " " + chunks[i])
        return result


# ============================================================
# 3. Embedder (å¤šåç«¯)
# ============================================================

class TFIDFEmbedder:
    """
    TF-IDF åµŒå…¥å™¨ â€” é›¶å¤–éƒ¨ä¾èµ–ï¼Œçº¯ numpy

    ç”¨äºæ—  API æ—¶çš„æœ¬åœ°å‘é‡åŒ–ã€‚æ•ˆæœä¸å¦‚ neural embedding
    ä½†å¯¹äºä¸­æ–‡é”€å”®é¢†åŸŸçš„å…³é”®è¯åŒ¹é…å·²ç»å¤Ÿç”¨ã€‚
    """

    def __init__(self, dim: int = 512):
        self.dim = dim
        self.vocabulary: Dict[str, int] = {}
        self.idf: Optional[np.ndarray] = None
        self._doc_count = 0
        self._fitted = False

    def fit(self, texts: List[str]):
        """æ„å»ºè¯æ±‡è¡¨å’Œ IDF"""
        df = Counter()  # document frequency
        all_tokens = set()

        for text in texts:
            tokens = set(self._tokenize(text))
            for t in tokens:
                df[t] += 1
            all_tokens.update(tokens)

        # é€‰å– top-dim é«˜é¢‘è¯
        most_common = df.most_common(self.dim)
        self.vocabulary = {word: i for i, (word, _) in enumerate(most_common)}

        # è®¡ç®— IDF
        n = len(texts)
        self._doc_count = n
        self.idf = np.zeros(len(self.vocabulary))
        for word, idx in self.vocabulary.items():
            self.idf[idx] = math.log((n + 1) / (df.get(word, 0) + 1)) + 1

        self._fitted = True

    def embed(self, text: str) -> np.ndarray:
        """å°†æ–‡æœ¬è½¬ä¸º TF-IDF å‘é‡"""
        if not self._fitted:
            # æœª fit æ—¶è¿”å›ç®€å• hash å‘é‡
            return self._hash_embed(text)

        tokens = self._tokenize(text)
        tf = Counter(tokens)
        vec = np.zeros(len(self.vocabulary))

        for word, idx in self.vocabulary.items():
            if word in tf:
                # TF: log(1 + count)
                vec[idx] = math.log(1 + tf[word]) * self.idf[idx]

        # L2 normalize
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec /= norm

        # Pad or truncate to dim
        if len(vec) < self.dim:
            vec = np.pad(vec, (0, self.dim - len(vec)))

        return vec[:self.dim]

    def embed_batch(self, texts: List[str]) -> np.ndarray:
        """æ‰¹é‡åµŒå…¥"""
        return np.array([self.embed(t) for t in texts])

    def _tokenize(self, text: str) -> List[str]:
        """ä¸­è‹±æ–‡æ··åˆåˆ†è¯ï¼ˆç®€å•ä½†æœ‰æ•ˆï¼‰"""
        # ä¸­æ–‡ï¼šé€å­— + åŒå­—
        cn_chars = re.findall(r'[\u4e00-\u9fff]', text)
        cn_bigrams = [cn_chars[i] + cn_chars[i+1]
                      for i in range(len(cn_chars) - 1)]

        # è‹±æ–‡ï¼šæŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹
        en_words = re.findall(r'[a-zA-Z]{2,}', text.lower())

        # æ•°å­—
        numbers = re.findall(r'\d+\.?\d*', text)

        return cn_chars + cn_bigrams + en_words + numbers

    def _hash_embed(self, text: str) -> np.ndarray:
        """Hash-based embedding fallback"""
        vec = np.zeros(self.dim)
        tokens = self._tokenize(text)
        for token in tokens:
            h = int(hashlib.md5(token.encode()).hexdigest(), 16)
            idx = h % self.dim
            vec[idx] += 1.0
        norm = np.linalg.norm(vec)
        if norm > 0:
            vec /= norm
        return vec


class APIEmbedder:
    """
    API åµŒå…¥å™¨ â€” ä½¿ç”¨ OpenAI/Voyage/æœ¬åœ°æ¨¡å‹

    æ¯” TF-IDF æ•ˆæœå¥½å¾—å¤šï¼Œä½†éœ€è¦ API Key
    """

    def __init__(self, provider: str = "openai", api_key: str = "", model: str = ""):
        self.provider = provider
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY", "")
        self.model = model or self._default_model()
        self.dim = 1536 if "openai" in provider else 1024

    def _default_model(self) -> str:
        if self.provider == "openai":
            return "text-embedding-3-small"
        elif self.provider == "voyage":
            return "voyage-3"
        return "text-embedding-3-small"

    def embed(self, text: str) -> np.ndarray:
        if self.provider == "openai":
            return self._openai_embed([text])[0]
        raise ValueError(f"Unsupported provider: {self.provider}")

    def embed_batch(self, texts: List[str]) -> np.ndarray:
        if self.provider == "openai":
            return self._openai_embed(texts)
        raise ValueError(f"Unsupported provider: {self.provider}")

    def _openai_embed(self, texts: List[str]) -> np.ndarray:
        try:
            from openai import OpenAI
            client = OpenAI(api_key=self.api_key)
            resp = client.embeddings.create(input=texts, model=self.model)
            vecs = [np.array(d.embedding) for d in resp.data]
            return np.array(vecs)
        except Exception as e:
            logger.warning(f"API embedding failed: {e}, falling back to TF-IDF")
            # Fallback
            fb = TFIDFEmbedder(dim=self.dim)
            return fb.embed_batch(texts)


# ============================================================
# 4. Vector Store (numpy-based)
# ============================================================

class NumpyVectorStore:
    """
    çº¯ numpy å‘é‡å­˜å‚¨ â€” é›¶ä¾èµ–

    ç‰¹æ€§:
    - ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢
    - æŒä¹…åŒ–åˆ°ç£ç›˜ (.npz + .json)
    - å¢é‡æ·»åŠ 
    - æœ€å¤§ 100K å‘é‡ï¼ˆå†…å­˜é™åˆ¶ï¼‰
    """

    def __init__(self, dim: int = 512, persist_dir: str = ""):
        self.dim = dim
        self.vectors: Optional[np.ndarray] = None  # (N, dim)
        self.chunks: List[DocChunk] = []
        self.persist_dir = persist_dir
        self._dirty = False

        if persist_dir:
            self._load()

    def add(self, chunks: List[DocChunk], embeddings: np.ndarray):
        """æ·»åŠ å‘é‡"""
        if self.vectors is None:
            self.vectors = embeddings
        else:
            self.vectors = np.vstack([self.vectors, embeddings])
        self.chunks.extend(chunks)
        self._dirty = True

    def search(self, query_vec: np.ndarray, top_k: int = 5) -> List[Tuple[DocChunk, float]]:
        """ä½™å¼¦ç›¸ä¼¼åº¦æ£€ç´¢"""
        if self.vectors is None or len(self.chunks) == 0:
            return []

        # Cosine similarity
        query_norm = query_vec / (np.linalg.norm(query_vec) + 1e-10)
        norms = np.linalg.norm(self.vectors, axis=1, keepdims=True) + 1e-10
        normalized = self.vectors / norms
        scores = normalized @ query_norm

        # Top-K
        top_k = min(top_k, len(scores))
        top_indices = np.argsort(scores)[-top_k:][::-1]

        results = []
        for idx in top_indices:
            if scores[idx] > 0.01:  # æœ€ä½é˜ˆå€¼
                results.append((self.chunks[idx], float(scores[idx])))

        return results

    def size(self) -> int:
        return len(self.chunks)

    def clear(self):
        self.vectors = None
        self.chunks = []
        self._dirty = True

    def save(self):
        """æŒä¹…åŒ–"""
        if not self.persist_dir or not self._dirty:
            return
        os.makedirs(self.persist_dir, exist_ok=True)
        if self.vectors is not None:
            np.save(os.path.join(self.persist_dir, "vectors.npy"), self.vectors)
        meta = []
        for c in self.chunks:
            meta.append({
                "chunk_id": c.chunk_id, "text": c.text,
                "source": c.source, "doc_type": c.doc_type,
                "page": c.page, "chunk_index": c.chunk_index,
                "metadata": c.metadata,
            })
        with open(os.path.join(self.persist_dir, "chunks.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False)
        self._dirty = False
        logger.info(f"Saved {len(self.chunks)} chunks to {self.persist_dir}")

    def _load(self):
        """ä»ç£ç›˜åŠ è½½"""
        vec_path = os.path.join(self.persist_dir, "vectors.npy")
        meta_path = os.path.join(self.persist_dir, "chunks.json")
        if os.path.exists(vec_path) and os.path.exists(meta_path):
            self.vectors = np.load(vec_path)
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
            self.chunks = [
                DocChunk(
                    chunk_id=m["chunk_id"], text=m["text"],
                    source=m["source"], doc_type=m["doc_type"],
                    page=m.get("page", 0), chunk_index=m.get("chunk_index", 0),
                    metadata=m.get("metadata", {}),
                )
                for m in meta
            ]
            logger.info(f"Loaded {len(self.chunks)} chunks from {self.persist_dir}")


# ============================================================
# 5. BM25 å…³é”®è¯ç´¢å¼•
# ============================================================

class BM25Index:
    """
    BM25 å…³é”®è¯æ£€ç´¢ â€” è¡¥å……å‘é‡æ£€ç´¢çš„ç²¾ç¡®åŒ¹é…èƒ½åŠ›

    å¯¹äº "HMD Q3è®¢å•" è¿™ç±»ç²¾ç¡®æŸ¥è¯¢ï¼ŒBM25 æ¯”å‘é‡æ£€ç´¢æ›´å‡†ç¡®
    """

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.chunks: List[DocChunk] = []
        self.doc_tokens: List[List[str]] = []
        self.doc_freq: Counter = Counter()
        self.avg_dl: float = 0

    def add(self, chunks: List[DocChunk]):
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        for chunk in chunks:
            tokens = self._tokenize(chunk.text)
            self.chunks.append(chunk)
            self.doc_tokens.append(tokens)
            for t in set(tokens):
                self.doc_freq[t] += 1

        total_len = sum(len(t) for t in self.doc_tokens)
        self.avg_dl = total_len / max(len(self.doc_tokens), 1)

    def search(self, query: str, top_k: int = 5) -> List[Tuple[DocChunk, float]]:
        """BM25 æ£€ç´¢"""
        if not self.chunks:
            return []

        query_tokens = self._tokenize(query)
        scores = []
        n = len(self.chunks)

        for i, doc_tokens in enumerate(self.doc_tokens):
            score = 0
            dl = len(doc_tokens)
            tf_map = Counter(doc_tokens)

            for qt in query_tokens:
                if qt not in tf_map:
                    continue
                tf = tf_map[qt]
                df = self.doc_freq.get(qt, 0)
                idf = math.log((n - df + 0.5) / (df + 0.5) + 1)
                tf_norm = (tf * (self.k1 + 1)) / (
                    tf + self.k1 * (1 - self.b + self.b * dl / max(self.avg_dl, 1))
                )
                score += idf * tf_norm

            scores.append(score)

        # Top-K
        scores = np.array(scores)
        top_k = min(top_k, len(scores))
        top_indices = np.argsort(scores)[-top_k:][::-1]

        results = []
        max_score = scores.max() if len(scores) > 0 else 1
        for idx in top_indices:
            if scores[idx] > 0:
                # Normalize to 0-1
                norm_score = scores[idx] / max(max_score, 1e-10)
                results.append((self.chunks[idx], float(norm_score)))

        return results

    def _tokenize(self, text: str) -> List[str]:
        cn_chars = re.findall(r'[\u4e00-\u9fff]+', text)
        cn_bigrams = []
        for word in cn_chars:
            for i in range(len(word) - 1):
                cn_bigrams.append(word[i:i+2])
        en_words = re.findall(r'[a-zA-Z]{2,}', text.lower())
        numbers = re.findall(r'\d+\.?\d*', text)
        return cn_chars + cn_bigrams + en_words + numbers

    def clear(self):
        self.chunks = []
        self.doc_tokens = []
        self.doc_freq = Counter()
        self.avg_dl = 0


# ============================================================
# 6. Hybrid Retriever
# ============================================================

class HybridRetriever:
    """
    æ··åˆæ£€ç´¢å™¨ â€” å‘é‡ + BM25 èåˆ

    Reciprocal Rank Fusion (RRF) èåˆç­–ç•¥:
    score = Î£ 1/(k + rank_i) for each retriever
    """

    def __init__(self, rrf_k: int = 60, vector_weight: float = 0.6):
        self.rrf_k = rrf_k
        self.vector_weight = vector_weight
        self.keyword_weight = 1.0 - vector_weight

    def fuse(
        self,
        vector_results: List[Tuple[DocChunk, float]],
        keyword_results: List[Tuple[DocChunk, float]],
        top_k: int = 5,
    ) -> List[SearchResult]:
        """RRF èåˆä¸¤è·¯æ£€ç´¢ç»“æœ"""
        scores: Dict[str, float] = {}
        chunk_map: Dict[str, DocChunk] = {}

        # Vector results
        for rank, (chunk, score) in enumerate(vector_results):
            cid = chunk.chunk_id
            chunk_map[cid] = chunk
            rrf = self.vector_weight / (self.rrf_k + rank + 1)
            scores[cid] = scores.get(cid, 0) + rrf

        # Keyword results
        for rank, (chunk, score) in enumerate(keyword_results):
            cid = chunk.chunk_id
            chunk_map[cid] = chunk
            rrf = self.keyword_weight / (self.rrf_k + rank + 1)
            scores[cid] = scores.get(cid, 0) + rrf

        # Sort by fused score
        sorted_ids = sorted(scores, key=scores.get, reverse=True)[:top_k]

        results = []
        for cid in sorted_ids:
            match_type = "hybrid"
            if cid in {c.chunk_id for c, _ in vector_results} and \
               cid not in {c.chunk_id for c, _ in keyword_results}:
                match_type = "vector"
            elif cid not in {c.chunk_id for c, _ in vector_results}:
                match_type = "keyword"

            results.append(SearchResult(
                chunk=chunk_map[cid],
                score=scores[cid],
                match_type=match_type,
            ))

        return results


# ============================================================
# 7. RAG Engine (ä¸»å…¥å£)
# ============================================================

class RAGEngine:
    """
    RAG å¼•æ“ â€” ä¸€ç«™å¼æ–‡æ¡£ç†è§£

    Usage:
        rag = RAGEngine(persist_dir="./rag_store")
        rag.ingest_file("contract.pdf")
        rag.ingest_text("ä¼šè®®çºªè¦å†…å®¹...", source="meeting_20250201")
        context = rag.build_context("HMDè®¢å•æƒ…å†µ")
    """

    def __init__(
        self,
        embedding_provider: str = "tfidf",  # tfidf / openai
        embedding_api_key: str = "",
        embedding_dim: int = 512,
        chunk_size: int = 500,
        chunk_overlap: int = 80,
        persist_dir: str = "",
        vector_weight: float = 0.6,
    ):
        self.chunker = SmartChunker(chunk_size, chunk_overlap)

        # Embedder
        if embedding_provider == "openai" and embedding_api_key:
            self.embedder = APIEmbedder("openai", embedding_api_key)
            self.dim = self.embedder.dim
        else:
            self.embedder = TFIDFEmbedder(dim=embedding_dim)
            self.dim = embedding_dim

        # Stores
        self.vector_store = NumpyVectorStore(dim=self.dim, persist_dir=persist_dir)
        self.bm25 = BM25Index()
        self.retriever = HybridRetriever(vector_weight=vector_weight)

        # Stats
        self._ingest_count = 0
        self._search_count = 0
        self._sources: Dict[str, int] = {}  # source â†’ chunk count

    def ingest_file(self, filepath: str, metadata: dict = None) -> int:
        """
        å¯¼å…¥æ–‡ä»¶

        Returns: ç”Ÿæˆçš„ chunk æ•°é‡
        """
        source = os.path.basename(filepath)
        text, doc_type = DocLoader.load(filepath)

        if not text.strip():
            logger.warning(f"Empty document: {filepath}")
            return 0

        return self._ingest(text, source, doc_type, metadata)

    def ingest_text(self, text: str, source: str = "manual", doc_type: str = "txt",
                    metadata: dict = None) -> int:
        """
        å¯¼å…¥æ–‡æœ¬

        Returns: ç”Ÿæˆçš„ chunk æ•°é‡
        """
        if not text.strip():
            return 0
        return self._ingest(text, source, doc_type, metadata)

    def _ingest(self, text: str, source: str, doc_type: str, metadata: dict = None) -> int:
        """å†…éƒ¨å¯¼å…¥é€»è¾‘"""
        # Chunk
        chunks = self.chunker.chunk(text, source, doc_type)
        if not chunks:
            return 0

        if metadata:
            for c in chunks:
                c.metadata.update(metadata)

        # Embed
        texts = [c.text for c in chunks]

        if isinstance(self.embedder, TFIDFEmbedder):
            # éœ€è¦é‡æ–° fitï¼ˆå¢é‡ï¼‰
            all_texts = [c.text for c in self.vector_store.chunks] + texts
            self.embedder.fit(all_texts)
            # é‡å»ºæ‰€æœ‰å‘é‡ï¼ˆTF-IDF éœ€è¦å…¨å±€ IDFï¼‰
            if self.vector_store.chunks:
                old_texts = [c.text for c in self.vector_store.chunks]
                old_embeddings = self.embedder.embed_batch(old_texts)
                self.vector_store.vectors = old_embeddings

            embeddings = self.embedder.embed_batch(texts)
        else:
            embeddings = self.embedder.embed_batch(texts)

        # Store
        self.vector_store.add(chunks, embeddings)
        self.bm25.add(chunks)

        # Stats
        self._ingest_count += len(chunks)
        self._sources[source] = self._sources.get(source, 0) + len(chunks)

        logger.info(f"Ingested {len(chunks)} chunks from '{source}'")

        # Auto-save
        if self.vector_store.persist_dir:
            self.vector_store.save()

        return len(chunks)

    def search(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """
        æ··åˆæ£€ç´¢

        Returns: SearchResult åˆ—è¡¨ï¼ˆå·²æŒ‰ç›¸å…³æ€§æ’åºï¼‰
        """
        self._search_count += 1

        if self.vector_store.size() == 0:
            return []

        # Vector search
        query_vec = self.embedder.embed(query)
        vec_results = self.vector_store.search(query_vec, top_k=top_k * 2)

        # BM25 search
        bm25_results = self.bm25.search(query, top_k=top_k * 2)

        # Fuse
        fused = self.retriever.fuse(vec_results, bm25_results, top_k=top_k)

        return fused

    def build_context(
        self,
        query: str,
        max_tokens: int = 2000,
        top_k: int = 8,
        include_source: bool = True,
    ) -> str:
        """
        æ„å»º Agent ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

        é€‚åˆç›´æ¥æ‹¼æ¥åˆ° structured_data åé¢æ³¨å…¥ prompt
        """
        results = self.search(query, top_k=top_k)

        if not results:
            return ""

        # ä¼°ç®— tokenï¼ˆä¸­æ–‡çº¦ 2 char/tokenï¼‰
        char_limit = max_tokens * 2
        sections = []
        total_chars = 0

        header = f"ã€ç›¸å…³æ–‡æ¡£å‚è€ƒ ({len(results)}æ¡)ã€‘\n"
        total_chars += len(header)

        for i, r in enumerate(results):
            text = r.chunk.text.strip()
            source_tag = f" [æ¥æº: {r.chunk.source}"
            if r.chunk.page > 0:
                source_tag += f" p{r.chunk.page}"
            source_tag += f" | ç›¸å…³åº¦: {r.score:.0%}]"

            entry = f"\n--- å‚è€ƒ{i+1} ---\n{text}"
            if include_source:
                entry += f"\n{source_tag}"

            if total_chars + len(entry) > char_limit:
                break

            sections.append(entry)
            total_chars += len(entry)

        if not sections:
            return ""

        return header + "\n".join(sections)

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_chunks": self.vector_store.size(),
            "total_ingests": self._ingest_count,
            "total_searches": self._search_count,
            "sources": self._sources,
            "embedding_type": type(self.embedder).__name__,
            "embedding_dim": self.dim,
        }

    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰æ•°æ®"""
        self.vector_store.clear()
        self.bm25.clear()
        self._sources.clear()
        self._ingest_count = 0

    def list_sources(self) -> List[dict]:
        """åˆ—å‡ºæ‰€æœ‰å·²å¯¼å…¥çš„æ–‡æ¡£æº"""
        return [{"source": s, "chunks": n} for s, n in self._sources.items()]


# ============================================================
# 8. Integration Helper â€” æ¥å…¥ multi_agent.py
# ============================================================

# Global RAG instance
_global_rag: Optional[RAGEngine] = None


def get_rag(persist_dir: str = "") -> RAGEngine:
    """è·å–å…¨å±€ RAG å®ä¾‹"""
    global _global_rag
    if _global_rag is None:
        _global_rag = RAGEngine(persist_dir=persist_dir)
    return _global_rag


def enrich_context_with_rag(
    question: str,
    structured_context: str,
    rag: RAGEngine = None,
    max_rag_tokens: int = 1500,
) -> str:
    """
    å°† RAG æ£€ç´¢ç»“æœåˆå¹¶åˆ°ç»“æ„åŒ–æ•°æ®ä¸Šä¸‹æ–‡ä¸­

    ä¾› multi_agent.py çš„ ask_multi_agent() è°ƒç”¨:

        context_data = smart_query(question, df)
        context_data = enrich_context_with_rag(question, context_data)
    """
    rag = rag or get_rag()

    if rag.vector_store.size() == 0:
        return structured_context

    rag_context = rag.build_context(question, max_tokens=max_rag_tokens)

    if not rag_context:
        return structured_context

    return structured_context + "\n\n" + rag_context


# ============================================================
# CLI Test
# ============================================================

def main():
    print("ğŸ§ª Testing RAG Engine...\n")

    rag = RAGEngine()

    # æ¨¡æ‹Ÿå¯¼å…¥æ–‡æ¡£
    doc1 = """
    ç¦¾è‹—é€šè®¯ä¸HMDåˆåŒå¤‡å¿˜å½•

    ç¬¬ä¸€æ¡ï¼šåˆä½œèŒƒå›´
    ç”²æ–¹ï¼ˆHMD Globalï¼‰å§”æ‰˜ä¹™æ–¹ï¼ˆç¦¾è‹—é€šè®¯ï¼‰è¿›è¡ŒåŠŸèƒ½æœºå’Œæ™ºèƒ½æ‰‹æœºçš„ODMè®¾è®¡ä¸åˆ¶é€ ã€‚
    æ¶µç›–CKDæ•£ä»¶å’Œæ•´æœºä¸¤ç§äº¤ä»˜æ¨¡å¼ã€‚

    ç¬¬äºŒæ¡ï¼šä»·æ ¼æ¡æ¬¾
    2025å¹´åŠŸèƒ½æœºåŸºå‡†ä»·æ ¼ä¸ºæ¯å°8.5ç¾å…ƒï¼ˆFOBæ·±åœ³ï¼‰ï¼Œæ™ºèƒ½æœºä¸º35-65ç¾å…ƒåŒºé—´ã€‚
    å­£åº¦ä»·æ ¼è°ƒæ•´æœºåˆ¶ï¼šæ ¹æ®BOMæˆæœ¬å˜åŠ¨ï¼Œå…è®¸Â±5%æµ®åŠ¨ã€‚

    ç¬¬ä¸‰æ¡ï¼šè®¢å•é¢„æµ‹
    HMDæ‰¿è¯º2025å¹´æœ€ä½é‡‡è´­é‡ä¸º500ä¸‡å°ï¼Œå…¶ä¸­åŠŸèƒ½æœº350ä¸‡å°ï¼Œæ™ºèƒ½æœº150ä¸‡å°ã€‚
    å­£åº¦åˆ†å¸ƒå‚è€ƒå†å²æ¨¡å¼ï¼šQ1(20%)ã€Q2(25%)ã€Q3(30%)ã€Q4(25%)ã€‚

    ç¬¬å››æ¡ï¼šä»˜æ¬¾æ¡ä»¶
    T/T 60å¤©ï¼Œæœˆç»“ã€‚è‹¥å•æœˆå‡ºè´§è¶…è¿‡200ä¸‡å°ï¼Œä»˜æ¬¾å‘¨æœŸå¯å»¶é•¿è‡³T/T 90å¤©ã€‚
    """

    doc2 = """
    2025å¹´2æœˆå®¢æˆ·æ‹œè®¿çºªè¦

    æ‹œè®¿å®¢æˆ·ï¼šHMD Globalï¼ˆèŠ¬å…°æ€»éƒ¨è§†é¢‘ä¼šï¼‰
    å‚ä¼šäººï¼šå¼ æ€»ï¼ˆç¦¾è‹—VPï¼‰ã€Markï¼ˆHMDé‡‡è´­æ€»ç›‘ï¼‰

    å…³é”®ä¿¡æ¯ï¼š
    1. HMDè¡¨ç¤º2026å¹´å°†è¿›ä¸€æ­¥ç¼©å‡åŠŸèƒ½æœºäº§å“çº¿ï¼Œé¢„è®¡ä»å½“å‰12ä¸ªå‹å·å‡è‡³6-8ä¸ª
    2. æ™ºèƒ½æœºæ–¹é¢ï¼ŒHMDè®¡åˆ’æ¨å‡º3æ¬¾æ–°çš„Android Goè®¾å¤‡ï¼Œå¸Œæœ›ç¦¾è‹—å‚ä¸ç«æ ‡
    3. Markæš—ç¤ºåå‹¤åœ¨ä»·æ ¼ä¸Šæ›´æœ‰ä¼˜åŠ¿ï¼Œå»ºè®®ç¦¾è‹—åœ¨äº¤ä»˜é€Ÿåº¦å’Œè´¨é‡ä¸Šå¼ºåŒ–å·®å¼‚åŒ–
    4. HMDå¯¹CKDæ•£ä»¶éœ€æ±‚å°†ä¸‹é™ï¼Œå› ä¸ºå°åº¦æœ¬åœ°ç»„è£…èƒ½åŠ›åœ¨æå‡

    Action Itemsï¼š
    - ç¦¾è‹—2å‘¨å†…æäº¤Android Goæ–°æœºå‹çš„æŠ¥ä»·æ–¹æ¡ˆ
    - è¯„ä¼°å°åº¦CKDä¸šåŠ¡2026å¹´é¢„æœŸä¸‹é™å¹…åº¦
    - å®‰æ’Q2å­£åº¦å•†åŠ¡å›é¡¾ä¼šè®®
    """

    doc3 = """
    IDCå…¨çƒåŠŸèƒ½æœºå¸‚åœºæŠ¥å‘Šæ‘˜è¦ï¼ˆ2025å¹´12æœˆï¼‰

    2025å¹´å…¨çƒåŠŸèƒ½æœºå‡ºè´§é‡çº¦7.2äº¿å°ï¼ŒåŒæ¯”ä¸‹é™12%ã€‚
    åˆ†å¸‚åœºæ¥çœ‹ï¼š
    - éæ´²ä»æ˜¯æœ€å¤§å¸‚åœºï¼ˆ2.8äº¿å°ï¼Œ-8%ï¼‰ï¼Œä¼ éŸ³ç»§ç»­ä¸»å¯¼
    - å°åº¦å¸‚åœºåŠ é€Ÿèç¼©ï¼ˆ1.1äº¿å°ï¼Œ-19%ï¼‰ï¼Œæ™ºèƒ½æœºæ›¿ä»£åŠ é€Ÿ
    - ä¸­ä¸œ&ä¸œå—äºšä¿æŒå¹³ç¨³ï¼ˆ-5%ï¼‰

    ODMæ ¼å±€ï¼š
    - åå‹¤å¸‚å ç‡çº¦35%ï¼ˆç¨³å®šï¼‰
    - é—»æ³°ä»½é¢ä¸‹é™è‡³22%ï¼ˆè½¬å‘æ±½è½¦ç”µå­ï¼‰
    - é¾™æ——çº¦18%
    - ç¦¾è‹—çº¦5%ï¼Œä½†åœ¨HMD/Lavaç»†åˆ†å¸‚åœºä»½é¢è¾ƒé«˜

    2026å¹´é¢„æµ‹ï¼šå…¨çƒåŠŸèƒ½æœºå°†ç»§ç»­èç¼©10-15%ï¼Œå°åº¦æ˜¯æœ€å¤§ä¸ç¡®å®šå› ç´ ã€‚
    """

    n1 = rag.ingest_text(doc1, source="HMD_contract_2025.pdf", doc_type="pdf")
    n2 = rag.ingest_text(doc2, source="meeting_20250201_HMD.md", doc_type="md")
    n3 = rag.ingest_text(doc3, source="IDC_featurephone_2025Q4.pdf", doc_type="pdf")
    print(f"âœ… Ingested: {n1} + {n2} + {n3} = {n1+n2+n3} chunks")

    # æµ‹è¯•æ£€ç´¢
    queries = [
        "HMDçš„åˆåŒä»·æ ¼æ˜¯å¤šå°‘ï¼Ÿ",
        "HMDä¸ºä»€ä¹ˆè¦ç¼©å‡è®¢å•ï¼Ÿ",
        "å…¨çƒåŠŸèƒ½æœºå¸‚åœºè¶‹åŠ¿",
        "ç¦¾è‹—åœ¨ODMè¡Œä¸šçš„å¸‚åœºä»½é¢",
        "å°åº¦CKDä¸šåŠ¡å±•æœ›",
    ]

    for q in queries:
        print(f"\nğŸ” Query: {q}")
        results = rag.search(q, top_k=3)
        for i, r in enumerate(results):
            print(f"  [{i+1}] ({r.match_type}, {r.score:.2%}) "
                  f"[{r.chunk.source}] {r.chunk.text[:80]}...")

    # æµ‹è¯• context æ„å»º
    print("\n" + "=" * 50)
    print("ğŸ“ Context for Agent:")
    ctx = rag.build_context("HMDè®¢å•ç¼©å‡çš„æ ¹æœ¬åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ", max_tokens=1000)
    print(ctx[:500])

    # æµ‹è¯• enrich
    print("\n" + "=" * 50)
    structured = "ã€2025å¹´æ€»è¥æ”¶ã€‘41.71äº¿å…ƒï¼ŒåŒæ¯”+54.1%"
    enriched = enrich_context_with_rag("HMDæƒ…å†µ", structured, rag)
    print(f"ğŸ“ Enriched context ({len(enriched)} chars):")
    print(enriched[:400])

    # Stats
    print(f"\nğŸ“Š Stats: {rag.get_stats()}")
    print("\nâœ… RAG Engine all tests passed!")


if __name__ == "__main__":
    main()
