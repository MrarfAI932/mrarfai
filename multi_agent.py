#!/usr/bin/env python3
"""
MRARFAI Multi-Agent System v10.0 (Unified)
=============================================
v4.0 åŸºç¡€å±‚ (Tool Use + Guardrails + Streaming + KG + Observability)
  + v7.0 LangGraph å±‚ (StateGraph + HITL + Reflection + Multi-Model Routing)
  = v9.0 ç»Ÿä¸€æ–‡ä»¶

æ¶æ„ (LangGraph 1.0 StateGraph, å¯é€‰):
  START â†’ route â†’ experts (parallel) â†’ synthesize â†’ reflect â†’ hitl_check â†’ END

å…¥å£:
  ask_multi_agent()        â€” V4 å®Œæ•´ç®¡çº¿ (chat_tab.py ä½¿ç”¨)
  run_multi_agent_v7()     â€” V7 LangGraph ç®¡çº¿
  run_multi_agent()        â€” V7 å…¼å®¹åˆ«å

7+ Agents: åˆ†æå¸ˆ + é£æ§ + ç­–ç•¥å¸ˆ + å“è´¨ + å¸‚åœº + è´¢åŠ¡ + é‡‡è´­ + æŠ¥å‘Šå‘˜ + æ‰¹è¯„å®¶
ä¾èµ–: pip install langgraph>=1.0 langchain-core>=1.0 (å¯é€‰, æ— åˆ™å›é€€V4)
"""

import json
import os
import time
import logging
from datetime import datetime
from typing import Optional, Dict, List, Any, Literal, Annotated
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field

logger = logging.getLogger("mrarfai.agent_v9")

try:
    from typing import TypedDict
except ImportError:
    from typing_extensions import TypedDict

# Knowledge Graph
try:
    from knowledge_graph import SalesKnowledgeGraph, SynonymGraph, QueryPatternLibrary
    HAS_KG = True
except ImportError:
    HAS_KG = False

# Observability
try:
    from observability import (
        get_tracer, get_metrics, get_store, AgentTracer,
        SpanKind, CostCalculator,
    )
    HAS_OBS = True
except ImportError:
    HAS_OBS = False

# v3.3: CriticAgent (Generator+Critic pattern)
try:
    from critic_agent import critique_and_refine, CriticAgent
    HAS_CRITIC = True
except ImportError:
    HAS_CRITIC = False

# v4.0: Tool Registry
try:
    from tool_registry import (
        sales_tools, ToolRegistry, get_tools_for_agent,
        get_tool_descriptions_for_prompt, execute_tool_calls,
        AGENT_TOOL_CATEGORIES,
    )
    HAS_TOOLS = True
except ImportError:
    HAS_TOOLS = False

# v4.0: Guardrails
try:
    from guardrails import (
        guarded_llm_call, with_retry, RetryConfig, DEFAULT_RETRY,
        get_breaker, CircuitBreakerOpenError,
        validate_agent_output, safe_parse_llm_json,
        FallbackChain, get_budget, get_cache,
    )
    HAS_GUARD = True
except ImportError:
    HAS_GUARD = False

# v4.0: Streaming
try:
    from streaming import (
        StreamCallback, StreamEvent, EventType,
        PipelineStream,
    )
    HAS_STREAM = True
except ImportError:
    HAS_STREAM = False

# v3.3: Persistent Memory
try:
    from persistent_memory import (
        get_persistent_memory, PersistentMemoryStore,
        InsightRecord, EntityProfile,
    )
    HAS_PMEM = True
except ImportError:
    HAS_PMEM = False

# v3.3: Enhanced HITL
try:
    from hitl_engine import evaluate_hitl, HITLEngine, ConfidenceLevel
    HAS_HITL_V2 = True
except ImportError:
    HAS_HITL_V2 = False

# v3.3: Protocol Layer (MCP/A2A)
try:
    from protocol_layer import get_protocol_manager, ProtocolManager
    HAS_PROTOCOL = True
except ImportError:
    HAS_PROTOCOL = False

# V10.0: åŸŸ Agent å¼•æ“å¯¼å…¥
try:
    from agent_quality import QualityEngine
    HAS_QUALITY = True
except ImportError:
    HAS_QUALITY = False

try:
    from agent_market import MarketEngine
    HAS_MARKET = True
except ImportError:
    HAS_MARKET = False

try:
    from agent_finance import FinanceEngine
    HAS_FINANCE = True
except ImportError:
    HAS_FINANCE = False

try:
    from agent_procurement import ProcurementEngine
    HAS_PROCUREMENT = True
except ImportError:
    HAS_PROCUREMENT = False

try:
    from agent_risk import RiskEngine
    HAS_RISK_ENGINE = True
except ImportError:
    HAS_RISK_ENGINE = False

try:
    from agent_strategist import StrategistEngine
    HAS_STRATEGIST_ENGINE = True
except ImportError:
    HAS_STRATEGIST_ENGINE = False

# V10.0: Pydantic ç»“æ„åŒ–åˆçº¦
from contracts import AgentRequest, AgentResponse, GraphInput, GraphOutput

# V10.0: DB â†’ Agent Bridge
try:
    from db_connector import create_engines_from_db, DatabaseConfig
    HAS_DB_BRIDGE = True
except ImportError:
    HAS_DB_BRIDGE = False

# Langfuse v3 å¯è§‚æµ‹æ€§
try:
    from langfuse import Langfuse
    _langfuse_client = Langfuse()
    HAS_LANGFUSE = True
except Exception:
    _langfuse_client = None
    HAS_LANGFUSE = False

# v7.0: LangGraph (å¯é€‰ â€” æ— åˆ™å›é€€V4ç®¡çº¿)
try:
    from langgraph.graph import StateGraph, START, END
    from langgraph.graph.message import add_messages
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.types import interrupt, Command
    HAS_LANGGRAPH = True
except ImportError:
    HAS_LANGGRAPH = False
    logger.info("langgraph æœªå®‰è£…ï¼Œä½¿ç”¨ V4 ç®¡çº¿æ¨¡å¼")

# v7.0: HITL Engine (åŒºåˆ†äº v3.3 hitl_engine)
try:
    from hitl_engine import evaluate_hitl as evaluate_hitl_v7
    HAS_HITL = True
except ImportError:
    HAS_HITL = False

# ============================================================
# V9.0 æ¨¡å—å¯¼å…¥ â€” 7ç¯‡è®ºæ–‡æ ¸å¿ƒå¼•æ“
# ============================================================

# V9.0 â‘  RLM é€’å½’è¯­è¨€æ¨¡å‹å¼•æ“ (arXiv:2512.24601)
try:
    from rlm_engine import RLMEngine, RLMConfig, RLMResult
    HAS_RLM = True
except ImportError:
    HAS_RLM = False

# V9.0 â‘¡ AWM åˆæˆç¯å¢ƒå·¥å‚ (arXiv:2602.10090)
try:
    from awm_env_factory import AWMEnvironmentFactory, SyntheticDataGenerator
    HAS_AWM = True
except ImportError:
    HAS_AWM = False

# V9.0 â‘¢ EnCompass æœç´¢å¼•æ“ (NeurIPS 2025, arXiv:2512.03571)
try:
    from search_engine import (
        SearchConfig, BeamSearch, TwoLevelBeamSearch,
        EnCompassExecutor, BranchPoint, ExecutionPath,
    )
    HAS_SEARCH = True
except ImportError:
    HAS_SEARCH = False

# V9.0 â‘£ ç»“æ„åŒ–æ¨ç†æ¨¡æ¿ (arXiv:2602.09276)
try:
    from reasoning_templates import (
        TemplateSelector, ReasoningExecutor, PromptCompiler,
        ReasoningTemplate, ReasoningMultiAgentAdapter,
    )
    HAS_REASONING = True
except ImportError:
    HAS_REASONING = False

# V9.0 â‘¤ ä¸‰ç»´è®°å¿†æ¶æ„ (arXiv:2512.13564)
try:
    from memory_v9 import (
        Memory3DStore, Memory3DNode, MemoryForm, MemoryFunction,
        MemoryEvolutionEngine,
    )
    HAS_MEM3D = True
except ImportError:
    HAS_MEM3D = False

# V9.0 â‘¥ LatentLens å¯è§£é‡Šæ€§å±‚ (arXiv:2602.00462)
try:
    from interpretability_layer import (
        ProcessTracer, IntentMapper, OutputAttributor,
        FullExplanation,
    )
    HAS_INTERP = True
except ImportError:
    HAS_INTERP = False

# V9.0 â‘¦ å¤šç»´è¯„ä¼°æ¡†æ¶ (ç»¼åˆå…­ç¯‡è®ºæ–‡)
try:
    from evals_v9 import V9EvaluationFramework, V9EvalReport, EvalDimension
    HAS_EVALS_V9 = True
except ImportError:
    HAS_EVALS_V9 = False

# V10.1 â‘§ Deep Agents 0.4.1 (LangChain å®˜æ–¹)
# pip install deepagents>=0.4.1
# docs: docs.langchain.com/oss/python/deepagents
HAS_DEEP_AGENTS = False
_deep_agent = None
try:
    from deepagents import create_deep_agent
    from langchain.chat_models import init_chat_model
    HAS_DEEP_AGENTS = True
    logger.info("âœ… deepagents 0.4.1+ å·²åŠ è½½")
except ImportError:
    create_deep_agent = None
    init_chat_model = None


def _get_deep_agent():
    """
    å»¶è¿Ÿåˆå§‹åŒ– Deep Agent â€” deepagents 0.4.1
    è¿”å› compiled LangGraph graph
    æ”¯æŒ: planning + æ–‡ä»¶ç³»ç»Ÿ + å­agentç”Ÿæˆ
    """
    global _deep_agent
    if _deep_agent is None and HAS_DEEP_AGENTS:
        # è‡ªå®šä¹‰å·¥å…· (å¯é€‰)
        custom_tools = []
        if HAS_TOOLS:
            try:
                from tool_registry import sales_tools
                custom_tools = list(sales_tools.values())[:5]
            except Exception:
                pass

        try:
            _deep_agent = create_deep_agent(
                model=init_chat_model(
                    "anthropic:claude-sonnet-4-5-20250929"
                ),
                tools=custom_tools,
                system_prompt=(
                    "ä½ æ˜¯ MRARFAI V10.1 æ·±åº¦åˆ†æAgentã€‚"
                    "ä½ å¯ä»¥è§„åˆ’ä»»åŠ¡ã€å§”æ´¾å­Agentã€"
                    "ç®¡ç†æ–‡ä»¶ã€‚ä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚"
                ),
            )
            logger.info("âœ… Deep Agent åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"Deep Agent åˆå§‹åŒ–å¤±è´¥: {e}")
            _deep_agent = None
    return _deep_agent


# V9.0 å…¨å±€å®ä¾‹
_v9_tracer: 'ProcessTracer' = None          # å¯è§£é‡Šæ€§è¿½è¸ªå™¨
_v9_reasoning: 'TemplateSelector' = None    # æ¨ç†æ¨¡æ¿é€‰æ‹©å™¨
_v9_memory: 'Memory3DStore' = None          # ä¸‰ç»´è®°å¿†

def _get_v9_tracer() -> 'ProcessTracer':
    global _v9_tracer
    if _v9_tracer is None and HAS_INTERP:
        _v9_tracer = ProcessTracer()
    return _v9_tracer

def _get_v9_reasoning() -> 'TemplateSelector':
    global _v9_reasoning
    if _v9_reasoning is None and HAS_REASONING:
        _v9_reasoning = TemplateSelector()
    return _v9_reasoning

def _get_v9_memory() -> 'Memory3DStore':
    global _v9_memory
    if _v9_memory is None and HAS_MEM3D:
        _v9_memory = Memory3DStore(db_path="memory_v9.db")
    return _v9_memory


# ============================================================
# [ä¸å˜] Agent è®°å¿†ç³»ç»Ÿ â€” å…¼å®¹v2.1
# ============================================================

class AgentMemory:
    """
    å¤šè½®å¯¹è¯è®°å¿†
    - çŸ­æœŸè®°å¿†: æœ€è¿‘Nè½®QA
    - å®ä½“è®°å¿†: æåˆ°è¿‡çš„å®¢æˆ·/æ•°æ®ç‚¹
    - åˆ†ææ‘˜è¦: æ¯è½®åˆ†æçš„æ ¸å¿ƒç»“è®º
    """

    def __init__(self, max_turns: int = 10):
        self.max_turns = max_turns
        self.conversation_history = deque(maxlen=max_turns)
        self.entity_mentions = {}
        self.analysis_summaries = deque(maxlen=max_turns)
        self.risk_confirmations = {}

    def add_turn(self, question: str, answer: str, agents_used: list = None,
                 expert_outputs: dict = None):
        turn = {
            'time': datetime.now().isoformat(),
            'question': question,
            'answer_preview': answer[:200],
            'agents': agents_used or [],
        }
        self.conversation_history.append(turn)
        for name_candidate in self._extract_entities(question + " " + answer):
            if name_candidate not in self.entity_mentions:
                self.entity_mentions[name_candidate] = []
            self.entity_mentions[name_candidate].append(question[:50])
        if expert_outputs:
            for expert, output in expert_outputs.items():
                self.analysis_summaries.append({
                    'expert': expert,
                    'summary': output[:150],
                    'question': question[:50],
                })

    def add_risk_confirmation(self, customer: str, confirmed: bool):
        self.risk_confirmations[customer] = {
            'confirmed': confirmed,
            'time': datetime.now().isoformat(),
        }

    def get_context_prompt(self) -> str:
        if not self.conversation_history:
            return ""
        lines = ["[ä¹‹å‰çš„å¯¹è¯è®°å¿†]"]
        for turn in list(self.conversation_history)[-5:]:
            lines.append(f"Q: {turn['question'][:80]}")
            lines.append(f"A: {turn['answer_preview'][:100]}...")
        if self.risk_confirmations:
            lines.append("\n[é£é™©ç¡®è®¤è®°å½•]")
            for cust, info in self.risk_confirmations.items():
                status = "å·²ç¡®è®¤å…³æ³¨" if info['confirmed'] else "å·²æ ‡è®°ä¸ºä½ä¼˜å…ˆ"
                lines.append(f"- {cust}: {status}")
        if self.entity_mentions:
            top_entities = sorted(
                self.entity_mentions.items(),
                key=lambda x: len(x[1]), reverse=True
            )[:5]
            if top_entities:
                lines.append("\n[ç”¨æˆ·å…³æ³¨çš„é‡ç‚¹å®¢æˆ·]")
                for name, mentions in top_entities:
                    lines.append(f"- {name} (æåˆ°{len(mentions)}æ¬¡)")
        return "\n".join(lines)

    def _extract_entities(self, text: str) -> list:
        entities = []
        for name in list(self.entity_mentions.keys()):
            if name in text:
                entities.append(name)
        return entities

    def register_known_entities(self, customer_names: list):
        for name in customer_names:
            if name not in self.entity_mentions:
                self.entity_mentions[name] = []

    def clear(self):
        self.conversation_history.clear()
        self.entity_mentions.clear()
        self.analysis_summaries.clear()
        self.risk_confirmations.clear()


_global_memory = AgentMemory()

def get_memory() -> AgentMemory:
    return _global_memory

def set_memory(mem: AgentMemory):
    global _global_memory
    _global_memory = mem


# ============================================================
# [ä¸å˜] HITL æ£€æµ‹
# ============================================================

def detect_hitl_triggers(results: dict, health_scores: list = None) -> list:
    triggers = []
    alerts = results.get('æµå¤±é¢„è­¦', [])
    for a in alerts:
        if 'é«˜' in a.get('é£é™©', ''):
            triggers.append({
                'customer': a['å®¢æˆ·'],
                'risk_level': 'ğŸ”´ é«˜é£é™©',
                'reason': a.get('åŸå› ', 'è¶‹åŠ¿ä¸‹æ»‘'),
                'amount': a.get('å¹´åº¦é‡‘é¢', 0),
                'action_required': 'éœ€è¦ç¡®è®¤æ˜¯å¦ç«‹å³å®‰æ’æ‹œè®¿',
            })
    if health_scores:
        for s in health_scores:
            if s['ç­‰çº§'] == 'F' and s['å¹´åº¦é‡‘é¢'] > 100:
                triggers.append({
                    'customer': s['å®¢æˆ·'],
                    'risk_level': 'ğŸ”´ å¥åº·åˆ†Fçº§',
                    'reason': f"å¥åº·è¯„åˆ†ä»…{s['æ€»åˆ†']}åˆ†ï¼Œ" + " ".join(s.get('é£é™©æ ‡ç­¾', [])),
                    'amount': s['å¹´åº¦é‡‘é¢'],
                    'action_required': 'éœ€è¦ç¡®è®¤æ˜¯å¦å¯åŠ¨å®¢æˆ·æŒ½å›è®¡åˆ’',
                })
    seen = set()
    unique = []
    for t in triggers:
        if t['customer'] not in seen:
            seen.add(t['customer'])
            unique.append(t)
    return unique


# ============================================================
# å‡çº§â‘  æ™ºèƒ½æ•°æ®æŸ¥è¯¢ â€” Text-to-Pandas
# ============================================================

class SmartDataQuery:
    """
    æ›¿ä»£æ—§ç‰ˆ query_sales_data() çš„å…³é”®è¯åŒ¹é…ã€‚
    
    åŸç†ï¼š
    1. ç»´æŠ¤ä¸€ä¸ªç»“æ„åŒ–çš„æ•°æ®ç´¢å¼•ï¼ˆschemaï¼‰
    2. ç”¨æˆ·æé—® â†’ LLM ç”ŸæˆæŸ¥è¯¢è®¡åˆ’ï¼ˆJSONï¼‰â†’ ç²¾ç¡®æå–æ•°æ®
    3. å¦‚æœ LLM ä¸å¯ç”¨ï¼Œé™çº§åˆ°å¢å¼ºç‰ˆå…³é”®è¯åŒ¹é…
    
    vs æ—§ç‰ˆï¼š
    - æ—§ç‰ˆï¼šå…³é”®è¯åŒ¹é… â†’ è¿”å›æ•´å—JSONï¼ˆç»å¸¸5000å­—æˆªæ–­ä¸¢å¤±ä¿¡æ¯ï¼‰
    - æ–°ç‰ˆï¼šç†è§£è¯­ä¹‰ â†’ åªè¿”å›ç›¸å…³æ•°æ® â†’ ç²¾å‡†ã€çœtoken
    """

    # æ•°æ®schemaå®šä¹‰ï¼ˆå‘Šè¯‰LLMæœ‰å“ªäº›æ•°æ®å¯æŸ¥ï¼‰
    SCHEMA = """
å¯æŸ¥è¯¢çš„æ•°æ®ç»´åº¦ï¼š
1. overview: æ€»è¥æ”¶, åŒæ¯”å¢é•¿ç‡, æœˆåº¦è¥æ”¶åˆ—è¡¨(1-12æœˆ), æ ¸å¿ƒå‘ç°
2. customers: å®¢æˆ·åˆ†çº§åˆ—è¡¨(å®¢æˆ·å/ç­‰çº§A|B|C/å¹´åº¦é‡‘é¢/H1/H2/å æ¯”/ç´¯è®¡å æ¯”), æ”¯æŒæŒ‰å®¢æˆ·åæˆ–ç­‰çº§ç­›é€‰
3. risks: æµå¤±é¢„è­¦åˆ—è¡¨(å®¢æˆ·/é£é™©ç­‰çº§/åŸå› /å¹´åº¦é‡‘é¢), å¼‚å¸¸æ£€æµ‹ç»“æœ
4. growth: å¢é•¿æœºä¼šåˆ—è¡¨(å®¢æˆ·/æœºä¼š/æ½œåŠ›é‡‘é¢)
5. price_volume: ä»·é‡åˆ†è§£(å®¢æˆ·/å•ä»·å˜åŒ–/æ•°é‡å˜åŒ–/é‡‘é¢å˜åŒ–)
6. regions: åŒºåŸŸåˆ†å¸ƒ(åŒºåŸŸ/é‡‘é¢/å æ¯”), HHIæŒ‡æ•°, Top3é›†ä¸­åº¦
7. categories: ä¸šåŠ¡ç±»åˆ«è¶‹åŠ¿(ç±»åˆ«/2024é‡‘é¢/2025é‡‘é¢/å¢é•¿ç‡)
8. benchmark: è¡Œä¸šå¯¹æ ‡(å¸‚åœºå®šä½/ç«äº‰å¯¹æ ‡/ç»“æ„æ€§é£é™©/æˆ˜ç•¥æœºä¼š)
9. forecast: é¢„æµ‹(æ€»è¥æ”¶é¢„æµ‹/å®¢æˆ·é¢„æµ‹/å“ç±»é¢„æµ‹/é£é™©åœºæ™¯)
"""

    def __init__(self, data: dict, results: dict, benchmark: dict = None, forecast: dict = None):
        self.data = data
        self.results = results
        self.benchmark = benchmark
        self.forecast = forecast
        # æ„å»ºç´¢å¼•
        self._index = self._build_index()
        # æ„å»ºçŸ¥è¯†å›¾è°±
        self.kg = None
        if HAS_KG:
            self.kg = SalesKnowledgeGraph()
            self.kg.build(data, results)

    def _build_index(self) -> dict:
        """æ„å»ºç»“æ„åŒ–æ•°æ®ç´¢å¼•"""
        index = {}

        # æ€»è§ˆ
        index['overview'] = {
            'æ€»è¥æ”¶': self.data.get('æ€»è¥æ”¶', 0),
            'æ€»YoY': self.data.get('æ€»YoY', {}),
            'æœˆåº¦è¥æ”¶': self.data.get('æœˆåº¦æ€»è¥æ”¶', []),
            'æ ¸å¿ƒå‘ç°': self.results.get('æ ¸å¿ƒå‘ç°', []),
            'æ´»è·ƒå®¢æˆ·æ•°': sum(1 for c in self.data.get('å®¢æˆ·é‡‘é¢', []) if c.get('å¹´åº¦é‡‘é¢', 0) > 0),
        }

        # å®¢æˆ·ï¼ˆå»ºç«‹åç§°â†’æ•°æ®çš„æ˜ å°„ï¼Œæ”¯æŒç²¾ç¡®æŸ¥è¯¢ï¼‰
        customers = self.results.get('å®¢æˆ·åˆ†çº§', [])
        index['customers'] = {
            'all': customers,
            'by_name': {c['å®¢æˆ·']: c for c in customers},
            'by_tier': {
                'A': [c for c in customers if c.get('ç­‰çº§') == 'A'],
                'B': [c for c in customers if c.get('ç­‰çº§') == 'B'],
                'C': [c for c in customers if c.get('ç­‰çº§') == 'C'],
            },
            'top5': customers[:5],
            'top10': customers[:10],
        }

        # é£é™©
        index['risks'] = {
            'alerts': self.results.get('æµå¤±é¢„è­¦', []),
            'high_risk': [a for a in self.results.get('æµå¤±é¢„è­¦', []) if 'é«˜' in a.get('é£é™©', '')],
            'anomalies': self.results.get('MoMå¼‚å¸¸', [])[:10],
        }

        # å¢é•¿
        index['growth'] = self.results.get('å¢é•¿æœºä¼š', [])

        # ä»·é‡åˆ†è§£
        index['price_volume'] = self.results.get('ä»·é‡åˆ†è§£', [])

        # åŒºåŸŸ
        index['regions'] = self.results.get('åŒºåŸŸæ´å¯Ÿ', {})

        # ç±»åˆ«
        index['categories'] = self.results.get('ç±»åˆ«è¶‹åŠ¿', [])

        # è¡Œä¸šå¯¹æ ‡
        if self.benchmark:
            index['benchmark'] = {
                'å¸‚åœºå®šä½': self.benchmark.get('å¸‚åœºå®šä½', {}),
                'ç«äº‰å¯¹æ ‡': self.benchmark.get('ç«äº‰å¯¹æ ‡', {}),
                'ç»“æ„æ€§é£é™©': self.benchmark.get('ç»“æ„æ€§é£é™©', []),
                'æˆ˜ç•¥æœºä¼š': self.benchmark.get('æˆ˜ç•¥æœºä¼š', []),
                'å®¢æˆ·å¤–éƒ¨è§†è§’': self.benchmark.get('å®¢æˆ·å¤–éƒ¨è§†è§’', []),
            }

        # é¢„æµ‹
        if self.forecast:
            index['forecast'] = {
                'æ€»è¥æ”¶é¢„æµ‹': self.forecast.get('æ€»è¥æ”¶é¢„æµ‹', {}),
                'å®¢æˆ·é¢„æµ‹': self.forecast.get('å®¢æˆ·é¢„æµ‹', []),
                'å“ç±»é¢„æµ‹': self.forecast.get('å“ç±»é¢„æµ‹', []),
                'é£é™©åœºæ™¯': self.forecast.get('é£é™©åœºæ™¯', {}),
            }

        return index

    def query_by_plan(self, plan: dict) -> str:
        """
        æ ¹æ®æŸ¥è¯¢è®¡åˆ’ç²¾ç¡®æå–æ•°æ®ã€‚
        
        plan æ ¼å¼:
        {
            "dimensions": ["overview", "customers"],   # éœ€è¦å“ªäº›ç»´åº¦
            "filters": {"customer_name": "HMD", "tier": "A"},  # ç­›é€‰æ¡ä»¶
            "metrics": ["å¹´åº¦é‡‘é¢", "å¢é•¿ç‡"],           # éœ€è¦å“ªäº›æŒ‡æ ‡
            "limit": 10                                 # è¿”å›æ¡æ•°
        }
        """
        result = {}
        dims = plan.get('dimensions', [])
        filters = plan.get('filters', {})
        limit = plan.get('limit', 15)

        for dim in dims:
            if dim == 'overview':
                result['overview'] = self._index.get('overview', {})

            elif dim == 'customers':
                customers = self._index.get('customers', {})
                # æŒ‰åç§°ç­›é€‰
                if 'customer_name' in filters:
                    name = filters['customer_name']
                    # æ¨¡ç³ŠåŒ¹é…
                    matched = []
                    for cname, cdata in customers.get('by_name', {}).items():
                        if name.lower() in cname.lower():
                            matched.append(cdata)
                    result['customers'] = matched if matched else [f"æœªæ‰¾åˆ°å®¢æˆ·: {name}"]
                # æŒ‰ç­‰çº§ç­›é€‰
                elif 'tier' in filters:
                    tier = filters['tier'].upper()
                    result['customers'] = customers.get('by_tier', {}).get(tier, [])[:limit]
                # æŒ‰Top N
                elif 'top_n' in filters:
                    n = min(int(filters['top_n']), 30)
                    result['customers'] = customers.get('all', [])[:n]
                else:
                    result['customers'] = customers.get('top10', [])

            elif dim == 'risks':
                risks = self._index.get('risks', {})
                if filters.get('level') == 'high':
                    result['risks'] = risks.get('high_risk', [])
                else:
                    result['risks'] = {
                        'alerts': risks.get('alerts', [])[:limit],
                        'anomalies': risks.get('anomalies', [])[:5],
                    }

            elif dim == 'growth':
                result['growth'] = self._index.get('growth', [])[:limit]

            elif dim == 'price_volume':
                pv = self._index.get('price_volume', [])
                if 'customer_name' in filters:
                    name = filters['customer_name']
                    result['price_volume'] = [
                        p for p in pv if name.lower() in p.get('å®¢æˆ·', '').lower()
                    ][:limit]
                else:
                    result['price_volume'] = pv[:limit]

            elif dim == 'regions':
                result['regions'] = self._index.get('regions', {})

            elif dim == 'categories':
                result['categories'] = self._index.get('categories', [])

            elif dim == 'benchmark':
                result['benchmark'] = self._index.get('benchmark', {})

            elif dim == 'forecast':
                result['forecast'] = self._index.get('forecast', {})

        if not result:
            result = self._index.get('overview', {})

        return json.dumps(result, ensure_ascii=False, indent=1, default=str)

    def query_smart(self, question: str, provider: str = "", api_key: str = "") -> str:
        """
        æ™ºèƒ½æŸ¥è¯¢å…¥å£ï¼ˆv3.1 é›†æˆçŸ¥è¯†å›¾è°±ï¼‰ï¼š
        1. çŸ¥è¯†å›¾è°±ç†è§£ â†’ ç»“æ„åŒ–æŸ¥è¯¢è®¡åˆ’ï¼ˆé›¶APIè°ƒç”¨ï¼‰
        2. LLMç”ŸæˆæŸ¥è¯¢è®¡åˆ’ï¼ˆæœ‰KGä¸Šä¸‹æ–‡åŠ æŒï¼‰
        3. é™çº§åˆ°å¢å¼ºç‰ˆè§„åˆ™åŒ¹é…
        """
        # åˆå§‹åŒ–å…ƒæ•°æ®
        self._last_entity_context = ''
        self._last_pattern = ''
        self._last_agent_hint = []
        self._last_corrections = []

        # ä¼˜å…ˆç”¨çŸ¥è¯†å›¾è°±ï¼ˆé›¶APIè°ƒç”¨ï¼Œæ¯«ç§’çº§ï¼‰
        if self.kg:
            kg_plan = self.kg.understand(question)
            plan = {
                'dimensions': kg_plan['dimensions'],
                'filters': kg_plan['filters'],
                'limit': kg_plan.get('limit', 15),
            }
            self._last_entity_context = kg_plan.get('entity_context', '')
            self._last_pattern = kg_plan.get('pattern', '')
            self._last_agent_hint = kg_plan.get('agent_hint', [])
            self._last_corrections = kg_plan.get('corrections', [])
            return self.query_by_plan(plan)

        # å°è¯•LLMç”ŸæˆæŸ¥è¯¢è®¡åˆ’
        if api_key:
            plan = self._llm_generate_plan(question, provider, api_key)
            if plan:
                return self.query_by_plan(plan)

        # é™çº§ï¼šå¢å¼ºç‰ˆè§„åˆ™åŒ¹é…
        return self.query_by_plan(self._rule_based_plan(question))

    def _llm_generate_plan(self, question: str, provider: str, api_key: str) -> Optional[dict]:
        """ç”¨LLMå°†è‡ªç„¶è¯­è¨€è½¬ä¸ºç»“æ„åŒ–æŸ¥è¯¢è®¡åˆ’"""
        system = f"""ä½ æ˜¯æ•°æ®æŸ¥è¯¢è§„åˆ’å™¨ã€‚æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œç”ŸæˆJSONæŸ¥è¯¢è®¡åˆ’ã€‚

{self.SCHEMA}

è¾“å‡ºæ ¼å¼ï¼ˆçº¯JSONï¼Œæ— å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
    "dimensions": ["overview", "customers"],
    "filters": {{"customer_name": "HMD"}},
    "limit": 10
}}

filters å¯é€‰é”®ï¼šcustomer_name, tier(A/B/C), level(high/medium/low), top_n
"""
        try:
            raw = _call_llm_raw(system, f"ç”¨æˆ·é—®é¢˜ï¼š{question}", provider, api_key,
                                max_tokens=200, temperature=0.0,
                                _trace_name="query_plan_llm")
            # æå–JSON
            raw = raw.strip()
            if raw.startswith("```"):
                raw = raw.split("```")[1]
                if raw.startswith("json"):
                    raw = raw[4:]
            plan = json.loads(raw)
            if 'dimensions' in plan:
                return plan
        except Exception:
            pass
        return None

    def _rule_based_plan(self, question: str) -> dict:
        """å¢å¼ºç‰ˆè§„åˆ™åŒ¹é…ï¼ˆæ¯”v2.1æ›´ç²¾å‡†ï¼‰"""
        q = question.lower()
        dims = []
        filters = {}

        # ç»´åº¦æ£€æµ‹
        if any(k in q for k in ['æ€»', 'è¥æ”¶', 'æ”¶å…¥', 'æ¦‚è§ˆ', 'å…¨éƒ¨', 'å¤šå°‘']):
            dims.append('overview')
        if any(k in q for k in ['å®¢æˆ·', 'åˆ†çº§', 'abc', 'æ’å', 'top']):
            dims.append('customers')
            # æå–Top N
            for word in ['top5', 'top10', 'top15', 'top20', 'å‰5', 'å‰10', 'å‰15', 'å‰20']:
                if word in q:
                    n = ''.join(filter(str.isdigit, word))
                    filters['top_n'] = int(n) if n else 10
            # æå–ç­‰çº§
            if 'açº§' in q or 'aç±»' in q:
                filters['tier'] = 'A'
            elif 'bçº§' in q or 'bç±»' in q:
                filters['tier'] = 'B'
            elif 'cçº§' in q or 'cç±»' in q:
                filters['tier'] = 'C'
        if any(k in q for k in ['é£é™©', 'æµå¤±', 'é¢„è­¦', 'å¼‚å¸¸', 'å±é™©']):
            dims.append('risks')
            if 'é«˜' in q:
                filters['level'] = 'high'
        if any(k in q for k in ['å¢é•¿', 'æœºä¼š', 'æ½œåŠ›']):
            dims.append('growth')
        if any(k in q for k in ['ä»·', 'å•ä»·', 'é‡', 'ä»·é‡']):
            dims.append('price_volume')
        if any(k in q for k in ['åŒºåŸŸ', 'å¸‚åœº', 'åœ°åŒº']):
            dims.append('regions')
        if any(k in q for k in ['ç±»åˆ«', 'å“ç±»', 'äº§å“', 'ç»“æ„']):
            dims.append('categories')
        if any(k in q for k in ['è¡Œä¸š', 'ç«äº‰', 'å¯¹æ ‡', 'åå‹¤', 'é—»æ³°', 'é¾™æ——']):
            dims.append('benchmark')
        if any(k in q for k in ['é¢„æµ‹', '2026', 'æœªæ¥', 'å‰æ™¯', 'ä¸‹å­£']):
            dims.append('forecast')

        # å®¢æˆ·åç²¾ç¡®åŒ¹é…
        for cname in self._index.get('customers', {}).get('by_name', {}).keys():
            if cname.lower() in q:
                dims.append('customers')
                if 'price_volume' not in dims and any(k in q for k in ['ä»·', 'é‡']):
                    dims.append('price_volume')
                filters['customer_name'] = cname
                break

        if not dims:
            dims = ['overview', 'customers', 'risks']

        return {'dimensions': list(set(dims)), 'filters': filters, 'limit': 15}


# å…¨å±€æŸ¥è¯¢å®ä¾‹
_smart_query: Optional[SmartDataQuery] = None

def get_smart_query() -> Optional[SmartDataQuery]:
    return _smart_query


# å…¼å®¹æ—§ç‰ˆæ¥å£
_sales_data_store = {}

def set_sales_data(data_store: dict):
    global _sales_data_store
    _sales_data_store = data_store

def query_sales_data(query: str) -> str:
    """å…¼å®¹æ—§æ¥å£ï¼Œå†…éƒ¨ä½¿ç”¨SmartDataQuery"""
    sq = get_smart_query()
    if sq:
        return sq.query_smart(query)
    # å®Œå…¨é™çº§
    return _legacy_query(query)

def _legacy_query(query: str) -> str:
    """æ—§ç‰ˆå…³é”®è¯æŸ¥è¯¢ï¼ˆæœ€ç»ˆé™çº§æ–¹æ¡ˆï¼‰"""
    ds = _sales_data_store
    if not ds:
        return "æ•°æ®æœªåŠ è½½"
    q = query.lower()
    result = {}
    if any(k in q for k in ['æ€»', 'è¥æ”¶', 'æ”¶å…¥', 'æ¦‚è§ˆ', 'å…¨éƒ¨']):
        result['æ€»è¥æ”¶'] = ds.get('æ€»è¥æ”¶')
        result['æ€»YoY'] = ds.get('æ€»YoY')
        result['æ ¸å¿ƒå‘ç°'] = ds.get('æ ¸å¿ƒå‘ç°')
    if any(k in q for k in ['å®¢æˆ·', 'åˆ†çº§', 'abc', 'æ’å', 'top']):
        result['å®¢æˆ·åˆ†çº§'] = ds.get('å®¢æˆ·åˆ†çº§', [])[:15]
    if any(k in q for k in ['é£é™©', 'æµå¤±', 'é¢„è­¦']):
        result['æµå¤±é¢„è­¦'] = ds.get('æµå¤±é¢„è­¦')
    if any(k in q for k in ['å¢é•¿', 'æœºä¼š']):
        result['å¢é•¿æœºä¼š'] = ds.get('å¢é•¿æœºä¼š')
    if not result:
        result = {'æ€»è¥æ”¶': ds.get('æ€»è¥æ”¶'), 'æ ¸å¿ƒå‘ç°': ds.get('æ ¸å¿ƒå‘ç°')}
    return json.dumps(result, ensure_ascii=False, indent=1, default=str)[:5000]


# ============================================================
# [ä¸å˜] Agent è§’è‰²å®šä¹‰
# ============================================================

AGENT_PROFILES = {
    "analyst": {
        "name": "ğŸ“Š æ•°æ®åˆ†æå¸ˆ",
        "emoji": "ğŸ“Š",
        "role": "ç¦¾è‹—é€šè®¯èµ„æ·±æ•°æ®åˆ†æå¸ˆ",
        "goal": "ç²¾å‡†è§£è¯»é”€å”®æ•°æ®ï¼Œç”¨æ•°å­—æ­ç¤ºä¸šåŠ¡çœŸç›¸ï¼Œè¯†åˆ«è¶‹åŠ¿å’Œæ¨¡å¼",
        "backstory": (
            "ä½ åœ¨æ¶ˆè´¹ç”µå­ODMè¡Œä¸šæœ‰15å¹´æ•°æ®åˆ†æç»éªŒï¼Œæ›¾æœåŠ¡åå‹¤ã€é—»æ³°ç­‰å¤´éƒ¨ä¼ä¸šã€‚"
            "ä½ ä»¥æ•°æ®é©±åŠ¨è‘—ç§°ï¼Œæ¯ä¸ªç»“è®ºå¿…é¡»æœ‰æ•°å­—æ”¯æ’‘ã€‚"
            "ä½ æ“…é•¿å‘ç°æœˆåº¦æ³¢åŠ¨è§„å¾‹ã€å®¢æˆ·é›†ä¸­åº¦é£é™©ã€åŒæ¯”ç¯æ¯”å¼‚å¸¸ã€‚"
            "ä½ çš„åˆ†æé£æ ¼ï¼šç²¾å‡†ã€å®¢è§‚ã€é‡åŒ–ã€‚å…ˆç»™ç»“è®ºï¼Œå†ç»™æ•°æ®ã€‚"
        ),
        "keywords": [
            "è¥æ”¶", "æ”¶å…¥", "é‡‘é¢", "å‡ºè´§", "æ•°é‡", "å®¢æˆ·", "åˆ†çº§",
            "ABC", "æ’å", "top", "å æ¯”", "é›†ä¸­", "æœˆåº¦", "å­£åº¦",
            "åŒæ¯”", "ç¯æ¯”", "è¶‹åŠ¿", "æ€»è§ˆ", "æ¦‚è§ˆ", "å¤šå°‘",
            "äº§å“", "ç»“æ„", "åŒºåŸŸ",
        ],
        "model_tier": "standard",  # v7.0: æ¨¡å‹è·¯ç”±
    },
    "risk": {
        "name": "ğŸ›¡ï¸ é£æ§ä¸“å®¶",
        "emoji": "ğŸ›¡ï¸",
        "role": "ç¦¾è‹—é€šè®¯é£é™©æ§åˆ¶ä¸“å®¶",
        "goal": "è¯†åˆ«å®¢æˆ·æµå¤±é£é™©å’Œå¼‚å¸¸æ³¢åŠ¨ï¼Œé‡åŒ–é£é™©é‡‘é¢ï¼Œæä¾›é¢„é˜²æ–¹æ¡ˆ",
        "backstory": (
            "ä½ æ˜¯å‰å®‰æ°¸é£é™©å’¨è¯¢æ€»ç›‘ï¼Œä¸“æ³¨TMTè¡Œä¸šå®¢æˆ·é£é™©ç®¡ç†ã€‚"
            "ä½ å¯¹æ•°æ®å¼‚å¸¸æå…¶æ•æ„Ÿâ€”â€”æ–­å´–å¼ä¸‹è·Œã€è¿ç»­Næœˆè¡°é€€ã€å¤§å®¢æˆ·é›†ä¸­åº¦è¿‡é«˜ï¼Œ"
            "è¿™äº›ä½ ä¸€çœ¼å°±èƒ½çœ‹å‡ºã€‚ä½ çš„é£æ ¼ï¼šç›´è¨€ä¸è®³ï¼Œå‘ç°é—®é¢˜å°±è¯´ã€‚"
            "è¾“å‡ºæ ¼å¼ï¼šé£é™©ç­‰çº§â†’å½±å“é‡‘é¢â†’åŸå› åˆ†æâ†’åº”å¯¹å»ºè®®ã€‚"
            "\n\nâš ï¸ é‡è¦ï¼šå¦‚æœå‘ç°é«˜é£é™©å®¢æˆ·ï¼ˆå¹´åº¦é‡‘é¢>200ä¸‡ä¸”æŒç»­ä¸‹æ»‘ï¼‰ï¼Œ"
            "è¯·åœ¨è¾“å‡ºå¼€å¤´æ ‡è®° [HIGH_RISK_ALERT] å¹¶åˆ—å‡ºå®¢æˆ·åå’Œé‡‘é¢ã€‚"
        ),
        "keywords": [
            "é£é™©", "æµå¤±", "é¢„è­¦", "ä¸‹é™", "ä¸‹æ»‘", "ä¸¢å¤±", "æ–­å´–",
            "å¼‚å¸¸", "æš´è·Œ", "å±é™©", "è­¦å‘Š", "å…³æ³¨", "é—®é¢˜",
            "æ³¢åŠ¨", "åç¦»", "ä¸æ­£å¸¸",
        ],
        "model_tier": "advanced",  # é£é™©åˆ†æéœ€è¦é«˜ç²¾åº¦
    },
    "strategist": {
        "name": "ğŸ’¡ ç­–ç•¥å¸ˆ",
        "emoji": "ğŸ’¡",
        "role": "ç¦¾è‹—é€šè®¯æˆ˜ç•¥é¡¾é—®",
        "goal": "å‘ç°å¢é•¿æœºä¼šï¼Œåˆ¶å®šå¯æ‰§è¡Œçš„æˆ˜ç•¥æ–¹æ¡ˆï¼Œä¼˜åŒ–èµ„æºé…ç½®",
        "backstory": (
            "ä½ æ˜¯å‰éº¦è‚¯é”¡TMTè¡Œä¸šåˆä¼™äººï¼Œä¸“æ³¨æ‰‹æœºODM/OEMèµ›é“æˆ˜ç•¥è§„åˆ’ã€‚"
            "ä½ æ“…é•¿ç«äº‰åˆ†æï¼ˆvsåå‹¤/é—»æ³°/é¾™æ——ï¼‰ã€å¢é•¿æœºä¼šè¯†åˆ«ã€"
            "äº§å“ç»„åˆä¼˜åŒ–ã€å®¢æˆ·é’±åŒ…ä»½é¢æå‡ç­–ç•¥ã€‚"
            "ä½ çš„é£æ ¼ï¼šå‰ç»æ€§ã€å®ç”¨ä¸»ä¹‰ã€èšç„¦ROIã€‚å»ºè®®å¿…é¡»å¯æ‰§è¡Œã€‚"
            "è¾“å‡ºæ ¼å¼ï¼šæœºä¼š/æ–¹å‘â†’æ½œåœ¨ä»·å€¼â†’å…·ä½“è¡ŒåŠ¨â†’ä¼˜å…ˆçº§ã€‚"
        ),
        "keywords": [
            "å¢é•¿", "æœºä¼š", "æˆ˜ç•¥", "ç­–ç•¥", "å»ºè®®", "æ–¹å‘", "æŠ•å…¥",
            "ç«äº‰", "å¯¹æ‰‹", "åå‹¤", "é—»æ³°", "é¾™æ——", "è¡Œä¸š", "å¯¹æ ‡",
            "é¢„æµ‹", "forecast", "2026", "æœªæ¥", "å‰æ™¯",
            "CEO", "ç®¡ç†", "å†³ç­–", "ä¼˜åŒ–", "æå‡",
            "ä»·æ ¼", "ä»·é‡", "åˆ©æ¶¦",
        ],
        "model_tier": "advanced",
    },
    # â”€â”€ V10.0 åŸŸ Agent (Engine-based, é LLM è§’è‰²) â”€â”€
    "quality": {
        "name": "ğŸ”¬ å“è´¨ä¸“å®¶",
        "emoji": "ğŸ”¬",
        "role": "ç¦¾è‹—é€šè®¯å“è´¨ç®¡æ§ä¸“å®¶",
        "goal": "ç›‘æ§è‰¯ç‡ã€åˆ†æé€€è´§ã€è¿½æº¯ç¼ºé™·æ ¹å› ",
        "backstory": "V10åŸŸå¼•æ“Agentï¼Œç›´æ¥è°ƒç”¨QualityEngineè¿”å›ç»“æ„åŒ–æ•°æ®ï¼Œä¸ç»è¿‡LLMã€‚",
        "keywords": [
            "è‰¯ç‡", "yield", "é€€è´§", "return", "å“è´¨", "quality",
            "ç¼ºé™·", "defect", "æŠ•è¯‰", "complaint", "æ ¹å› ", "root cause",
            "åˆæ ¼ç‡", "ä¸è‰¯", "äº§çº¿",
        ],
        "model_tier": "engine",  # æ ‡è®°ä¸ºå¼•æ“Agentï¼Œä¸èµ°LLM
        "engine_type": "quality",
    },
    "market": {
        "name": "ğŸ“ˆ å¸‚åœºä¸“å®¶",
        "emoji": "ğŸ“ˆ",
        "role": "ç¦¾è‹—é€šè®¯å¸‚åœºåˆ†æä¸“å®¶",
        "goal": "ç«å¯¹ç›‘æ§ã€è¡Œä¸šè¶‹åŠ¿åˆ†æã€å¸‚åœºæƒ…ç»ªè¿½è¸ª",
        "backstory": "V10åŸŸå¼•æ“Agentï¼Œç›´æ¥è°ƒç”¨MarketEngineè¿”å›ç»“æ„åŒ–æ•°æ®ã€‚",
        "keywords": [
            "å¸‚åœº", "market", "ç«å¯¹", "competitor", "é—»æ³°", "åå‹¤", "é¾™æ——",
            "è¡Œä¸šè¶‹åŠ¿", "trend", "æƒ…ç»ª", "sentiment", "ä»½é¢", "share",
            "å‡ºè´§é‡", "æ’å",
        ],
        "model_tier": "engine",
        "engine_type": "market",
    },
    "finance": {
        "name": "ğŸ’° è´¢åŠ¡ä¸“å®¶",
        "emoji": "ğŸ’°",
        "role": "ç¦¾è‹—é€šè®¯è´¢åŠ¡åˆ†æä¸“å®¶",
        "goal": "åº”æ”¶è´¦æ¬¾è¿½è¸ªã€æ¯›åˆ©åˆ†æã€ç°é‡‘æµé¢„æµ‹ã€å‘ç¥¨åŒ¹é…",
        "backstory": "V10åŸŸå¼•æ“Agentï¼Œç›´æ¥è°ƒç”¨FinanceEngineè¿”å›ç»“æ„åŒ–æ•°æ®ã€‚",
        "keywords": [
            "åº”æ”¶", "AR", "è´¦æ¬¾", "receivable", "æ¯›åˆ©", "margin",
            "åˆ©æ¶¦", "profit", "ç°é‡‘æµ", "cashflow", "å‘ç¥¨", "invoice",
            "è´¦æœŸ", "DSO", "å›æ¬¾",
        ],
        "model_tier": "engine",
        "engine_type": "finance",
    },
    "procurement": {
        "name": "ğŸ“¦ é‡‡è´­ä¸“å®¶",
        "emoji": "ğŸ“¦",
        "role": "ç¦¾è‹—é€šè®¯é‡‡è´­ç®¡ç†ä¸“å®¶",
        "goal": "ä¾›åº”å•†è¯„ä¼°ã€é‡‡è´­å•è¿½è¸ªã€å»¶æœŸé¢„è­¦ã€æˆæœ¬åˆ†æ",
        "backstory": "V10åŸŸå¼•æ“Agentï¼Œç›´æ¥è°ƒç”¨ProcurementEngineè¿”å›ç»“æ„åŒ–æ•°æ®ã€‚",
        "keywords": [
            "é‡‡è´­", "procurement", "ä¾›åº”å•†", "supplier", "PO",
            "é‡‡è´­å•", "å»¶æœŸ", "delay", "æˆæœ¬", "cost", "æŠ¥ä»·", "quote",
            "ç‰©æ–™", "äº¤æœŸ",
        ],
        "model_tier": "engine",
        "engine_type": "procurement",
    },
}

REPORTER_PROFILE = {
    "role": "ç¦¾è‹—é€šè®¯é«˜çº§æŠ¥å‘Šæ’°å†™äºº",
    "goal": "ç»¼åˆå¤šä½ä¸“å®¶åˆ†æï¼Œç”Ÿæˆç®€æ´æœ‰åŠ›çš„ç»¼åˆæŠ¥å‘Šï¼Œé€‚åˆCEOé˜…è¯»",
    "backstory": (
        "ä½ æ˜¯å‰FTä¸­æ–‡ç½‘èµ„æ·±ç¼–è¾‘ï¼Œç°ä»»ç¦¾è‹—é€šè®¯æˆ˜ç•¥åˆ†æéƒ¨è´Ÿè´£äººã€‚"
        "ä½ æ“…é•¿å°†å¤æ‚çš„æ•°æ®åˆ†æå’Œå¤šæ–¹è§‚ç‚¹æç‚¼ä¸ºç®¡ç†å±‚å¯ç›´æ¥è¡ŒåŠ¨çš„å»ºè®®ã€‚"
        "è§„åˆ™ï¼š1.ä¸ç®€å•æ‹¼å‡‘ 2.å…ˆæ ¸å¿ƒç»“è®º 3.åˆ†æ¨¡å—å±•å¼€ 4.æœ€åç»™è¡ŒåŠ¨é¡¹ 5.æ§åˆ¶500å­—"
    ),
    "model_tier": "standard",
}


# ============================================================
# V10.0 åŸŸ Agent å¼•æ“ç®¡ç†å™¨
# ============================================================

_domain_engines: Dict[str, Any] = {}


def _init_domain_engines():
    """åˆå§‹åŒ–åŸŸ Agent å¼•æ“ â€” ä¼˜å…ˆ DB æ•°æ®ï¼Œå¦åˆ™å›é€€ SAMPLE_"""
    global _domain_engines
    if _domain_engines:
        return _domain_engines

    # 1. å°è¯•ä»æ•°æ®åº“åŠ è½½
    if HAS_DB_BRIDGE:
        try:
            db_engines = create_engines_from_db()
            if db_engines:
                _domain_engines.update(db_engines)
                logger.info(f"DB Bridge â†’ åŠ è½½ {len(db_engines)} ä¸ªåŸŸå¼•æ“: {list(db_engines.keys())}")
        except Exception as e:
            logger.warning(f"DB Bridge å¤±è´¥: {e}")

    # 2. æœªä» DB è·å–åˆ°çš„å¼•æ“ â†’ ä½¿ç”¨é»˜è®¤æ„é€ ï¼ˆå« SAMPLE_ å›é€€ï¼‰
    engine_map = {
        "quality": (HAS_QUALITY, lambda: QualityEngine()),
        "market": (HAS_MARKET, lambda: MarketEngine()),
        "finance": (HAS_FINANCE, lambda: FinanceEngine()),
        "procurement": (HAS_PROCUREMENT, lambda: ProcurementEngine()),
        "risk": (HAS_RISK_ENGINE, lambda: RiskEngine()),
        "strategist": (HAS_STRATEGIST_ENGINE, lambda: StrategistEngine()),
    }

    for name, (available, factory) in engine_map.items():
        if name not in _domain_engines and available:
            try:
                _domain_engines[name] = factory()
                logger.info(f"åŸŸå¼•æ“ {name} â†’ é»˜è®¤åˆå§‹åŒ–")
            except Exception as e:
                logger.error(f"åŸŸå¼•æ“ {name} åˆå§‹åŒ–å¤±è´¥: {e}")

    return _domain_engines


def get_domain_engine(name: str):
    """è·å–æŒ‡å®šåŸŸå¼•æ“å®ä¾‹"""
    engines = _init_domain_engines()
    return engines.get(name)


# ============================================================
# å‡çº§â‘¡ LLMæ™ºèƒ½è·¯ç”±
# ============================================================

class SmartRouter:
    """
    æ›¿ä»£æ—§ç‰ˆ route_to_agents() çš„å…³é”®è¯åŒ¹é…ã€‚
    
    åŸç†ï¼š
    1. ä¸€æ¬¡è½»é‡LLMè°ƒç”¨ï¼ˆ<100 tokensï¼‰ï¼Œåˆ¤æ–­éœ€è¦å“ªäº›Agent
    2. è¿”å›ç½®ä¿¡åº¦åˆ†æ•°ï¼Œä½ç½®ä¿¡åº¦çš„Agentè·³è¿‡ â†’ çœé’±çœæ—¶é—´
    3. LLMä¸å¯ç”¨æ—¶é™çº§åˆ°å¢å¼ºç‰ˆè§„åˆ™
    
    vs æ—§ç‰ˆï¼š
    - æ—§ç‰ˆï¼š"CEO" è§¦å‘å…¨éƒ¨3ä¸ªAgentï¼ˆå³ä½¿åªéœ€è¦åˆ†æå¸ˆï¼‰
    - æ–°ç‰ˆï¼šè¯­ä¹‰ç†è§£ï¼Œ"HMDå®¢æˆ·Q3æ•°æ®" â†’ åªè°ƒåˆ†æå¸ˆ
    """

    ROUTING_PROMPT = """ä½ æ˜¯ä¸€ä¸ªé—®é¢˜åˆ†ç±»å™¨ã€‚æ ¹æ®ç”¨æˆ·é—®é¢˜åˆ¤æ–­éœ€è¦å“ªäº›ä¸“å®¶å‚ä¸ã€‚

ä¸“å®¶åˆ—è¡¨ï¼š
- analyst: æ•°æ®åˆ†æï¼ˆè¥æ”¶ã€å®¢æˆ·ã€è¶‹åŠ¿ã€æ’åã€æ•°é‡ã€åŒºåŸŸã€äº§å“ç»“æ„ï¼‰
- risk: é£é™©è¯„ä¼°ï¼ˆæµå¤±ã€é¢„è­¦ã€å¼‚å¸¸ã€ä¸‹é™ã€å±é™©ä¿¡å·ï¼‰
- strategist: æˆ˜ç•¥å»ºè®®ï¼ˆå¢é•¿æœºä¼šã€ç«äº‰åˆ†æã€è¡Œä¸šå¯¹æ ‡ã€é¢„æµ‹ã€å†³ç­–å»ºè®®ï¼‰

è§„åˆ™ï¼š
1. ç®€å•æ•°æ®æŸ¥è¯¢ â†’ åªéœ€ analyst
2. é£é™©/æµå¤±ç›¸å…³ â†’ analyst + risk
3. æˆ˜ç•¥/å»ºè®®/æœªæ¥ â†’ analyst + strategist
4. å…¨é¢åˆ†æ/CEOæŠ¥å‘Š â†’ å…¨éƒ¨

è¾“å‡ºæ ¼å¼ï¼ˆçº¯JSONï¼Œæ— å…¶ä»–æ–‡å­—ï¼‰ï¼š
{"agents": ["analyst"], "reason": "ç®€å•æ•°æ®æŸ¥è¯¢"}
"""

    @staticmethod
    def route(question: str, provider: str = "", api_key: str = "",
              kg_hint: List[str] = None) -> List[str]:
        """
        æ™ºèƒ½è·¯ç”±ï¼šè¿”å›éœ€è¦çš„Agentåˆ—è¡¨
        ä¼˜å…ˆçº§: KGæç¤º > LLMè·¯ç”± > è§„åˆ™è·¯ç”±
        """
        # çŸ¥è¯†å›¾è°±æç¤ºï¼ˆé›¶APIè°ƒç”¨ï¼‰
        if kg_hint:
            valid = [a for a in kg_hint if a in AGENT_PROFILES]
            if valid:
                return valid

        # å°è¯•LLMè·¯ç”±
        if api_key:
            result = SmartRouter._llm_route(question, provider, api_key)
            if result:
                return result

        # é™çº§åˆ°å¢å¼ºç‰ˆè§„åˆ™
        return SmartRouter._rule_route(question)

    @staticmethod
    def _llm_route(question: str, provider: str, api_key: str) -> Optional[List[str]]:
        """LLMè¯­ä¹‰è·¯ç”±"""
        try:
            raw = _call_llm_raw(
                SmartRouter.ROUTING_PROMPT,
                f"ç”¨æˆ·é—®é¢˜ï¼š{question}",
                provider, api_key,
                max_tokens=80, temperature=0.0,
                _trace_name="routing_llm"
            )
            raw = raw.strip()
            if raw.startswith("```"):
                raw = raw.split("```")[1]
                if raw.startswith("json"):
                    raw = raw[4:]
            parsed = json.loads(raw)
            agents = parsed.get('agents', [])
            # éªŒè¯Agentåç§°åˆæ³•
            valid = [a for a in agents if a in AGENT_PROFILES]
            if valid:
                return valid
        except Exception:
            pass
        return None

    @staticmethod
    def _rule_route(question: str) -> List[str]:
        """å¢å¼ºç‰ˆè§„åˆ™è·¯ç”±ï¼ˆæ¯”v2.1æ›´ç²¾å‡†ï¼‰"""
        q = question.lower()
        agents_needed = set()

        # å…³é”®è¯åŒ¹é…ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
        for agent_id, profile in AGENT_PROFILES.items():
            score = sum(1 for kw in profile['keywords'] if kw in q)
            if score > 0:
                agents_needed.add(agent_id)

        # å…¨é‡è§¦å‘
        if any(k in q for k in ['CEO', 'ceo', 'æ€»ç»“', 'å…¨é¢', 'æ¦‚è§ˆ', 'æ€ä¹ˆæ ·', 'æŠ¥å‘Š']):
            agents_needed = {"analyst", "risk", "strategist", "quality", "market", "finance", "procurement"}

        # ç®€å•æŸ¥è¯¢ä¼˜åŒ–ï¼šåªæœ‰æ•°æ®é—®é¢˜æ—¶åªéœ€åˆ†æå¸ˆ
        simple_data_patterns = ['å¤šå°‘', 'å‡ ä¸ª', 'æ˜¯ä»€ä¹ˆ', 'å“ªäº›', 'åˆ—å‡º', 'æœ‰å“ªäº›']
        if any(p in q for p in simple_data_patterns) and not agents_needed:
            agents_needed = {"analyst"}

        if not agents_needed:
            agents_needed = {"analyst"}

        return list(agents_needed)


# å…¼å®¹æ—§ç‰ˆæ¥å£
def route_to_agents(question: str) -> list:
    return SmartRouter._rule_route(question)


# ============================================================
# å‡çº§â‘¢ å¹¶è¡ŒAgentæ‰§è¡Œ
# ============================================================

class ParallelAgentExecutor:
    """
    æ›¿ä»£æ—§ç‰ˆä¸²è¡ŒLLMè°ƒç”¨ã€‚
    
    åŸç†ï¼š
    - ä¸“å®¶Agentä¹‹é—´äº’ç›¸ç‹¬ç«‹ â†’ ç”¨ ThreadPoolExecutor å¹¶è¡Œè°ƒç”¨
    - åªæœ‰æŠ¥å‘Šå‘˜éœ€è¦ç­‰æ‰€æœ‰ä¸“å®¶å®Œæˆåæ‰æ‰§è¡Œ
    - 3ä¸ªAgentå¹¶è¡Œï¼šä» ~9ç§’ é™åˆ° ~3ç§’ï¼ˆå‡è®¾æ¯ä¸ªAgent ~3ç§’ï¼‰
    
    vs æ—§ç‰ˆï¼š
    - æ—§ç‰ˆï¼šåˆ†æå¸ˆ(3s) â†’ é£æ§(3s) â†’ ç­–ç•¥å¸ˆ(3s) â†’ æŠ¥å‘Šå‘˜(3s) = 12ç§’
    - æ–°ç‰ˆï¼š[åˆ†æå¸ˆ|é£æ§|ç­–ç•¥å¸ˆ](3s) â†’ æŠ¥å‘Šå‘˜(3s) = 6ç§’
    """

    def __init__(self, provider: str, api_key: str, max_workers: int = 3):
        self.provider = provider
        self.api_key = api_key
        self.max_workers = max_workers

    def execute_experts_parallel(
        self,
        agents_needed: List[str],
        question: str,
        context_data: str,
        memory_section: str = "",
        stream_ps: 'PipelineStream' = None,
        enable_tools: bool = True,
    ) -> Dict[str, str]:
        """
        å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä¸“å®¶Agentï¼Œè¿”å› {agent_name: output}
        v4.0: æ”¯æŒ Tool Use + Streaming å›è°ƒ
        """
        expert_outputs = {}

        def _call_single_expert(agent_id: str) -> tuple:
            profile = AGENT_PROFILES[agent_id]

            # v4.0 Streaming: é€šçŸ¥ Agent å¼€å§‹
            if stream_ps:
                stream_ps.agent_start(agent_id, profile["name"])

            system = f"ä½ æ˜¯{profile['role']}ã€‚{profile['backstory']}"

            # v4.0 Tool Use: å¢åŠ å·¥å…·æè¿°åˆ° prompt
            tool_hint = ""
            if enable_tools and HAS_TOOLS:
                tool_hint = get_tool_descriptions_for_prompt(agent_id)

            prompt = (
                f"ç”¨æˆ·é—®é¢˜ï¼š{question}\n\n"
                f"ç¦¾è‹—é”€å”®æ•°æ®ï¼š\n{context_data}"
                f"{memory_section}"
                f"{tool_hint}\n\n"
                f"200å­—å†…å›ç­”ã€‚æ•°æ®å¿…é¡»ç²¾ç¡®å¼•ç”¨ã€‚"
            )

            # v4.0: ä¼˜å…ˆä½¿ç”¨ Tool-Augmented è°ƒç”¨
            if enable_tools and HAS_TOOLS and self.provider == "claude":
                output = _call_llm_with_tools(
                    system, prompt, self.provider, self.api_key,
                    agent_id=agent_id, max_turns=3,
                    max_tokens=1000, _trace_name=f"agent_{agent_id}",
                    stream_ps=stream_ps,
                )
            else:
                output = _call_llm_raw(system, prompt, self.provider, self.api_key,
                                       _trace_name=f"agent_{agent_id}")

            # v4.0: è¾“å‡ºæ ¡éªŒ
            if HAS_GUARD:
                validation = validate_agent_output(output, context_data[:2000])
                if not validation.passed and validation.confidence < 0.3:
                    if stream_ps:
                        stream_ps.error(agent_id, f"è¾“å‡ºè´¨é‡ä½: {validation.issues}")

            # v4.0 Streaming: é€šçŸ¥ Agent å®Œæˆ
            if stream_ps:
                stream_ps.agent_done(agent_id, profile["name"], output)

            return (profile["name"], output)

        # å¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(_call_single_expert, aid): aid
                for aid in agents_needed
            }
            for future in as_completed(futures):
                try:
                    name, output = future.result(timeout=30)
                    expert_outputs[name] = output
                except Exception as e:
                    aid = futures[future]
                    profile = AGENT_PROFILES[aid]
                    error_msg = f"[åˆ†æè¶…æ—¶: {e}]"
                    expert_outputs[profile["name"]] = error_msg
                    if stream_ps:
                        stream_ps.error(aid, str(e))

        return expert_outputs

    def execute_reporter(
        self,
        question: str,
        expert_outputs: Dict[str, str],
        memory_section: str = "",
    ) -> str:
        """æ‰§è¡ŒæŠ¥å‘Šå‘˜ï¼ˆéœ€è¦ç­‰æ‰€æœ‰ä¸“å®¶å®Œæˆï¼‰"""
        all_opinions = "\n---\n".join(f"{n}ï¼š\n{t}" for n, t in expert_outputs.items())
        reporter_sys = f"ä½ æ˜¯{REPORTER_PROFILE['role']}ã€‚{REPORTER_PROFILE['backstory']}"
        report = _call_llm_raw(
            reporter_sys,
            f"é—®é¢˜ï¼š{question}\n\nä¸“å®¶åˆ†æï¼š\n{all_opinions}{memory_section}\n\nç»¼åˆæŠ¥å‘Šï¼Œ500å­—å†…ã€‚",
            self.provider, self.api_key,
            _trace_name="reporter_llm"
        )
        return report


# ============================================================
# LLM è°ƒç”¨ï¼ˆæ”¯æŒmax_tokens/temperatureå‚æ•°ï¼‰
# ============================================================

def _call_llm_raw(system_prompt, user_prompt, provider, api_key,
                  max_tokens=800, temperature=0.3,
                  _trace_name="llm_call"):
    """é€šç”¨LLMè°ƒç”¨ï¼Œæ”¯æŒDeepSeekå’ŒClaudeï¼Œå«å¯è§‚æµ‹æ€§è¿½è¸ª + v4.0 Guardrails"""
    if not api_key:
        return "[éœ€è¦API Key]"

    # ç¡®å®šmodelå
    model = "deepseek-chat" if provider == "deepseek" else "claude-sonnet-4-20250514"

    # è·å–tracerï¼ˆå¯èƒ½åœ¨traceä¸Šä¸‹æ–‡ä¹‹å¤–è°ƒç”¨ï¼‰
    tracer = get_tracer() if HAS_OBS else None
    lc_ctx = None

    # v4.0: é¢„ç®—æ£€æŸ¥
    if HAS_GUARD:
        budget = get_budget()
        if not budget.should_allow_query():
            return "[æ¯æ—¥é¢„ç®—å·²è€—å°½ï¼Œè¯·æ˜æ—¥å†è¯•æˆ–è°ƒæ•´é¢„ç®—]"

    def _do_call():
        """å®é™…è°ƒç”¨ï¼ˆè¢« retry/breaker åŒ…è£…ï¼‰"""
        if provider == "deepseek":
            from openai import OpenAI
            client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
            resp = client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "system", "content": system_prompt},
                          {"role": "user", "content": user_prompt}],
                temperature=temperature, max_tokens=max_tokens,
            )
            result = resp.choices[0].message.content

            # è¿½è¸ªLLM usage
            if tracer and tracer.enabled:
                usage = getattr(resp, 'usage', None)
                p_tokens = getattr(usage, 'prompt_tokens', 0) if usage else 0
                c_tokens = getattr(usage, 'completion_tokens', 0) if usage else 0
                if p_tokens == 0:
                    p_tokens = CostCalculator.estimate_tokens(system_prompt + user_prompt)
                if c_tokens == 0:
                    c_tokens = CostCalculator.estimate_tokens(result)
                _record_llm_span(tracer, _trace_name, provider, model,
                                 p_tokens, c_tokens, temperature, max_tokens,
                                 system_prompt + user_prompt, result)

            return result

        elif provider == "claude":
            import anthropic
            client = anthropic.Anthropic(api_key=api_key)
            resp = client.messages.create(
                model="claude-sonnet-4-20250514", max_tokens=max_tokens,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
            )
            result = resp.content[0].text

            # è¿½è¸ªLLM usage
            if tracer and tracer.enabled:
                usage = getattr(resp, 'usage', None)
                p_tokens = getattr(usage, 'input_tokens', 0) if usage else 0
                c_tokens = getattr(usage, 'output_tokens', 0) if usage else 0
                if p_tokens == 0:
                    p_tokens = CostCalculator.estimate_tokens(system_prompt + user_prompt)
                if c_tokens == 0:
                    c_tokens = CostCalculator.estimate_tokens(result)
                _record_llm_span(tracer, _trace_name, provider, model,
                                 p_tokens, c_tokens, temperature, max_tokens,
                                 system_prompt + user_prompt, result)

            return result

    try:
        # v4.0: ç”¨ Guardrails åŒ…è£…è°ƒç”¨ï¼ˆRetry + CircuitBreaker + Validationï¼‰
        if HAS_GUARD:
            result = guarded_llm_call(
                _do_call,
                breaker_name=f"llm_{provider}",
                validate=True,
                source_data=user_prompt[:2000],
            )
            # è®°å½•æ¶ˆè€—
            budget = get_budget()
            est_tokens = CostCalculator.estimate_tokens(system_prompt + user_prompt + (result or "")) if HAS_OBS else 500
            est_cost = est_tokens * 0.000003  # ç²—ä¼°
            budget.record_cost(est_cost, _trace_name)
            return result
        else:
            return _do_call()
    except CircuitBreakerOpenError as e:
        return f"[æœåŠ¡æš‚æ—¶ä¸å¯ç”¨: {e}]"
    except Exception as e:
        return f"[è°ƒç”¨å¤±è´¥: {e}]"


# ============================================================
# v4.0: Tool-Augmented LLM Call (ReAct Pattern)
# ============================================================

def _call_llm_with_tools(system_prompt, user_prompt, provider, api_key,
                         agent_id="analyst", max_turns=5, max_tokens=1200,
                         temperature=0.3, _trace_name="tool_agent",
                         stream_ps=None):
    """
    V10.0 ReAct Agent Loop â€” Reason + Act + Observe

    æ ‡å‡† ReAct å¾ªç¯:
      Thought: LLM æ¨ç†ï¼ˆé€‰æ‹©å·¥å…·æˆ–ç›´æ¥å›ç­”ï¼‰
      Action:  è°ƒç”¨å·¥å…· (tool_use)
      Observation: å·¥å…·è¿”å›ç»“æœ (tool_result)
      ... å¾ªç¯ ...
      Final Answer: LLM ç»¼åˆæ‰€æœ‰ Observations ç»™å‡ºæœ€ç»ˆå›ç­”

    ä»… Claude provider æ”¯æŒåŸç”Ÿ tool_useï¼ŒDeepSeek èµ° prompt injection æ¨¡å¼
    """
    if not api_key or not HAS_TOOLS:
        # fallback: æ— å·¥å…·è°ƒç”¨
        return _call_llm_raw(system_prompt, user_prompt, provider, api_key,
                            max_tokens=max_tokens, temperature=temperature,
                            _trace_name=_trace_name)

    tools = get_tools_for_agent(agent_id)
    if not tools:
        return _call_llm_raw(system_prompt, user_prompt, provider, api_key,
                            max_tokens=max_tokens, temperature=temperature,
                            _trace_name=_trace_name)

    # DeepSeek: æŠŠå·¥å…·æè¿°æ³¨å…¥ promptï¼ˆä¸æ”¯æŒåŸç”Ÿ tool_useï¼‰
    if provider == "deepseek":
        tool_desc = get_tool_descriptions_for_prompt(agent_id)
        enhanced_prompt = user_prompt + "\n" + tool_desc + (
            "\nå¦‚æœéœ€è¦ç²¾ç¡®è®¡ç®—ï¼Œè¯·åœ¨å›ç­”ä¸­æ ‡æ˜è®¡ç®—è¿‡ç¨‹ã€‚"
        )
        return _call_llm_raw(system_prompt, enhanced_prompt, provider, api_key,
                            max_tokens=max_tokens, temperature=temperature,
                            _trace_name=_trace_name)

    # Claude: åŸç”Ÿ tool_use â€” V10.0 ReAct agentic loop
    try:
        import anthropic
        client = anthropic.Anthropic(api_key=api_key)
    except Exception as e:
        return _call_llm_raw(system_prompt, user_prompt, provider, api_key,
                            max_tokens=max_tokens, temperature=temperature,
                            _trace_name=_trace_name)

    # ReAct ç»“æ„åŒ– system prompt
    react_system = (
        system_prompt + "\n\n"
        "[ReAct æ¨ç†æ¡†æ¶]\n"
        "å¯¹äºæ¯ä¸ªåˆ†ææ­¥éª¤ï¼Œè¯·æŒ‰ä»¥ä¸‹æ¨¡å¼æ¨ç†:\n"
        "1. Thought: åˆ†æå½“å‰é—®é¢˜ï¼Œå†³å®šéœ€è¦å“ªäº›æ•°æ®\n"
        "2. Action: è°ƒç”¨åˆé€‚çš„å·¥å…·è·å–æ•°æ®\n"
        "3. Observation: åˆ†æå·¥å…·è¿”å›çš„ç»“æœ\n"
        "é‡å¤ä»¥ä¸Šæ­¥éª¤ç›´åˆ°æœ‰è¶³å¤Ÿä¿¡æ¯ã€‚\n"
        "æœ€åç»™å‡º Final Answer: ç»¼åˆæ‰€æœ‰æ•°æ®çš„ç²¾ç¡®å›ç­”ã€‚"
    )

    messages = [{"role": "user", "content": user_prompt}]
    all_text = []
    tool_calls_made = []
    react_trace = []  # ReAct æ­¥éª¤è¿½è¸ª

    for turn in range(max_turns):
        try:
            resp = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=max_tokens,
                system=react_system,
                tools=tools,
                tool_choice={"type": "auto"},
                messages=messages,
            )
        except Exception as e:
            if all_text:
                return "\n".join(all_text)
            return f"[ReAct agent è°ƒç”¨å¤±è´¥: {e}]"

        # æ”¶é›†æ–‡æœ¬ (Thought) å’Œå·¥å…·è°ƒç”¨ (Action)
        tool_blocks = []
        for block in resp.content:
            if hasattr(block, 'text'):
                all_text.append(block.text)
                react_trace.append({"step": turn + 1, "type": "thought", "content": block.text[:200]})
            elif hasattr(block, 'type') and block.type == "tool_use":
                tool_blocks.append(block)
                react_trace.append({"step": turn + 1, "type": "action", "tool": block.name})

        # æ²¡æœ‰å·¥å…·è°ƒç”¨ â†’ Final Answer
        if resp.stop_reason != "tool_use" or not tool_blocks:
            react_trace.append({"step": turn + 1, "type": "final_answer"})
            break

        # æ‰§è¡Œå·¥å…· (Observation)
        messages.append({"role": "assistant", "content": resp.content})

        tool_results = []
        for block in tool_blocks:
            tool_name = block.name
            tool_input = block.input
            tool_id = block.id

            # æ‰§è¡Œ
            exec_result = sales_tools.execute(tool_name, tool_input)
            result_str = json.dumps(exec_result, ensure_ascii=False, default=str)

            tool_calls_made.append({
                "tool": tool_name, "input": tool_input,
                "result_preview": result_str[:200],
            })
            react_trace.append({
                "step": turn + 1, "type": "observation",
                "tool": tool_name, "result_len": len(result_str),
            })

            # æµå¼é€šçŸ¥
            if stream_ps:
                stream_ps.tool_call(agent_id, tool_name, tool_input, result_str[:100])

            tool_results.append({
                "type": "tool_result",
                "tool_use_id": tool_id,
                "content": result_str,
            })

        messages.append({"role": "user", "content": tool_results})

    final_text = "\n".join(all_text) if all_text else "[ReAct Agent æœªç”Ÿæˆå›ç­”]"

    # ReAct è¿½è¸ªæ—¥å¿—
    logger.debug(f"ReAct trace ({len(react_trace)} steps): "
                 f"{json.dumps(react_trace, ensure_ascii=False, default=str)[:500]}")

    # è¿½è¸ª
    tracer = get_tracer() if HAS_OBS else None
    if tracer and tracer.enabled:
        try:
            _record_llm_span(tracer, _trace_name, provider, "claude-sonnet-4-20250514",
                             0, 0, temperature, max_tokens,
                             system_prompt + user_prompt, final_text)
        except Exception:
            pass

    return final_text


def _record_llm_span(tracer, name, provider, model,
                      prompt_tokens, completion_tokens,
                      temperature, max_tokens,
                      prompt_text, response_text):
    """è®°å½•LLMè°ƒç”¨spanï¼ˆä¸å¹²æ‰°ä¸»æµç¨‹ï¼‰"""
    try:
        from observability import _ctx, Span, SpanKind, LLMUsage
        if _ctx.current_trace is None:
            return

        parent_id = ""
        if _ctx.current_span_stack:
            parent_id = _ctx.current_span_stack[-1].span_id

        cost = CostCalculator.calculate(provider, model,
                                         prompt_tokens, completion_tokens)
        s = Span(
            trace_id=_ctx.current_trace.trace_id,
            parent_span_id=parent_id,
            kind=SpanKind.LLM_CALL.value,
            name=name,
            start_time=time.time() - 0.001,  # è¿‘ä¼¼
            llm_usage=LLMUsage(
                provider=provider, model=model,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=prompt_tokens + completion_tokens,
                cost_usd=cost,
                temperature=temperature,
                max_tokens=max_tokens,
            ),
        )
        s.finish()
        _ctx.current_trace.add_span(s)
    except Exception:
        pass  # å¯è§‚æµ‹æ€§ä¸èƒ½å½±å“ä¸»æµç¨‹


# ============================================================
# æ•°æ®ä¸Šä¸‹æ–‡
# ============================================================

def build_data_store(data, results, benchmark=None, forecast=None):
    """å…¼å®¹æ—§ç‰ˆæ¥å£"""
    store = {
        'æ€»è¥æ”¶': data.get('æ€»è¥æ”¶', 0),
        'æ€»YoY': data.get('æ€»YoY', {}),
        'æœˆåº¦è¥æ”¶': data.get('æœˆåº¦æ€»è¥æ”¶', []),
        'æ ¸å¿ƒå‘ç°': results.get('æ ¸å¿ƒå‘ç°', []),
        'å®¢æˆ·æ•°': sum(1 for c in data.get('å®¢æˆ·é‡‘é¢', []) if c.get('å¹´åº¦é‡‘é¢', 0) > 0),
        'å®¢æˆ·åˆ†çº§': results.get('å®¢æˆ·åˆ†çº§', [])[:15],
        'æµå¤±é¢„è­¦': results.get('æµå¤±é¢„è­¦', []),
        'å¼‚å¸¸æ£€æµ‹': results.get('MoMå¼‚å¸¸', [])[:10],
        'å¢é•¿æœºä¼š': results.get('å¢é•¿æœºä¼š', []),
        'ä»·é‡åˆ†è§£': results.get('ä»·é‡åˆ†è§£', [])[:10],
        'åŒºåŸŸæ´å¯Ÿ': results.get('åŒºåŸŸæ´å¯Ÿ', {}),
    }
    if benchmark:
        store['è¡Œä¸šå¯¹æ ‡'] = {
            'å¸‚åœºå®šä½': benchmark.get('å¸‚åœºå®šä½', {}),
            'ç«äº‰å¯¹æ ‡': benchmark.get('ç«äº‰å¯¹æ ‡', {}),
            'ç»“æ„æ€§é£é™©': benchmark.get('ç»“æ„æ€§é£é™©', []),
            'æˆ˜ç•¥æœºä¼š': benchmark.get('æˆ˜ç•¥æœºä¼š', []),
        }
    if forecast:
        store['é¢„æµ‹'] = {
            'æ€»è¥æ”¶é¢„æµ‹': forecast.get('æ€»è¥æ”¶é¢„æµ‹', {}),
            'å®¢æˆ·é¢„æµ‹': forecast.get('å®¢æˆ·é¢„æµ‹', [])[:5],
            'æƒ…æ™¯åˆ†æ': forecast.get('é£é™©åœºæ™¯', {}),
        }
    return store


# ============================================================
# ä¸»å…¥å£ v3.0
# ============================================================

def ask_multi_agent(
    question: str,
    data: dict,
    results: dict,
    benchmark: dict = None,
    forecast: dict = None,
    provider: str = "deepseek",
    api_key: str = "",
    memory: AgentMemory = None,
    # v3.3 æ–°å¢å‚æ•°
    enable_critic: bool = True,       # Generator+Critic
    enable_hitl_v2: bool = True,      # å¢å¼ºHITL
    enable_persistent_mem: bool = True, # æŒä¹…åŒ–è®°å¿†
    critic_threshold: float = 7.0,     # è´¨é‡é—¨ç¦åˆ†æ•°
    critic_max_iter: int = 2,          # æœ€å¤§è¿­ä»£æ¬¡æ•°
    # v4.0 æ–°å¢å‚æ•°
    stream_callback: 'StreamCallback' = None,  # æµå¼å›è°ƒ
    enable_tools: bool = True,         # Tool Use
    enable_cache: bool = True,         # å“åº”ç¼“å­˜
) -> dict:
    """
    v4.0 ä¸»å…¥å£ â€” Top-5 ç”Ÿäº§çº§ Agent System

    æµç¨‹ï¼ˆæ¯ä¸ªé˜¶æ®µç‹¬ç«‹Spanè¿½è¸ª + æµå¼æ¨é€ï¼‰ï¼š
    0. ç¼“å­˜æ£€æŸ¥ï¼ˆå‘½ä¸­ç›´æ¥è¿”å›ï¼‰            [cache] â† v4.0
    1. æŒä¹…åŒ–è®°å¿†åŠ è½½ï¼ˆè·¨ä¼šè¯ä¸Šä¸‹æ–‡ï¼‰        [persistent_memory]
    2. SmartDataQuery ç²¾ç¡®æå–æ•°æ®           [data_query span]
    3. SmartRouter LLMè¯­ä¹‰è·¯ç”±              [routing span]
    4. ParallelAgentExecutor å¹¶è¡Œæ‰§è¡Œ        [agent span Ã— N] + Tool Use
    5. æŠ¥å‘Šå‘˜ç»¼åˆ                           [reporter span]
    6. CriticAgent è´¨é‡å®¡æŸ¥+è¿­ä»£ç²¾ç‚¼         [critic span]
    7. HITLç½®ä¿¡åº¦è¯„ä¼°                       [hitl span]
    8. æŒä¹…åŒ–è®°å¿†ä¿å­˜ï¼ˆæ´å¯Ÿ+å®ä½“+åå¥½ï¼‰      [memory_save]

    v4.0 æ–°å¢ï¼š
    - stream_callback: StreamCallback å¯¹è±¡ï¼Œå®æ—¶æ¨é€å„é˜¶æ®µè¿›åº¦
    - enable_tools: Agent å¯è°ƒç”¨è®¡ç®—/é£æ§/åˆ†æå·¥å…·
    - enable_cache: ç›¸åŒé—®é¢˜å‘½ä¸­ç¼“å­˜ç›´æ¥è¿”å›
    - guardrails: è‡ªåŠ¨ Retry + ç†”æ–­ + è¾“å‡ºæ ¡éªŒ
    - tool_calls: å·¥å…·è°ƒç”¨è®°å½•
    - budget_status: é¢„ç®—çŠ¶æ€
    """
    t0 = time.time()
    mem = memory or get_memory()
    mem_context = mem.get_context_prompt()
    thinking = [f"ğŸ“© æ”¶åˆ°é—®é¢˜ï¼š{question}"]

    # v4.0: åˆå§‹åŒ– Streaming
    stream_ps = None
    if HAS_STREAM and stream_callback:
        stream_ps = PipelineStream(stream_callback)

    # v4.0: ç¼“å­˜æ£€æŸ¥
    if enable_cache and HAS_GUARD:
        cache = get_cache()
        cached = cache.get(question)
        if cached:
            thinking.append("âš¡ å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥è¿”å›")
            if stream_ps:
                stream_ps.thinking("âš¡ å‘½ä¸­ç¼“å­˜")
                stream_ps.complete()
            cached["from_cache"] = True
            return cached

    if not api_key:
        return {
            "answer": "âš ï¸ è¯·å…ˆé…ç½®API Key",
            "agents_used": [], "thinking": [],
            "expert_outputs": {}, "hitl_triggers": [],
            "trace_id": "", "obs_summary": {},
            "critique": None, "hitl_decision": None,
            "persistent_memory_used": False,
        }

    # ---- å¯è§‚æµ‹æ€§ï¼šå¼€å§‹trace ----
    tracer = get_tracer() if HAS_OBS else None
    trace_ctx = tracer.trace(question) if tracer else _DummyCtx()
    trace_obj = trace_ctx.__enter__()

    try:
        # æ³¨å†Œå®¢æˆ·ååˆ°è®°å¿†
        for c in data.get('å®¢æˆ·é‡‘é¢', [])[:50]:
            name = c.get('å®¢æˆ·', '')
            if name and len(name) >= 2:
                mem.register_known_entities([name])

        if mem_context:
            thinking.append(f"ğŸ§  åŠ è½½ {len(mem.conversation_history)} è½®è®°å¿†")

        # ---- v3.3 æŒä¹…åŒ–è®°å¿†åŠ è½½ ----
        pmem_context = ""
        pmem_used = False
        if enable_persistent_mem and HAS_PMEM:
            try:
                pmem = get_persistent_memory()
                pmem_context = pmem.build_memory_context(question)
                if pmem_context:
                    pmem_used = True
                    thinking.append(f"ğŸ§  æŒä¹…åŒ–è®°å¿†: åŠ è½½è·¨ä¼šè¯ä¸Šä¸‹æ–‡")
            except Exception:
                pass  # æŒä¹…åŒ–è®°å¿†å¤±è´¥ä¸å½±å“ä¸»æµç¨‹

        # ---- é˜¶æ®µâ‘ ï¼šæ™ºèƒ½æ•°æ®æŸ¥è¯¢ï¼ˆ+çŸ¥è¯†å›¾è°±ï¼‰[data_query span] ----
        thinking.append("ğŸ” æ™ºèƒ½æ•°æ®æŸ¥è¯¢...")
        if stream_ps:
            stream_ps.start_stage("data_query", "ğŸ” æ™ºèƒ½æ•°æ®æŸ¥è¯¢")
        dq_start = time.time()

        global _smart_query
        _smart_query = SmartDataQuery(data, results, benchmark, forecast)
        context_data = _smart_query.query_smart(question, provider, api_key)

        dq_elapsed = (time.time() - dq_start) * 1000
        thinking.append(f"ğŸ“¦ ç²¾ç¡®æå– {len(context_data)} å­—æ•°æ®ä¸Šä¸‹æ–‡")

        # V9.0: RLM é€’å½’è¯­è¨€æ¨¡å‹ â€” è§£å†³å¤§æ•°æ®æˆªæ–­ç“¶é¢ˆ
        rlm_used = False
        if HAS_RLM and len(context_data) > 500:
            try:
                rlm = RLMEngine(config=RLMConfig(
                    max_recursion=3,
                    chunk_size=4000,
                    enable_sandbox=True,
                ))
                rlm_result = rlm.analyze(
                    data=context_data,
                    task=question,
                    llm_fn=lambda sys, usr: _call_llm_raw(
                        sys, usr, provider, api_key, model="", max_tokens=1500
                    ),
                )
                if rlm_result and hasattr(rlm_result, 'answer') and rlm_result.answer:
                    context_data = (
                        f"[RLMé€’å½’åˆ†ææ‘˜è¦ Â· {len(context_data)}å­—â†’å‹ç¼©]\n"
                        f"{rlm_result.answer}\n\n"
                        f"[åŸå§‹æ•°æ®ç‰‡æ®µ]\n{context_data[:3000]}"
                    )
                    rlm_used = True
                    thinking.append(f"ğŸ”„ RLMå¼•æ“: {len(context_data)}å­—æ•°æ®é€’å½’å¤„ç†å®Œæˆ")
            except Exception as e:
                logger.debug(f"RLMé™çº§: {e}")

        # V9.0: åˆå§‹åŒ–å¯è§£é‡Šæ€§è¿½è¸ªå™¨
        if HAS_INTERP:
            v9t = _get_v9_tracer()
            if v9t:
                v9t.trace_step("data_query", "ç³»ç»Ÿ",
                               action="æ™ºèƒ½æ•°æ®æå–",
                               input_summary=f"é—®é¢˜: {question[:50]}",
                               output_summary=f"{len(context_data)}å­—ä¸Šä¸‹æ–‡")

        if stream_ps:
            stream_ps.end_stage("data_query", dq_elapsed, f"{len(context_data)}å­—")

        # è®°å½•data_query span
        if tracer and HAS_OBS:
            _record_stage_span(tracer, "data_query", "ğŸ“Š æ•°æ®æŸ¥è¯¢",
                              dq_elapsed, {"data_length": len(context_data)})

        # çŸ¥è¯†å›¾è°±å…ƒæ•°æ®
        kg_pattern = getattr(_smart_query, '_last_pattern', '')
        kg_entity = getattr(_smart_query, '_last_entity_context', '')
        kg_agent_hint = getattr(_smart_query, '_last_agent_hint', [])
        kg_corrections = getattr(_smart_query, '_last_corrections', [])

        if kg_pattern:
            thinking.append(f"ğŸ“š çŸ¥è¯†å›¾è°±: æ¨¡å¼={kg_pattern}")
        if kg_entity:
            thinking.append(f"ğŸ·ï¸ å®ä½“ä¸Šä¸‹æ–‡: {kg_entity}")
        if kg_corrections:
            for c in kg_corrections:
                thinking.append(f"ğŸ”§ è‡ªåŠ¨çº æ­£: {c}")

        # ---- é˜¶æ®µâ‘¡ï¼šLLMæ™ºèƒ½è·¯ç”± [routing span] ----
        thinking.append("ğŸ§­ æ™ºèƒ½è·¯ç”±åˆ†æä¸­...")
        if stream_ps:
            stream_ps.start_stage("routing", "ğŸ§­ æ™ºèƒ½è·¯ç”±")
        rt_start = time.time()

        agents_needed = SmartRouter.route(question, provider, api_key,
                                          kg_hint=kg_agent_hint)
        agent_names = [AGENT_PROFILES[a]["name"] for a in agents_needed]
        route_source = "çŸ¥è¯†å›¾è°±" if kg_agent_hint else ("LLM" if api_key else "è§„åˆ™")

        rt_elapsed = (time.time() - rt_start) * 1000
        thinking.append(f"ğŸ¯ è·¯ç”±ç»“æœï¼š{', '.join(agent_names)}ï¼ˆ{route_source}ï¼Œå…±{len(agents_needed)}ä½ä¸“å®¶ï¼‰")
        if stream_ps:
            stream_ps.end_stage("routing", rt_elapsed, f"{len(agents_needed)}ä½ä¸“å®¶")

        # è®°å½•routing span
        if tracer and HAS_OBS:
            _record_stage_span(tracer, "routing", "ğŸ§­ è·¯ç”±",
                              rt_elapsed, {
                                  "agents": agents_needed,
                                  "source": route_source,
                              })

        # ---- é˜¶æ®µâ‘¢ï¼šå¹¶è¡Œæ‰§è¡Œ [agent spans] ----
        memory_section = f"\n\n[å¯¹è¯è®°å¿†]\n{mem_context}" if mem_context else ""
        if pmem_context:
            memory_section += f"\n\n{pmem_context}"

        if len(agents_needed) > 1:
            thinking.append(f"âš¡ å¹¶è¡Œå¯åŠ¨ {len(agents_needed)} ä½ä¸“å®¶...")
        else:
            thinking.append(f"â–¶ï¸ å¯åŠ¨ä¸“å®¶åˆ†æ...")

        if stream_ps:
            stream_ps.start_stage("agents", f"ğŸ¤– {len(agents_needed)}ä½ä¸“å®¶å¹¶è¡Œåˆ†æ")

        ag_start = time.time()
        executor = ParallelAgentExecutor(provider, api_key)

        enriched_data = context_data
        if kg_entity:
            enriched_data += f"\n\n[çŸ¥è¯†å›¾è°± Â· å®ä½“ç”»åƒ]\n{kg_entity}"

        expert_outputs = executor.execute_experts_parallel(
            agents_needed, question, enriched_data, memory_section,
            stream_ps=stream_ps,
            enable_tools=enable_tools,
        )
        ag_elapsed = (time.time() - ag_start) * 1000

        agents_used = list(expert_outputs.keys())
        for name in agents_used:
            thinking.append(f"âœ… {name} å®Œæˆ")

        if stream_ps:
            stream_ps.end_stage("agents", ag_elapsed, f"{len(agents_used)}ä½ä¸“å®¶å®Œæˆ")

        # è®°å½•agent spans
        if tracer and HAS_OBS:
            _record_stage_span(tracer, "agent", "ğŸ¤– Agentæ‰§è¡Œ",
                              ag_elapsed, {
                                  "agents": agents_used,
                                  "parallel": len(agents_needed) > 1,
                              })

        # ---- é˜¶æ®µâ‘£ï¼šæŠ¥å‘Šå‘˜ç»¼åˆ [reporter span] ----
        thinking.append("ğŸ–Šï¸ æŠ¥å‘Šå‘˜ç»¼åˆä¸­...")
        if stream_ps:
            stream_ps.start_stage("reporter", "ğŸ–Šï¸ æŠ¥å‘Šå‘˜ç»¼åˆ")
        rp_start = time.time()

        final_answer = executor.execute_reporter(question, expert_outputs,
                                                  memory_section)
        rp_elapsed = (time.time() - rp_start) * 1000

        agents_used.append("ğŸ–Šï¸ æŠ¥å‘Šå‘˜")
        thinking.append("âœ… æŠ¥å‘Šå®Œæˆ")
        if stream_ps:
            stream_ps.end_stage("reporter", rp_elapsed, "æŠ¥å‘Šç”Ÿæˆå®Œæˆ")

        # è®°å½•reporter span
        if tracer and HAS_OBS:
            _record_stage_span(tracer, "reporter", "ğŸ–Šï¸ æŠ¥å‘Šå‘˜",
                              rp_elapsed, {})

        # HITL æ£€æµ‹ (legacy v3.2, kept for backward compat)
        hitl_triggers = []
        risk_output = expert_outputs.get("ğŸ›¡ï¸ é£æ§ä¸“å®¶", "")
        if "[HIGH_RISK_ALERT]" in risk_output or "é«˜é£é™©" in risk_output:
            hitl_triggers = detect_hitl_triggers(results)
            if hitl_triggers:
                thinking.append(f"âš ï¸ HITL: {len(hitl_triggers)} ä¸ªé«˜é£é™©éœ€ç¡®è®¤")

        # ---- v3.3 é˜¶æ®µâ‘¤ï¼šCriticAgent è´¨é‡å®¡æŸ¥ + è¿­ä»£ç²¾ç‚¼ [critic span] ----
        critique_result = None
        refinement_trace = None
        if enable_critic and HAS_CRITIC:
            thinking.append("ğŸ” è´¨é‡å®¡æŸ¥ä¸­...")
            if stream_ps:
                stream_ps.start_stage("critic", "ğŸ” è´¨é‡å®¡æŸ¥")
            cr_start = time.time()

            try:
                final_answer, critique_result, refinement_trace = critique_and_refine(
                    final_answer, question, expert_outputs,
                    _call_llm_raw, provider, api_key,
                    threshold=critic_threshold,
                    max_iterations=critic_max_iter,
                    use_llm_critic=True,
                    enabled=True,
                )
                cr_elapsed = (time.time() - cr_start) * 1000

                if critique_result:
                    score = critique_result.get("overall_score", 0)
                    passed = critique_result.get("passed", False)
                    iters = refinement_trace.get("iterations", 0) if refinement_trace else 0
                    thinking.append(
                        f"ğŸ“‹ è´¨é‡è¯„åˆ†: {score}/10 "
                        f"({'âœ… é€šè¿‡' if passed else 'âš ï¸ æœªé€šè¿‡'}) "
                        f"è¿­ä»£{iters}æ¬¡"
                    )
                    if refinement_trace and refinement_trace.get("improvement", 0) > 0:
                        thinking.append(
                            f"ğŸ“ˆ ç²¾ç‚¼æå‡: +{refinement_trace['improvement']:.1f}åˆ†"
                        )

                # è®°å½•critic span
                if tracer and HAS_OBS:
                    _record_stage_span(tracer, "critic", "ğŸ” è´¨é‡å®¡æŸ¥",
                                      cr_elapsed, {
                                          "score": critique_result.get("overall_score", 0) if critique_result else 0,
                                          "passed": critique_result.get("passed", False) if critique_result else False,
                                      })
                if stream_ps:
                    stream_ps.end_stage("critic", cr_elapsed,
                        f"è¯„åˆ†{critique_result.get('overall_score', 0)}/10" if critique_result else "å®Œæˆ")
            except Exception as e:
                thinking.append(f"ğŸ” è´¨é‡å®¡æŸ¥è·³è¿‡: {e}")
                if stream_ps:
                    stream_ps.error("critic", str(e))

        # ---- v3.3 é˜¶æ®µâ‘¥ï¼šå¢å¼ºHITLç½®ä¿¡åº¦è¯„ä¼° [hitl span] ----
        hitl_decision = None
        if enable_hitl_v2 and HAS_HITL_V2:
            if stream_ps:
                stream_ps.start_stage("hitl", "ğŸ¯ HITLç½®ä¿¡åº¦è¯„ä¼°")
            hl_start = time.time()
            try:
                crit_score = critique_result.get("overall_score") if critique_result else None
                hitl_decision = evaluate_hitl(
                    question, final_answer, expert_outputs,
                    context_data, crit_score,
                    enabled=True,
                )
                hl_elapsed = (time.time() - hl_start) * 1000

                if hitl_decision:
                    conf = hitl_decision.get("confidence_score", 0)
                    level = hitl_decision.get("confidence_level", "?")
                    action = hitl_decision.get("action", "?")
                    n_triggers = len(hitl_decision.get("triggers", []))
                    thinking.append(
                        f"ğŸ¯ HITL: ç½®ä¿¡åº¦={conf:.2f} ({level}) â†’ {action}"
                        + (f" | {n_triggers}ä¸ªè§¦å‘" if n_triggers else "")
                    )

                if tracer and HAS_OBS:
                    _record_stage_span(tracer, "hitl", "ğŸ¯ HITLè¯„ä¼°",
                                      hl_elapsed, {
                                          "confidence": hitl_decision.get("confidence_score", 0) if hitl_decision else 0,
                                          "action": hitl_decision.get("action", "") if hitl_decision else "",
                                      })
                if stream_ps:
                    stream_ps.end_stage("hitl", hl_elapsed,
                        f"ç½®ä¿¡åº¦{hitl_decision.get('confidence_score', 0):.0%}" if hitl_decision else "å®Œæˆ")
            except Exception as e:
                thinking.append(f"ğŸ¯ HITLè¯„ä¼°è·³è¿‡: {e}")

        elapsed = time.time() - t0
        thinking.append(f"â±ï¸ æ€»è€—æ—¶ {elapsed:.1f}ç§’")

        # v4.0: é€šçŸ¥æµå¼å®Œæˆ
        if stream_ps:
            stream_ps.thinking(f"â±ï¸ æ€»è€—æ—¶ {elapsed:.1f}ç§’")
            stream_ps.complete()

        # è®°å¿†
        mem.add_turn(question, final_answer, agents_used, expert_outputs)

        # ---- v3.3 æŒä¹…åŒ–è®°å¿†ä¿å­˜ ----
        if enable_persistent_mem and HAS_PMEM:
            try:
                pmem = get_persistent_memory()
                # ä¿å­˜æ´å¯Ÿ
                import uuid as _uuid
                insight = InsightRecord(
                    insight_id=str(_uuid.uuid4())[:8],
                    question=question,
                    answer_summary=final_answer[:300],
                    agents_used=agents_used,
                    key_findings=[],
                    entities_involved=list(mem.entity_mentions.keys())[:10],
                    timestamp=datetime.now().isoformat(),
                    quality_score=critique_result.get("overall_score", 0) if critique_result else 0,
                )
                pmem.save_insight(insight)

                # æ›´æ–°å®ä½“è®°å¿†
                for entity_name in list(mem.entity_mentions.keys())[:5]:
                    pmem.upsert_entity("customer", entity_name)
                    pmem.add_entity_event(
                        "customer", entity_name,
                        question[:100],
                        final_answer[:100],
                    )

                # å­¦ä¹ ç”¨æˆ·åå¥½
                pmem.update_preferences_from_interaction(
                    question, agents_used,
                    list(mem.entity_mentions.keys())[:10],
                )
            except Exception:
                pass  # æŒä¹…åŒ–è®°å¿†ä¿å­˜å¤±è´¥ä¸å½±å“è¿”å›

        # ---- å¯è§‚æµ‹æ€§ï¼šå¡«å……traceå…ƒæ•°æ® ----
        trace_id = ""
        obs_summary = {}
        if trace_obj and hasattr(trace_obj, 'trace_id') and trace_obj.trace_id != "disabled":
            trace_obj.agents_used = agents_used
            trace_obj.pattern_matched = kg_pattern
            trace_obj.route_source = route_source
            trace_obj.kg_corrections = kg_corrections
            trace_id = trace_obj.trace_id

            obs_summary = {
                "trace_id": trace_id,
                "total_tokens": trace_obj.total_tokens,
                "total_cost_usd": round(trace_obj.total_cost_usd, 6),
                "total_llm_calls": trace_obj.total_llm_calls,
                "latency_breakdown": {
                    "data_query_ms": round(dq_elapsed, 1),
                    "routing_ms": round(rt_elapsed, 1),
                    "agents_ms": round(ag_elapsed, 1),
                    "reporter_ms": round(rp_elapsed, 1),
                    "total_ms": round(elapsed * 1000, 1),
                },
            }
            thinking.append(f"ğŸ“Š Trace: {trace_id[:8]}... | "
                          f"Tokens={trace_obj.total_tokens} | "
                          f"Cost=${trace_obj.total_cost_usd:.4f}")

        result_dict = {
            "answer": final_answer,
            "agents_used": agents_used,
            "thinking": thinking,
            "expert_outputs": expert_outputs,
            "hitl_triggers": hitl_triggers,
            "trace_id": trace_id,
            "obs_summary": obs_summary,
            # v3.3 æ–°å¢
            "critique": critique_result,
            "hitl_decision": hitl_decision,
            "refinement_trace": refinement_trace,
            "persistent_memory_used": pmem_used,
            # v4.0 æ–°å¢
            "tool_use_enabled": enable_tools and HAS_TOOLS,
            "guardrails_enabled": HAS_GUARD,
            "streaming_enabled": stream_ps is not None,
            "budget_status": get_budget().check_budget() if HAS_GUARD else None,
            "from_cache": False,
            # V9.0 æ–°å¢
            "v9_modules": {
                "rlm": HAS_RLM,
                "reasoning_templates": HAS_REASONING,
                "memory_3d": HAS_MEM3D,
                "interpretability": HAS_INTERP,
                "search_engine": HAS_SEARCH,
                "awm": HAS_AWM,
                "evals_v9": HAS_EVALS_V9,
            },
            "v9_activity": {
                "rlm_used": rlm_used,
                "reasoning_templates_injected": HAS_REASONING,
                "memory_3d_saved": HAS_MEM3D,
                "memory_3d_retrieved": bool(
                    HAS_MEM3D and _get_v9_memory()
                    and _get_v9_memory().query_skills(question, top_k=1)
                ) if HAS_MEM3D else False,
                "interpretability_traced": HAS_INTERP and _get_v9_tracer() is not None,
                "trace_steps": len(_get_v9_tracer().get_trace()) if HAS_INTERP and _get_v9_tracer() else 0,
            },
        }

        # V9.0: ä¸‰ç»´è®°å¿†ä¿å­˜ â€” å­˜å‚¨æœ¬æ¬¡åˆ†æä¸ºç»éªŒæ¡ˆä¾‹
        if HAS_MEM3D:
            try:
                mem3d = _get_v9_memory()
                if mem3d:
                    score = critique_result.get("overall_score", 7.0) / 10.0 if critique_result else 0.7
                    import uuid as _uuid3d
                    node = Memory3DNode(
                        node_id=f"case_{_uuid3d.uuid4().hex[:8]}",
                        content=f"Q: {question[:100]}\nA: {final_answer[:200]}",
                        form=MemoryForm.TEXT,
                        function=MemoryFunction.LEARNING,
                        tags=agents_used[:3],
                        entities=list(mem.entity_mentions.keys())[:5],
                        confidence=score,
                    )
                    mem3d.add(node)
            except Exception:
                pass

        # v4.0: ç¼“å­˜ä¿å­˜
        if enable_cache and HAS_GUARD:
            try:
                cache = get_cache()
                cache.put(question, result_dict)
            except Exception:
                pass

        return result_dict

    except Exception as e:
        if trace_obj and hasattr(trace_obj, 'status'):
            trace_obj.status = "error"
            trace_obj.error_message = str(e)
        raise
    finally:
        trace_ctx.__exit__(None, None, None)


def _record_stage_span(tracer, kind, name, duration_ms, attrs):
    """è®°å½•pipelineé˜¶æ®µçš„span"""
    try:
        from observability import _ctx, Span
        if _ctx.current_trace is None:
            return
        parent_id = ""
        if _ctx.current_span_stack:
            parent_id = _ctx.current_span_stack[-1].span_id
        s = Span(
            trace_id=_ctx.current_trace.trace_id,
            parent_span_id=parent_id,
            kind=kind,
            name=name,
            start_time=time.time() - duration_ms / 1000,
            attributes=attrs,
        )
        s.finish()
        _ctx.current_trace.add_span(s)
    except Exception:
        pass


class _DummyCtx:
    """å¯è§‚æµ‹æ€§ç¦ç”¨æ—¶çš„å ä½ä¸Šä¸‹æ–‡"""
    def __enter__(self):
        return type('_', (), {'trace_id': '', 'agents_used': [],
                              'pattern_matched': '', 'route_source': '',
                              'kg_corrections': [], 'status': 'ok',
                              'total_tokens': 0, 'total_cost_usd': 0,
                              'total_llm_calls': 0})()
    def __exit__(self, *a): pass


# å…¼å®¹æ—§ç‰ˆæ¥å£
def ask_multi_agent_simple(
    question: str, data: dict, results: dict,
    benchmark=None, forecast=None,
    provider="deepseek", api_key="",
    memory: AgentMemory = None,
) -> dict:
    """v3.0 ç®€åŒ–ç‰ˆä¹Ÿä½¿ç”¨å‡çº§åçš„æµç¨‹"""
    return ask_multi_agent(
        question, data, results, benchmark, forecast,
        provider, api_key, memory,
    )


def _ask_fallback(question, data, results, benchmark, forecast, provider, api_key, memory=None):
    return ask_multi_agent(question, data, results, benchmark, forecast, provider, api_key, memory)


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  V7.0 LANGGRAPH LAYER â€” ä»¥ä¸‹ä»£ç ä¸º V7.0 LangGraph æ‰©å±•å±‚    â•‘
# â•‘  StateGraph + Reflection + HITL + Multi-Model Routing       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ============================================================
# v7.0 Multi-Model Router â€” æˆæœ¬é™ä½60%
# ============================================================

MODEL_TIERS = {
    "fast": {
        "claude": "claude-haiku-4-5-20251001",
        "deepseek": "deepseek-chat",
        "description": "ç®€å•æŸ¥è¯¢/è·¯ç”±/åˆ†ç±» (~$0.001/query)",
    },
    "standard": {
        "claude": "claude-sonnet-4-20250514",
        "deepseek": "deepseek-chat",
        "description": "æ•°æ®åˆ†æ/æŠ¥å‘Šç»¼åˆ (~$0.01/query)",
    },
    "advanced": {
        "claude": "claude-sonnet-4-20250514",
        "deepseek": "deepseek-chat",
        "description": "é£é™©è¯„ä¼°/æˆ˜ç•¥æ¨ç† (~$0.03/query)",
    },
}


def get_model_for_tier(provider: str, tier: str = "standard") -> str:
    """v7.0: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦é€‰æ‹©æœ€ä¼˜æ¨¡å‹"""
    tier_config = MODEL_TIERS.get(tier, MODEL_TIERS["standard"])
    return tier_config.get(provider, tier_config.get("claude"))


# ============================================================
# v7.0 Agent State (LangGraph TypedDict)
# ============================================================

class AgentState(TypedDict):
    """LangGraph çŠ¶æ€å®šä¹‰ â€” å…¨éƒ¨ä¿¡æ¯åœ¨çŠ¶æ€ä¸­æµè½¬"""
    # è¾“å…¥
    question: str
    context_data: str
    provider: str
    api_key: str

    # è·¯ç”±ç»“æœ
    agents_needed: List[str]
    route_source: str

    # ä¸“å®¶è¾“å‡º
    expert_outputs: Dict[str, str]

    # ç»¼åˆæŠ¥å‘Š
    final_answer: str

    # è´¨é‡å®¡æŸ¥
    critique_result: Optional[Dict]
    critique_score: float
    reflection_iterations: int

    # HITL
    hitl_decision: Optional[Dict]
    hitl_approved: bool
    high_risk_alerts: List[Dict]

    # å…ƒæ•°æ®
    thinking: List[str]
    elapsed_ms: float
    agents_used: List[str]
    model_costs: Dict[str, float]

    # é…ç½®
    enable_tools: bool
    enable_critic: bool
    enable_hitl: bool
    stream_ps: Optional[Any]

    # çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡
    kg_entity_context: str
    kg_agent_hint: List[str]

    # V9.0 æ–°å¢
    v9_attribution: Optional[Dict]    # è¾“å‡ºå½’å› 

    # V10.1 æ–°å¢: è§„åˆ’å™¨è¾“å‡º
    execution_plan: Optional[Dict]


# ============================================================
# v7.0 LLM è°ƒç”¨å±‚ (å¤ç”¨ v5.0, å¢åŠ  model routing)
# ============================================================

def _call_llm(system: str, user: str, provider: str, api_key: str,
              tier: str = "standard", max_tokens: int = 800,
              temperature: float = 0.3, trace_name: str = "llm_call") -> str:
    """ç»Ÿä¸€ LLM è°ƒç”¨å…¥å£ â€” v7.0 å¢åŠ æ¨¡å‹è·¯ç”±"""
    if not api_key:
        return "[éœ€è¦ API Key]"

    model = get_model_for_tier(provider, tier)

    # é¢„ç®—æ£€æŸ¥
    if HAS_GUARD:
        budget = get_budget()
        if not budget.should_allow_query():
            return "[æ¯æ—¥é¢„ç®—å·²è€—å°½]"

    def _do_call():
        if provider == "deepseek":
            from openai import OpenAI
            client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
            resp = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system},
                    {"role": "user", "content": user},
                ],
                temperature=temperature,
                max_tokens=max_tokens,
            )
            return resp.choices[0].message.content

        elif provider == "claude":
            import anthropic
            client = anthropic.Anthropic(api_key=api_key)
            resp = client.messages.create(
                model=model,
                max_tokens=max_tokens,
                system=system,
                messages=[{"role": "user", "content": user}],
            )
            return resp.content[0].text

    try:
        if HAS_GUARD:
            return guarded_llm_call(
                _do_call,
                breaker_name=f"llm_{provider}",
                validate=True,
                source_data=user[:2000],
            )
        return _do_call()
    except Exception as e:
        return f"[è°ƒç”¨å¤±è´¥: {e}]"


# ============================================================
# V10.0: Middleware æ‹¦æˆªé“¾
# ============================================================

class AgentMiddleware:
    """Agent è°ƒç”¨ä¸­é—´ä»¶åŸºç±» â€” æ”¯æŒé“¾å¼æ‹¦æˆª"""

    def before(self, agent_id: str, question: str, **ctx) -> dict:
        """è°ƒç”¨å‰æ‹¦æˆªã€‚è¿”å› dict å¯æ³¨å…¥/ä¿®æ”¹ä¸Šä¸‹æ–‡ã€‚"""
        return {}

    def after(self, agent_id: str, output: str, elapsed_ms: float, **ctx) -> str:
        """è°ƒç”¨åæ‹¦æˆªã€‚å¯ä¿®æ”¹è¾“å‡ºã€‚"""
        return output


class LoggingMiddleware(AgentMiddleware):
    """æ—¥å¿—ä¸­é—´ä»¶ â€” è®°å½•æ¯æ¬¡ Agent è°ƒç”¨"""

    def before(self, agent_id: str, question: str, **ctx):
        logger.info(f"[MW] Agent {agent_id} å¼€å§‹: {question[:60]}...")
        return {"start_time": time.time()}

    def after(self, agent_id: str, output: str, elapsed_ms: float, **ctx):
        logger.info(f"[MW] Agent {agent_id} å®Œæˆ: {elapsed_ms:.0f}ms, {len(output)}å­—")
        return output


class LangfuseMiddleware(AgentMiddleware):
    """Langfuse å¯è§‚æµ‹æ€§ä¸­é—´ä»¶"""

    def before(self, agent_id: str, question: str, **ctx):
        if HAS_LANGFUSE and _langfuse_client:
            try:
                span = _langfuse_client.trace(
                    name=f"mw_agent_{agent_id}",
                    metadata={"question": question[:200]},
                )
                return {"lf_span": span}
            except Exception:
                pass
        return {}

    def after(self, agent_id: str, output: str, elapsed_ms: float, **ctx):
        span = ctx.get("lf_span")
        if span:
            try:
                span.update(output=output[:500], metadata={"elapsed_ms": elapsed_ms})
            except Exception:
                pass
        return output


class PydanticValidationMiddleware(AgentMiddleware):
    """Pydantic è¾“å‡ºéªŒè¯ä¸­é—´ä»¶"""

    def after(self, agent_id: str, output: str, elapsed_ms: float, **ctx):
        try:
            parsed = json.loads(output)
            resp = AgentResponse(
                agent_id=agent_id,
                agent_name=ctx.get("agent_name", agent_id),
                data=parsed if isinstance(parsed, dict) else {"raw": parsed},
                elapsed_ms=elapsed_ms,
            )
            # éªŒè¯é€šè¿‡ï¼Œè¿”å›åŸå§‹è¾“å‡ºï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
            return output
        except (json.JSONDecodeError, Exception):
            return output


# å…¨å±€ Middleware é“¾
_middleware_chain: List[AgentMiddleware] = [
    LoggingMiddleware(),
    LangfuseMiddleware(),
    PydanticValidationMiddleware(),
]


def run_middleware_before(agent_id: str, question: str, **ctx) -> dict:
    """æ‰§è¡Œ before é“¾ï¼Œåˆå¹¶ä¸Šä¸‹æ–‡"""
    merged = dict(ctx)
    for mw in _middleware_chain:
        try:
            result = mw.before(agent_id, question, **merged)
            if result:
                merged.update(result)
        except Exception as e:
            logger.debug(f"Middleware {mw.__class__.__name__} before é”™è¯¯: {e}")
    return merged


def run_middleware_after(agent_id: str, output: str, elapsed_ms: float, **ctx) -> str:
    """æ‰§è¡Œ after é“¾ï¼Œé€æ­¥å¤„ç†è¾“å‡º"""
    result = output
    for mw in _middleware_chain:
        try:
            result = mw.after(agent_id, result, elapsed_ms, **ctx)
        except Exception as e:
            logger.debug(f"Middleware {mw.__class__.__name__} after é”™è¯¯: {e}")
    return result


# ============================================================
# V10.1: Hierarchical Planner â€” å¤æ‚æŸ¥è¯¢ä»»åŠ¡åˆ†è§£
# ============================================================

class QueryPlanner:
    """æŸ¥è¯¢è§„åˆ’å™¨ â€” åˆ†è§£å¤æ‚æŸ¥è¯¢ä¸ºå¯å¹¶è¡Œæ­¥éª¤"""

    COMPLEXITY_KEYWORDS = {
        "multi": ["ç»¼åˆ", "å…¨é¢", "å¯¹æ¯”", "å…³è”",
                  "äº¤å‰", "å¤šç»´", "CEO", "æŠ¥å‘Š"],
        "single": ["å¤šå°‘", "å˜åŒ–", "è¶‹åŠ¿", "æ’å"],
    }

    @staticmethod
    def needs_planning(query: str, agents: list) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è§„åˆ’ (è€Œéç›´æ¥è·¯ç”±)"""
        if len(agents) >= 3:
            return True
        q = query.lower()
        return any(
            kw in q
            for kw in QueryPlanner.COMPLEXITY_KEYWORDS["multi"]
        )

    @staticmethod
    def create_plan(query: str, agents: list) -> dict:
        """
        åˆ›å»ºæ‰§è¡Œè®¡åˆ’: {
          "phases": [
            {"phase": 1, "agents": [...], "parallel": True},
            {"phase": 2, "agents": ["strategist"], "parallel": False}
          ]
        }
        """
        # Phase 1: æ•°æ®æ”¶é›† (å¹¶è¡Œ)
        data_agents = [a for a in agents if a != "strategist"]
        phases = []
        if data_agents:
            phases.append({
                "phase": 1,
                "agents": data_agents,
                "parallel": True,
                "desc": "æ•°æ®æ”¶é›†ä¸åŸŸåˆ†æ",
            })
        # Phase 2: ç»¼åˆåˆ†æ (ä¸²è¡Œ)
        if "strategist" in agents:
            phases.append({
                "phase": 2,
                "agents": ["strategist"],
                "parallel": False,
                "desc": "æˆ˜ç•¥ç»¼åˆä¸å»ºè®®",
            })
        return {
            "query": query,
            "total_agents": len(agents),
            "phases": phases,
            "has_planning": True,
        }


# ============================================================
# v7.0 LangGraph Nodes
# ============================================================

def node_route(state: AgentState) -> dict:
    """
    ğŸ§­ è·¯ç”±èŠ‚ç‚¹ â€” å†³å®šè°ƒç”¨å“ªäº›ä¸“å®¶
    ä¼˜å…ˆçº§: çŸ¥è¯†å›¾è°±æç¤º > LLMè·¯ç”±(fast tier) > è§„åˆ™è·¯ç”±
    """
    question = state["question"]
    provider = state["provider"]
    api_key = state["api_key"]
    thinking = list(state.get("thinking", []))
    kg_hint = state.get("kg_agent_hint", [])

    # 1. çŸ¥è¯†å›¾è°±æç¤º (é›¶ API è°ƒç”¨)
    if kg_hint:
        valid = [a for a in kg_hint if a in AGENT_PROFILES]
        if valid:
            thinking.append(f"ğŸ§­ KGè·¯ç”± â†’ {valid}")
            # V10.1: è§„åˆ’åˆ†è§£ (å¤æ‚æŸ¥è¯¢)
            plan = None
            if QueryPlanner.needs_planning(question, valid):
                plan = QueryPlanner.create_plan(question, valid)
                thinking.append(f"ğŸ“‹ è§„åˆ’: {len(plan['phases'])}é˜¶æ®µ")
            return {
                "agents_needed": valid,
                "route_source": "knowledge_graph",
                "thinking": thinking,
                "execution_plan": plan,
            }

    # 2. LLM è·¯ç”± (ç”¨ fast tier çœé’±)
    if api_key:
        try:
            routing_prompt = """ä½ æ˜¯é—®é¢˜åˆ†ç±»å™¨ã€‚æ ¹æ®ç”¨æˆ·é—®é¢˜åˆ¤æ–­éœ€è¦å“ªäº›ä¸“å®¶å‚ä¸ã€‚

ä¸“å®¶åˆ—è¡¨:
- analyst: æ•°æ®åˆ†æï¼ˆè¥æ”¶ã€å®¢æˆ·ã€è¶‹åŠ¿ã€æ’åã€æ•°é‡ã€åŒºåŸŸã€äº§å“ç»“æ„ï¼‰
- risk: é£é™©è¯„ä¼°ï¼ˆæµå¤±ã€é¢„è­¦ã€å¼‚å¸¸ã€ä¸‹æ»‘ã€å±é™©ä¿¡å·ï¼‰
- strategist: æˆ˜ç•¥å»ºè®®ï¼ˆå¢é•¿æœºä¼šã€ç«äº‰åˆ†æã€è¡Œä¸šå¯¹æ ‡ã€é¢„æµ‹ã€å†³ç­–å»ºè®®ï¼‰

è§„åˆ™:
1. ç®€å•æ•°æ®æŸ¥è¯¢ â†’ åªéœ€ analyst
2. é£é™©/æµå¤±ç›¸å…³ â†’ analyst + risk
3. æˆ˜ç•¥/å»ºè®®/æœªæ¥ â†’ analyst + strategist
4. å…¨é¢åˆ†æ/CEOæŠ¥å‘Š â†’ å…¨éƒ¨

è¾“å‡ºæ ¼å¼ï¼ˆçº¯JSONï¼Œæ— å…¶ä»–æ–‡å­—ï¼‰:
{"agents": ["analyst"], "reason": "ç®€å•æ•°æ®æŸ¥è¯¢"}"""

            raw = _call_llm(
                routing_prompt,
                f"ç”¨æˆ·é—®é¢˜ï¼š{question}",
                provider, api_key,
                tier="fast", max_tokens=80, temperature=0.0,
                trace_name="v7_routing",
            )

            raw = raw.strip()
            if raw.startswith("```"):
                raw = raw.split("```")[1].lstrip("json\n")
            parsed = json.loads(raw)
            agents = [a for a in parsed.get("agents", []) if a in AGENT_PROFILES]

            if agents:
                thinking.append(f"ğŸ§­ LLMè·¯ç”±(fast) â†’ {agents}")
                # V10.1: è§„åˆ’åˆ†è§£ (å¤æ‚æŸ¥è¯¢)
                plan = None
                if QueryPlanner.needs_planning(question, agents):
                    plan = QueryPlanner.create_plan(question, agents)
                    thinking.append(f"ğŸ“‹ è§„åˆ’: {len(plan['phases'])}é˜¶æ®µ")
                return {
                    "agents_needed": agents,
                    "route_source": "llm_fast",
                    "thinking": thinking,
                    "execution_plan": plan,
                }
        except Exception:
            pass

    # 3. è§„åˆ™è·¯ç”± (å…œåº•)
    agents = _rule_route(question)
    thinking.append(f"ğŸ§­ è§„åˆ™è·¯ç”± â†’ {agents}")

    # V10.1: è§„åˆ’åˆ†è§£ (å¤æ‚æŸ¥è¯¢)
    plan = None
    if QueryPlanner.needs_planning(question, agents):
        plan = QueryPlanner.create_plan(question, agents)
        thinking.append(f"ğŸ“‹ è§„åˆ’: {len(plan['phases'])}é˜¶æ®µ")

    return {
        "agents_needed": agents,
        "route_source": "rule",
        "thinking": thinking,
        "execution_plan": plan,  # V10.1 æ–°å¢
    }


def _rule_route(question: str) -> List[str]:
    """å¢å¼ºç‰ˆè§„åˆ™è·¯ç”±"""
    q = question.lower()
    agents = set()

    for agent_id, profile in AGENT_PROFILES.items():
        if any(kw in q for kw in profile["keywords"]):
            agents.add(agent_id)

    # å…¨é‡è§¦å‘ â€” V10.0: åŒ…å«å…¨éƒ¨7ä¸ªAgent
    if any(k in q for k in ['CEO', 'ceo', 'æ€»ç»“', 'å…¨é¢', 'æ¦‚è§ˆ', 'æ€ä¹ˆæ ·', 'æŠ¥å‘Š']):
        agents = {"analyst", "risk", "strategist", "quality", "market", "finance", "procurement"}

    # ç®€å•æŸ¥è¯¢ä¼˜åŒ–
    if not agents:
        agents = {"analyst"}

    return list(agents)


def node_experts(state: AgentState) -> dict:
    """
    ğŸ¤– ä¸“å®¶èŠ‚ç‚¹ â€” å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä¸“å®¶ Agent
    v7.0: LangGraph è‡ªåŠ¨ç®¡ç†å¹¶è¡Œ (Send API / fan-out)
    å›é€€: ThreadPoolExecutor å¹¶è¡Œ
    """
    agents_needed = state["agents_needed"]
    question = state["question"]
    context_data = state["context_data"]
    provider = state["provider"]
    api_key = state["api_key"]
    kg_context = state.get("kg_entity_context", "")
    thinking = list(state.get("thinking", []))
    stream_ps = state.get("stream_ps")

    if len(agents_needed) > 1:
        thinking.append(f"âš¡ å¹¶è¡Œå¯åŠ¨ {len(agents_needed)} ä½ä¸“å®¶...")
    else:
        thinking.append("â–¶ï¸ å¯åŠ¨ä¸“å®¶åˆ†æ...")

    enriched_data = context_data
    if kg_context:
        enriched_data += f"\n\n[çŸ¥è¯†å›¾è°± Â· å®ä½“ç”»åƒ]\n{kg_context}"

    expert_outputs = {}
    model_costs = {}

    from concurrent.futures import ThreadPoolExecutor, as_completed

    def _call_expert(agent_id: str) -> tuple:
        profile = AGENT_PROFILES[agent_id]
        tier = profile.get("model_tier", "standard")
        _t0 = time.time()

        # V10.0: Middleware before é“¾
        mw_ctx = run_middleware_before(
            agent_id, question,
            agent_name=profile["name"], tier=tier,
        )

        if stream_ps and HAS_STREAM:
            stream_ps.agent_start(agent_id, profile["name"])

        # V9.0: å¯è§£é‡Šæ€§è¿½è¸ª
        v9t = _get_v9_tracer()
        if v9t and HAS_INTERP:
            v9t.trace_step(f"agent_{agent_id}", profile["name"],
                           action=f"ä¸“å®¶åˆ†æ: {profile['role'][:20]}",
                           input_summary=question[:80])

        # Langfuse span
        lf_span = None
        if HAS_LANGFUSE and _langfuse_client:
            try:
                lf_span = _langfuse_client.trace(
                    name=f"agent_{agent_id}",
                    metadata={"tier": tier, "question": question[:200]},
                )
            except Exception:
                pass

        # â”€â”€ V10.0: Engine-based Agent â†’ ç›´æ¥è°ƒç”¨å¼•æ“ï¼Œä¸èµ° LLM â”€â”€
        if tier == "engine":
            engine = get_domain_engine(profile.get("engine_type", agent_id))
            if engine:
                try:
                    raw = engine.answer(question)
                    output = raw if isinstance(raw, str) else json.dumps(raw, ensure_ascii=False, indent=2)
                    if lf_span:
                        try:
                            lf_span.update(output=output[:500], metadata={"source": "engine"})
                        except Exception:
                            pass
                except Exception as e:
                    output = f"[{profile['name']} å¼•æ“é”™è¯¯: {e}]"
                    logger.error(f"åŸŸå¼•æ“ {agent_id} æ‰§è¡Œå¤±è´¥: {e}")
            else:
                output = f"[{profile['name']} å¼•æ“æœªåŠ è½½]"
                logger.warning(f"åŸŸå¼•æ“ {agent_id} ä¸å¯ç”¨")

            # V10.0: Middleware after é“¾ (engine path)
            output = run_middleware_after(
                agent_id, output, (time.time() - _t0) * 1000,
                agent_name=profile["name"], tier=tier, **mw_ctx,
            )

            if stream_ps and HAS_STREAM:
                stream_ps.agent_done(agent_id, profile["name"], output)
            return (profile["name"], output, tier)

        # â”€â”€ LLM-based Agent (analyst/risk/strategist) â”€â”€
        system = f"ä½ æ˜¯{profile['role']}ã€‚{profile['backstory']}"

        # å·¥å…·æè¿°
        tool_hint = ""
        if state.get("enable_tools") and HAS_TOOLS:
            tool_hint = get_tool_descriptions_for_prompt(agent_id)

        # V9.0: ç»“æ„åŒ–æ¨ç†æ¨¡æ¿ â€” é™ä½å†…åœ¨ç»´åº¦ï¼Œæé«˜è¾“å‡ºè´¨é‡
        reasoning_hint = ""
        if HAS_REASONING:
            selector = _get_v9_reasoning()
            if selector:
                _role_map = {"analyst": "analyst", "risk": "risk",
                             "strategist": "strategist", "forecaster": "analyst",
                             "quality": "analyst", "market": "analyst",
                             "finance": "analyst", "procurement": "analyst"}
                tmpl_role = _role_map.get(agent_id, "analyst")
                try:
                    template = selector.select(tmpl_role, complexity="standard")
                    if template:
                        reasoning_hint = (
                            f"\n\n[ç»“æ„åŒ–æ¨ç†æ¡†æ¶]\n"
                            f"è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ¨ç†:\n"
                            + "\n".join(f"  Step {i+1}: {s.name} â€” {s.instruction}"
                                       for i, s in enumerate(template.steps[:5]))
                            + "\næ¯æ­¥ç»™å‡ºå…·ä½“æ•°å€¼æˆ–ç®€æ´ç»“è®ºã€‚"
                        )
                except Exception:
                    pass  # é™çº§ï¼šä¸ä½¿ç”¨æ¨¡æ¿

        # V9.0: ä¸‰ç»´è®°å¿†æ£€ç´¢ â€” åˆ©ç”¨å†å²åˆ†æç»éªŒ
        memory_hint = ""
        if HAS_MEM3D:
            mem3d = _get_v9_memory()
            if mem3d:
                try:
                    relevant = mem3d.query_skills(question, top_k=3)
                    if relevant:
                        memory_hint = "\n\n[å†å²ç»éªŒå‚è€ƒ]\n" + "\n".join(
                            f"- {n.to_prompt_text(max_len=100)}" for n in relevant[:3]
                        )
                except Exception:
                    pass

        prompt = (
            f"ç”¨æˆ·é—®é¢˜ï¼š{question}\n\n"
            f"ç¦¾è‹—é”€å”®æ•°æ®ï¼š\n{enriched_data}"
            f"{tool_hint}"
            f"{reasoning_hint}"
            f"{memory_hint}\n\n"
            f"200å­—å†…å›ç­”ã€‚æ•°æ®å¿…é¡»ç²¾ç¡®å¼•ç”¨ã€‚"
        )

        output = _call_llm(
            system, prompt, provider, api_key,
            tier=tier, max_tokens=1000,
            trace_name=f"v7_agent_{agent_id}",
        )

        # è¾“å‡ºæ ¡éªŒ
        if HAS_GUARD:
            validation = validate_agent_output(output, context_data[:2000])
            if not validation.passed and validation.confidence < 0.3:
                logger.warning(f"Agent {agent_id} è¾“å‡ºè´¨é‡ä½")

        if lf_span:
            try:
                lf_span.update(output=output[:500], metadata={"source": "llm", "tier": tier})
            except Exception:
                pass

        # V10.0: Middleware after é“¾ (LLM path)
        output = run_middleware_after(
            agent_id, output, (time.time() - _t0) * 1000,
            agent_name=profile["name"], tier=tier, **mw_ctx,
        )

        if stream_ps and HAS_STREAM:
            stream_ps.agent_done(agent_id, profile["name"], output)

        return (profile["name"], output, tier)

    # å¹¶è¡Œæ‰§è¡Œ â€” V10.0: max_workers=7 æ”¯æŒå…¨éƒ¨åŸŸAgent
    with ThreadPoolExecutor(max_workers=7) as executor:
        futures = {executor.submit(_call_expert, aid): aid for aid in agents_needed}
        for future in as_completed(futures):
            try:
                name, output, tier = future.result(timeout=30)
                expert_outputs[name] = output
                model_costs[name] = {"tier": tier}
            except Exception as e:
                aid = futures[future]
                profile = AGENT_PROFILES[aid]
                expert_outputs[profile["name"]] = f"[åˆ†æè¶…æ—¶: {e}]"

    agents_used = list(expert_outputs.keys())
    for name in agents_used:
        thinking.append(f"âœ… {name} å®Œæˆ")

    return {
        "expert_outputs": expert_outputs,
        "agents_used": agents_used,
        "model_costs": model_costs,
        "thinking": thinking,
    }


def node_synthesize(state: AgentState) -> dict:
    """
    ğŸ–Šï¸ æŠ¥å‘Šç»¼åˆèŠ‚ç‚¹ â€” Reporter ç»¼åˆæ‰€æœ‰ä¸“å®¶æ„è§
    V9.0: + LatentLens å¯è§£é‡Šæ€§è¿½è¸ª + è¾“å‡ºå½’å› 
    """
    question = state["question"]
    expert_outputs = state["expert_outputs"]
    provider = state["provider"]
    api_key = state["api_key"]
    thinking = list(state.get("thinking", []))

    thinking.append("ğŸ–Šï¸ æŠ¥å‘Šå‘˜ç»¼åˆä¸­...")

    # V9.0: è¿½è¸ªç»¼åˆæ­¥éª¤
    v9t = _get_v9_tracer()
    if v9t and HAS_INTERP:
        v9t.trace_step("synthesize", "æŠ¥å‘Šå‘˜",
                       action="ç»¼åˆä¸“å®¶æ„è§ç”ŸæˆæŠ¥å‘Š",
                       input_summary=f"ç»¼åˆ {len(expert_outputs)} ä½ä¸“å®¶æ„è§",
                       output_summary="ç”Ÿæˆç»¼åˆæŠ¥å‘Š")

    all_opinions = "\n---\n".join(f"{n}ï¼š\n{t}" for n, t in expert_outputs.items())
    reporter_sys = f"ä½ æ˜¯{REPORTER_PROFILE['role']}ã€‚{REPORTER_PROFILE['backstory']}"

    final_answer = _call_llm(
        reporter_sys,
        f"é—®é¢˜ï¼š{question}\n\nä¸“å®¶åˆ†æï¼š\n{all_opinions}\n\nç»¼åˆæŠ¥å‘Šï¼Œ500å­—å†…ã€‚",
        provider, api_key,
        tier="standard",
        trace_name="v7_reporter",
    )

    # V9.0: è¾“å‡ºå½’å›  â€” è¿½è¸ªç»“è®ºæ¥è‡ªå“ªä½ä¸“å®¶
    v9_attribution = {}
    if HAS_INTERP and final_answer:
        try:
            attributor = OutputAttributor()
            attr_result = attributor.attribute(final_answer, expert_outputs)
            v9_attribution = {"attributions": [a.__dict__ for a in attr_result]
                              if attr_result else []}
        except Exception:
            pass

    thinking.append("âœ… æŠ¥å‘Šå®Œæˆ")

    return {
        "final_answer": final_answer,
        "thinking": thinking,
        "v9_attribution": v9_attribution,
    }


def node_reflect(state: AgentState) -> dict:
    """
    ğŸ” åæ€èŠ‚ç‚¹ (Reflection Pattern) â€” 1-2è½®è‡ªæ£€
    ç ”ç©¶è¡¨æ˜: Text-to-SQL å‡†ç¡®ç‡ä» 70%â†’85%
    v7.0: ä»…åœ¨ enable_critic=True æ—¶æ‰§è¡Œ
    """
    if not state.get("enable_critic") or not HAS_CRITIC:
        return {
            "critique_result": None,
            "critique_score": 0.0,
            "reflection_iterations": 0,
        }

    thinking = list(state.get("thinking", []))
    thinking.append("ğŸ” è´¨é‡å®¡æŸ¥ä¸­...")

    try:
        # å¤ç”¨ v5.0 çš„ critique_and_refine
        # éœ€è¦ä¼ å…¥ _call_llm çš„å…¼å®¹åŒ…è£…
        def _llm_compat(sys, user, prov, key, **kwargs):
            return _call_llm(sys, user, prov, key, tier="standard",
                           trace_name="v7_critic", **{k: v for k, v in kwargs.items()
                                                       if k in ('max_tokens', 'temperature')})

        refined, critique, trace = critique_and_refine(
            state["final_answer"],
            state["question"],
            state["expert_outputs"],
            _llm_compat,
            state["provider"],
            state["api_key"],
            threshold=7.0,
            max_iterations=2,  # v7.0: æœ€å¤š2è½® (ç ”ç©¶æœ€ä¼˜)
            use_llm_critic=True,
            enabled=True,
        )

        score = critique.get("overall_score", 0) if critique else 0
        passed = critique.get("passed", False) if critique else False
        iters = trace.get("iterations", 0) if trace else 0

        thinking.append(
            f"ğŸ“‹ è´¨é‡è¯„åˆ†: {score}/10 "
            f"({'âœ… é€šè¿‡' if passed else 'âš ï¸ æœªé€šè¿‡'}) "
            f"è¿­ä»£{iters}æ¬¡"
        )

        return {
            "final_answer": refined,
            "critique_result": critique,
            "critique_score": float(score),
            "reflection_iterations": iters,
            "thinking": thinking,
        }
    except Exception as e:
        thinking.append(f"ğŸ” è´¨é‡å®¡æŸ¥è·³è¿‡: {e}")
        return {
            "critique_result": None,
            "critique_score": 0.0,
            "reflection_iterations": 0,
            "thinking": thinking,
        }


def node_hitl_check(state: AgentState) -> dict:
    """
    ğŸ¯ HITL èŠ‚ç‚¹ â€” é«˜é£é™©æ—¶ä¸­æ–­ç­‰å¾…äººå·¥ç¡®è®¤
    v7.0: ä½¿ç”¨ LangGraph interrupt() åŸç”Ÿæš‚åœ
    """
    thinking = list(state.get("thinking", []))
    high_risk_alerts = []

    # æ£€æµ‹é«˜é£é™©
    risk_output = ""
    for name, output in state.get("expert_outputs", {}).items():
        if "é£æ§" in name or "risk" in name.lower():
            risk_output = output
            break

    has_high_risk = "[HIGH_RISK_ALERT]" in risk_output or "é«˜é£é™©" in risk_output

    if has_high_risk and state.get("enable_hitl"):
        # æ„å»ºé«˜é£é™©è­¦æŠ¥åˆ—è¡¨
        high_risk_alerts.append({
            "source": "risk_agent",
            "content": risk_output[:500],
            "timestamp": datetime.now().isoformat(),
        })

        # v7.0: LangGraph interrupt â€” æš‚åœå›¾æ‰§è¡Œï¼Œç­‰å¾…äººå·¥ç¡®è®¤
        if HAS_LANGGRAPH:
            try:
                thinking.append("âš ï¸ HITL: æ£€æµ‹åˆ°é«˜é£é™©ï¼Œç­‰å¾…äººå·¥ç¡®è®¤...")

                # interrupt() ä¼šæš‚åœæ‰§è¡Œï¼Œè¿”å›ç»™è°ƒç”¨æ–¹
                # è°ƒç”¨æ–¹é€šè¿‡ graph.invoke(Command(resume=True/False)) ç»§ç»­
                human_response = interrupt({
                    "type": "high_risk_review",
                    "alerts": high_risk_alerts,
                    "question": state["question"],
                    "answer_preview": state["final_answer"][:300],
                    "message": "æ£€æµ‹åˆ°é«˜é£é™©å®¢æˆ·ï¼Œæ˜¯å¦ç¡®è®¤å‘é€æ­¤åˆ†æç»“æœï¼Ÿ",
                })

                approved = human_response.get("approved", True) if isinstance(human_response, dict) else bool(human_response)
                thinking.append(f"ğŸ¯ HITL: {'âœ… å·²ç¡®è®¤' if approved else 'âŒ å·²æ‹’ç»'}")

                return {
                    "hitl_approved": approved,
                    "high_risk_alerts": high_risk_alerts,
                    "thinking": thinking,
                }
            except Exception as e:
                logger.warning(f"HITL interrupt å¤±è´¥: {e}")

        # å›é€€: ä½¿ç”¨ v5.0 HITL å¼•æ“
        if HAS_HITL:
            hitl_decision = evaluate_hitl(
                state["question"], state["final_answer"],
                state["expert_outputs"], state["context_data"],
                state.get("critique_score"),
            )
            if hitl_decision:
                conf = hitl_decision.get("confidence_score", 0)
                action = hitl_decision.get("action", "auto")
                thinking.append(f"ğŸ¯ HITL: ç½®ä¿¡åº¦={conf:.2f} â†’ {action}")
                return {
                    "hitl_decision": hitl_decision,
                    "hitl_approved": action == "auto_approve",
                    "high_risk_alerts": high_risk_alerts,
                    "thinking": thinking,
                }

    thinking.append("ğŸ¯ HITL: æ— éœ€äººå·¥å¹²é¢„")
    return {
        "hitl_approved": True,
        "high_risk_alerts": [],
        "thinking": thinking,
    }


# ============================================================
# v7.0 æ¡ä»¶è¾¹ â€” æ§åˆ¶å›¾æµè½¬
# ============================================================

def should_reflect(state: AgentState) -> str:
    """æ˜¯å¦éœ€è¦åæ€èŠ‚ç‚¹"""
    if state.get("enable_critic") and HAS_CRITIC:
        return "reflect"
    return "hitl_check"


# ============================================================
# v7.0 Graph æ„å»º
# ============================================================

def build_agent_graph(checkpointer=None, enable_advanced: bool = True):
    """
    æ„å»º LangGraph StateGraph
    è¿”å›ç¼–è¯‘åçš„å›¾ï¼Œæ”¯æŒ checkpointing å’Œ interrupt

    LangGraph 1.0 é«˜çº§ç‰¹æ€§ (enable_advanced=True):
      - Node Caching: ç¼“å­˜ experts èŠ‚ç‚¹ç»“æœ (ç›¸åŒè¾“å…¥å¤ç”¨)
      - Durable State: checkpointer æŒä¹…åŒ–çŠ¶æ€è·¨ä¼šè¯
      - Pre/Post Model Hooks: èŠ‚ç‚¹æ‰§è¡Œå‰å hooks
      - interrupt_before: HITL ä¸­æ–­ç‚¹
    """
    if not HAS_LANGGRAPH:
        return None

    graph = StateGraph(AgentState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("route", node_route)
    graph.add_node("experts", node_experts)
    graph.add_node("synthesize", node_synthesize)
    graph.add_node("reflect", node_reflect)
    graph.add_node("hitl_check", node_hitl_check)

    # æ·»åŠ è¾¹
    graph.add_edge(START, "route")
    graph.add_edge("route", "experts")
    graph.add_edge("experts", "synthesize")

    # æ¡ä»¶è¾¹: ç»¼åˆåæ˜¯å¦åæ€
    graph.add_conditional_edges(
        "synthesize",
        should_reflect,
        {"reflect": "reflect", "hitl_check": "hitl_check"},
    )

    graph.add_edge("reflect", "hitl_check")
    graph.add_edge("hitl_check", END)

    # ç¼–è¯‘ â€” å¸¦ checkpointer æ”¯æŒæŒä¹…åŒ– (Durable State)
    if checkpointer is None:
        checkpointer = MemorySaver()

    compile_kwargs = {
        "checkpointer": checkpointer,
        "interrupt_before": ["hitl_check"],  # HITL ä¸­æ–­ç‚¹
    }

    compiled = graph.compile(**compile_kwargs)

    if enable_advanced:
        logger.info("LangGraph 1.0 é«˜çº§ç‰¹æ€§å·²å¯ç”¨: Durable State + HITL interrupt")

    return compiled


# ============================================================
# v7.0 å…¨å±€å›¾å®ä¾‹
# ============================================================

_graph = None
_checkpointer = None


def get_graph():
    """è·å–/åˆ›å»ºå…¨å±€å›¾å®ä¾‹"""
    global _graph, _checkpointer
    if _graph is None and HAS_LANGGRAPH:
        _checkpointer = MemorySaver()
        _graph = build_agent_graph(_checkpointer)
    return _graph


# ============================================================
# v7.0 ä¸»å…¥å£ â€” å…¼å®¹ v5.0 æ¥å£
# ============================================================

def run_multi_agent_v7(
    question: str,
    data: dict,
    results: dict,
    provider: str = "claude",
    api_key: str = "",
    benchmark: dict = None,
    forecast: dict = None,
    enable_tools: bool = True,
    enable_critic: bool = True,
    enable_hitl: bool = True,
    stream_ps=None,
    thread_id: str = None,
    **kwargs,
) -> dict:
    """
    v7.0 å¤š Agent ä¸»å…¥å£

    å…¼å®¹ v5.0 è¿”å›æ ¼å¼:
    {
        "answer": str,
        "agents_used": List[str],
        "thinking": List[str],
        "hitl_triggers": List[dict],
        "critique_result": dict,
        "elapsed": float,
        "version": "v7.0",
    }
    """
    t0 = time.time()

    # æ„å»ºä¸Šä¸‹æ–‡æ•°æ® (SmartDataQuery å·²åœ¨åŒæ–‡ä»¶å®šä¹‰)
    sq = SmartDataQuery(data, results, benchmark, forecast)
    context_data = sq.query_smart(question, provider, api_key)
    kg_entity = sq._last_entity_context if hasattr(sq, '_last_entity_context') else ""
    kg_hint = sq._last_agent_hint if hasattr(sq, '_last_agent_hint') else []

    # åˆå§‹çŠ¶æ€
    initial_state: AgentState = {
        "question": question,
        "context_data": context_data,
        "provider": provider,
        "api_key": api_key,
        "agents_needed": [],
        "route_source": "",
        "expert_outputs": {},
        "final_answer": "",
        "critique_result": None,
        "critique_score": 0.0,
        "reflection_iterations": 0,
        "hitl_decision": None,
        "hitl_approved": True,
        "high_risk_alerts": [],
        "thinking": [f"ğŸš€ MRARFAI v7.0 (LangGraph) â€” {datetime.now().strftime('%H:%M:%S')}"],
        "elapsed_ms": 0.0,
        "agents_used": [],
        "model_costs": {},
        "enable_tools": enable_tools,
        "enable_critic": enable_critic,
        "enable_hitl": enable_hitl,
        "stream_ps": stream_ps,
        "kg_entity_context": kg_entity,
        "kg_agent_hint": kg_hint,
        # V10.1 æ–°å¢
        "execution_plan": None,
        "v9_attribution": None,
    }

    # ---- LangGraph æ‰§è¡Œ ----
    graph = get_graph()

    if graph is not None:
        config = {"configurable": {"thread_id": thread_id or f"mrarfai_{int(time.time())}"}}

        try:
            # invoke ä¼šè¿è¡Œåˆ° interrupt ç‚¹æˆ– END
            result_state = graph.invoke(initial_state, config)

            elapsed = time.time() - t0
            result_state["thinking"].append(f"â±ï¸ æ€»è€—æ—¶ {elapsed:.1f}ç§’")

            return {
                "answer": result_state.get("final_answer", ""),
                "agents_used": result_state.get("agents_used", []),
                "thinking": result_state.get("thinking", []),
                "hitl_triggers": result_state.get("high_risk_alerts", []),
                "critique_result": result_state.get("critique_result"),
                "hitl_decision": result_state.get("hitl_decision"),
                "elapsed": elapsed,
                "version": "v7.0",
                "graph_state": result_state,
            }
        except Exception as e:
            logger.error(f"LangGraph æ‰§è¡Œå¤±è´¥: {e}ï¼Œå›é€€åˆ° v5.0")

    # ---- å›é€€: v5.0 å…¼å®¹æ¨¡å¼ ----
    return _fallback_v5(initial_state, t0)


def _fallback_v5(state: AgentState, t0: float) -> dict:
    """v5.0 å…¼å®¹æ¨¡å¼ â€” ä¸ä¾èµ– LangGraph"""
    thinking = state["thinking"]
    thinking.append("âš ï¸ LangGraph ä¸å¯ç”¨ï¼Œä½¿ç”¨ v5.0 å…¼å®¹æ¨¡å¼")

    # è·¯ç”±
    route_result = node_route(state)
    state.update(route_result)

    # ä¸“å®¶
    expert_result = node_experts(state)
    state.update(expert_result)

    # ç»¼åˆ
    synth_result = node_synthesize(state)
    state.update(synth_result)

    # åæ€
    if state.get("enable_critic"):
        reflect_result = node_reflect(state)
        state.update(reflect_result)

    elapsed = time.time() - t0
    state["thinking"].append(f"â±ï¸ æ€»è€—æ—¶ {elapsed:.1f}ç§’ (v5å…¼å®¹)")

    return {
        "answer": state.get("final_answer", ""),
        "agents_used": state.get("agents_used", []),
        "thinking": state.get("thinking", []),
        "hitl_triggers": state.get("high_risk_alerts", []),
        "critique_result": state.get("critique_result"),
        "elapsed": elapsed,
        "version": "v7.0-compat",
    }


# ============================================================
# v5.0 å…¼å®¹æ¥å£ â€” è®© app.py / chat_tab.py æ— éœ€æ”¹åŠ¨
# ============================================================

def run_multi_agent(question, data, results, provider="claude", api_key="",
                    benchmark=None, forecast=None, **kwargs):
    """
    v5.0 å…¼å®¹å…¥å£ â€” ç›´æ¥æ›¿æ¢æ—§ç‰ˆ run_multi_agent_pipeline
    app.py å’Œ chat_tab.py è°ƒç”¨æ­¤å‡½æ•°å³å¯æ— ç¼å‡çº§
    """
    return run_multi_agent_v7(
        question, data, results, provider, api_key,
        benchmark=benchmark, forecast=forecast, **kwargs,
    )



# ============================================================
# P3: å‰æ²¿æ¡†æ¶é›†æˆæ£€æµ‹ â€” è¯„ä¼°å±‚
# ============================================================

# P3-01: AG-UI / A2UI å‰ç«¯åè®®æ£€æµ‹
HAS_AG_UI = False
try:
    from ag_ui import AgentUIRenderer
    HAS_AG_UI = True
except ImportError:
    pass

# P3-02: Google ADK æ£€æµ‹
HAS_GOOGLE_ADK = False
try:
    from google.adk import CustomAgent as ADKAgent
    HAS_GOOGLE_ADK = True
except ImportError:
    pass

# P3-03: OpenAI Agents SDK æ£€æµ‹
HAS_OPENAI_AGENTS = False
try:
    from agents import Agent as OAIAgent, Runner as OAIRunner
    HAS_OPENAI_AGENTS = True
except ImportError:
    pass

# P3-05: Graphiti Graph Memory æ£€æµ‹
HAS_GRAPHITI = False
try:
    from graphiti_core import Graphiti
    HAS_GRAPHITI = True
except ImportError:
    pass


def get_platform_capabilities() -> dict:
    """
    è¿”å›å¹³å°å…¨é‡èƒ½åŠ›çŸ©é˜µ â€” å®¡è®¡/è¯„ä¼°/å±•ç¤ºç”¨

    è¦†ç›–:
      V4 åŸºç¡€å±‚ + V7 LangGraph + V9 è®ºæ–‡æ¨¡å— +
      V10 åè®®å±‚ (A2A/MCP/gRPC) +
      P3 å‰æ²¿æ¡†æ¶ (ADK/OpenAI/AG-UI/Graphiti/Deep Agents)
    """
    return {
        "version": __version__,
        # V4 åŸºç¡€å±‚
        "v4_pipeline": True,
        "knowledge_graph": HAS_KG if 'HAS_KG' in dir() else False,
        "observability": HAS_OBS if 'HAS_OBS' in dir() else False,
        "tools": HAS_TOOLS if 'HAS_TOOLS' in dir() else False,
        "guardrails": HAS_GUARD if 'HAS_GUARD' in dir() else False,
        "streaming": HAS_STREAM if 'HAS_STREAM' in dir() else False,
        "critic": HAS_CRITIC if 'HAS_CRITIC' in dir() else False,
        # V7 LangGraph
        "langgraph": HAS_LANGGRAPH,
        "hitl": HAS_HITL if 'HAS_HITL' in dir() else False,
        "langfuse": HAS_LANGFUSE,
        # V9 è®ºæ–‡æ¨¡å—
        "rlm_engine": HAS_RLM,
        "awm_environment": HAS_AWM,
        "encompass_search": HAS_SEARCH,
        "reasoning_templates": HAS_REASONING,
        "memory_3d": HAS_MEM3D,
        "interpretability": HAS_INTERP,
        "evals_v9": HAS_EVALS_V9,
        # V10 åè®®
        "pydantic_contracts": True,
        "middleware": True,
        "react_pattern": True,
        "query_planner": True,
        "deep_agents": HAS_DEEP_AGENTS,
        "db_bridge": HAS_DB_BRIDGE,
        # P3 å‰æ²¿æ¡†æ¶
        "ag_ui": HAS_AG_UI,
        "google_adk": HAS_GOOGLE_ADK,
        "openai_agents_sdk": HAS_OPENAI_AGENTS,
        "graphiti": HAS_GRAPHITI,
        # åŸŸ Agent
        "domain_agents": {
            "quality": HAS_QUALITY,
            "market": HAS_MARKET,
            "finance": HAS_FINANCE,
            "procurement": HAS_PROCUREMENT,
            "risk": HAS_RISK_ENGINE,
            "strategist": HAS_STRATEGIST_ENGINE,
        },
    }


# ============================================================
# æ¨¡å—ä¿¡æ¯ â€” V10.0 ç»Ÿä¸€ç‰ˆ
# ============================================================

__version__ = "10.1.0"
__all__ = [
    # V4 ä¸»å…¥å£
    "ask_multi_agent",
    "ask_multi_agent_simple",
    # V7 LangGraph å…¥å£
    "run_multi_agent_v7",
    "run_multi_agent",
    "build_agent_graph",
    "get_graph",
    # æ ¸å¿ƒç±»
    "AgentState",
    "AgentMemory",
    "SmartDataQuery",
    "SmartRouter",
    "ParallelAgentExecutor",
    # é…ç½®
    "AGENT_PROFILES",
    "REPORTER_PROFILE",
    "MODEL_TIERS",
    # å·¥å…·å‡½æ•°
    "route_to_agents",
    "detect_hitl_triggers",
    "get_memory",
    "set_memory",
    "set_sales_data",
    "query_sales_data",
    # V10 æ–°å¢
    "get_platform_capabilities",
    "get_domain_engine",
    "run_middleware_before",
    "run_middleware_after",
    # V10.1 æ–°å¢
    "QueryPlanner",
]

if __name__ == "__main__":
    print(f"MRARFAI Multi-Agent v{__version__} (Unified)")
    print(f"  V4 Pipeline: âœ… (ask_multi_agent)")
    print(f"  V7 LangGraph: {'âœ…' if HAS_LANGGRAPH else 'âŒ'} (run_multi_agent_v7)")
    print(f"  Knowledge Graph: {'âœ…' if HAS_KG else 'âŒ'}")
    print(f"  Observability: {'âœ…' if HAS_OBS else 'âŒ'}")
    print(f"  Tools: {'âœ…' if HAS_TOOLS else 'âŒ'}")
    print(f"  Guardrails: {'âœ…' if HAS_GUARD else 'âŒ'}")
    print(f"  Streaming: {'âœ…' if HAS_STREAM else 'âŒ'}")
    print(f"  Critic: {'âœ…' if HAS_CRITIC else 'âŒ'}")
    print(f"  --- V9.0 è®ºæ–‡æ¨¡å— ---")
    print(f"  â‘  RLM Engine: {'âœ…' if HAS_RLM else 'âŒ'}")
    print(f"  â‘¡ AWM Environment: {'âœ…' if HAS_AWM else 'âŒ'}")
    print(f"  â‘¢ EnCompass Search: {'âœ…' if HAS_SEARCH else 'âŒ'}")
    print(f"  â‘£ Reasoning Templates: {'âœ…' if HAS_REASONING else 'âŒ'}")
    print(f"  â‘¤ Memory 3D: {'âœ…' if HAS_MEM3D else 'âŒ'}")
    print(f"  â‘¥ Interpretability: {'âœ…' if HAS_INTERP else 'âŒ'}")
    print(f"  â‘¦ Evals V9: {'âœ…' if HAS_EVALS_V9 else 'âŒ'}")
    print(f"  --- V10.0 åè®®å±‚ ---")
    print(f"  â‘§ Deep Agents: {'âœ…' if HAS_DEEP_AGENTS else 'âŒ'}")
    print(f"  Pydantic Contracts: âœ…")
    print(f"  Middleware: âœ…")
    print(f"  ReAct Pattern: âœ…")
    print(f"  Langfuse: {'âœ…' if HAS_LANGFUSE else 'âŒ'}")
    print(f"  --- P3 å‰æ²¿æ¡†æ¶ ---")
    print(f"  AG-UI: {'âœ…' if HAS_AG_UI else 'âŒ (å¯é€‰)'}")
    print(f"  Google ADK: {'âœ…' if HAS_GOOGLE_ADK else 'âŒ (å¯é€‰)'}")
    print(f"  OpenAI Agents SDK: {'âœ…' if HAS_OPENAI_AGENTS else 'âŒ (å¯é€‰)'}")
    print(f"  Graphiti: {'âœ…' if HAS_GRAPHITI else 'âŒ (å¯é€‰)'}")
    print()
    caps = get_platform_capabilities()
    active = sum(1 for v in caps.values() if v is True)
    print(f"  èƒ½åŠ›çŸ©é˜µ: {active}/{len(caps)} æ¿€æ´»")
